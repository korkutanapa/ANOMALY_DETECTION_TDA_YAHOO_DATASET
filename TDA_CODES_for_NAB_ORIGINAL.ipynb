{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korkutanapa/ANOMALY_DETECTION_TDA_YAHOO_DATASET/blob/main/TDA_CODES_for_NAB_ORIGINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfQzThhxIx6e",
        "outputId": "353a8372-8236-4b4e-80f1-f1208eadbadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (713/713), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 7119 (delta 601), reused 545 (delta 545), pack-reused 6406 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.73 MiB | 23.73 MiB/s, done.\n",
            "Resolving deltas: 100% (5015/5015), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m842.1/842.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--- 3. WRITING TDA_VEAD_METHOD DETECTOR ---\n",
            "✅ my_algo.py written.\n",
            "--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\n",
            "\n",
            "Available TDA H0 features:\n",
            "   0 -> H0_ratio_auc_L1_to_sum\n",
            "   1 -> H0_ratio_auc_to_max\n",
            "   2 -> H0_ratio_auc_to_l2\n",
            "   3 -> H0_bottleneck\n",
            "   4 -> tail_share_q90\n",
            "   5 -> H0_sum_centroid\n",
            "   6 -> H0_L2_norm\n",
            "   7 -> PETE_p1.6_q0.5\n",
            "   8 -> H0_energy_concentration\n",
            "   9 -> H0_dominance_share\n",
            "  10 -> H0_tail_curvature_80_90\n",
            "  11 -> H0_centroid_to_energy\n",
            "  12 -> H0_gini\n",
            "Select feature by index or name (default: H0_bottleneck): 2\n",
            "\n",
            ">>> Using TDA feature: H0_ratio_auc_to_l2\n",
            "\n",
            "   Found 58 data files in 'data'\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "--- 5. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -64.9991142579701 with anomaly threshold 0.4858633606616446.\n",
            "Optimizer found a max score of -80.86826565153001 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -126.51884269577215 with anomaly threshold 0.4179873261312045.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 21.98\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 15.14\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 30.31\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# Remove old NAB clone if exists\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "# Clone NAB repository\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "\n",
        "# Install ripser for TDA\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "# Ensure config folder + empty thresholds.json (optimize will fill it)\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. WRITE TDA_VEAD_METHOD (my_algo.py)\n",
        "# ============================================================\n",
        "print(\"--- 3. WRITING TDA_VEAD_METHOD DETECTOR ---\")\n",
        "\n",
        "tda_code = \"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "WINDOW_SIZE = 20\n",
        "TAU = 1\n",
        "DIMENSION = 6\n",
        "_EPS = 1e-12\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TDA FEATURE NAMES (H0-BASED)\n",
        "# ==========================================================\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_max\",\n",
        "    \"H0_ratio_auc_to_l2\",\n",
        "    \"H0_bottleneck\",\n",
        "    \"tail_share_q90\",\n",
        "    \"H0_sum_centroid\",\n",
        "    \"H0_L2_norm\",\n",
        "    \"PETE_p1.6_q0.5\",\n",
        "    \"H0_energy_concentration\",\n",
        "    \"H0_dominance_share\",\n",
        "    \"H0_tail_curvature_80_90\",\n",
        "    \"H0_centroid_to_energy\",\n",
        "    \"H0_gini\",\n",
        "]\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 2. TDA UTILITY FUNCTIONS (ADAPTED FROM YOUR CODE)\n",
        "# ==========================================================\n",
        "def takens_embed(window: np.ndarray, tau: int, m: int) -> np.ndarray:\n",
        "    \\\"\"\"\n",
        "    1D Takens embedding for a given window:\n",
        "      window: 1D array length N\n",
        "      tau: delay\n",
        "      m: embedding dimension\n",
        "    Returns shape (m, L) where L = N - (m-1)*tau, or None if not enough points.\n",
        "    \\\"\"\"\n",
        "    L = len(window) - (m - 1) * tau\n",
        "    if L <= 0:\n",
        "        return None\n",
        "    return np.stack([window[j : j + L * tau : tau] for j in range(m)], axis=1)\n",
        "\n",
        "\n",
        "def _clean_diag_h(diag_h):\n",
        "    if diag_h is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag_h, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    finite_mask = np.isfinite(arr).all(axis=1)\n",
        "    arr = arr[finite_mask]\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    ok = np.isfinite(d) & (d > b)\n",
        "    if not np.any(ok):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[ok], d[ok]], axis=1)\n",
        "\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "\n",
        "def _lifetimes(arr):\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0) if arr.size else np.empty((0,), float)\n",
        "\n",
        "\n",
        "def _bottleneck_amp(arr):\n",
        "    L = _lifetimes(arr)\n",
        "    return float(np.max(L)) if L.size else 0.0\n",
        "\n",
        "\n",
        "def h0_l2_norm(arr):\n",
        "    L = _lifetimes(arr)\n",
        "    return float(np.sqrt(np.sum(L**2))) if L.size else 0.0\n",
        "\n",
        "\n",
        "def _auc_tri_max(arr):\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    if arr.shape[0] == 1:\n",
        "        return 0.25 * ((arr[0, 1] - arr[0, 0])**2)\n",
        "\n",
        "    # Simple grid approximation for AUC\n",
        "    n_grid = 64\n",
        "    lo, hi = (float(np.min(arr[:, 0])), float(np.max(arr[:, 1]))) if arr.size else (0.0, 1.0)\n",
        "    grid = np.linspace(lo, hi, num=n_grid)\n",
        "    lam1 = np.zeros_like(grid, float)\n",
        "\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    for bj, dj in zip(b, d):\n",
        "        m = 0.5 * (bj + dj)\n",
        "        h = 0.5 * (dj - bj)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        mask = (grid >= bj) & (grid <= dj)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "\n",
        "        # Left side of triangle\n",
        "        l_mask = mask & (grid <= m)\n",
        "        if l_mask.any():\n",
        "            lam1[l_mask] = np.maximum(lam1[l_mask], (grid[l_mask] - bj) * (h / (m - bj + _EPS)))\n",
        "\n",
        "        # Right side of triangle\n",
        "        r_mask = mask & (grid > m)\n",
        "        if r_mask.any():\n",
        "            lam1[r_mask] = np.maximum(lam1[r_mask], (dj - grid[r_mask]) * (h / (dj - m + _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "\n",
        "def compute_h0_features_for_window(window: np.ndarray) -> dict:\n",
        "    \\\"\"\"\n",
        "    Compute all H0-based TDA features for one window.\n",
        "    Returns a dict with keys in FEATURE_NAMES.\n",
        "    \\\"\"\"\n",
        "    try:\n",
        "        emb = takens_embed(window, TAU, DIMENSION)\n",
        "        if emb is None:\n",
        "            # Not enough points\n",
        "            return {name: 0.0 for name in FEATURE_NAMES}\n",
        "\n",
        "        dgms = ripser(emb, maxdim=0)[\"dgms\"]\n",
        "        D0 = _clean_diag_h(dgms[0] if len(dgms) else None)\n",
        "\n",
        "        L = _lifetimes(D0)\n",
        "        S = float(L.sum())\n",
        "        A = _auc_tri_max(D0)\n",
        "        L2 = h0_l2_norm(D0)\n",
        "        mx = float(np.max(L)) if L.size else 0.0\n",
        "\n",
        "        feats = {}\n",
        "\n",
        "        # 1) Ratios\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = 0.0 if S <= _EPS else A / S\n",
        "        feats[\"H0_ratio_auc_to_max\"] = 0.0 if mx <= _EPS else A / mx\n",
        "        feats[\"H0_ratio_auc_to_l2\"] = 0.0 if L2 <= _EPS else A / L2\n",
        "\n",
        "        # 2) Bottleneck\n",
        "        feats[\"H0_bottleneck\"] = mx\n",
        "\n",
        "        # 3) Tail share q90\n",
        "        if S > _EPS and L.size > 0:\n",
        "            qv = float(np.quantile(L, 0.90))\n",
        "            feats[\"tail_share_q90\"] = float(L[L >= qv].sum()) / S\n",
        "        else:\n",
        "            feats[\"tail_share_q90\"] = 0.0\n",
        "\n",
        "        # 4) Centroid\n",
        "        if S > _EPS and D0.size > 0:\n",
        "            radial = np.abs((D0[:, 0] + D0[:, 1]) / np.sqrt(2.0))\n",
        "            feats[\"H0_sum_centroid\"] = float(np.sum(radial * L)) / S\n",
        "        else:\n",
        "            feats[\"H0_sum_centroid\"] = 0.0\n",
        "\n",
        "        # 5) L2 norm\n",
        "        feats[\"H0_L2_norm\"] = L2\n",
        "\n",
        "        # 6) PETE\n",
        "        if S > _EPS and D0.size > 0:\n",
        "            radial = (D0[:, 0] + D0[:, 1]) / np.sqrt(2.0)\n",
        "            num = np.sum((L ** 1.6) * (np.abs(radial) ** 0.5))\n",
        "            feats[\"PETE_p1.6_q0.5\"] = num / S\n",
        "        else:\n",
        "            feats[\"PETE_p1.6_q0.5\"] = 0.0\n",
        "\n",
        "        # 7) Energy concentration & dominance\n",
        "        feats[\"H0_energy_concentration\"] = (L2 / S) if S > _EPS else 0.0\n",
        "        feats[\"H0_dominance_share\"] = (mx / S) if S > _EPS else 0.0\n",
        "\n",
        "        # 8) Tail curvature\n",
        "        if S > _EPS and L.size > 0:\n",
        "            q90 = float(L[L >= np.quantile(L, 0.90)].sum()) / S\n",
        "            q80 = float(L[L >= np.quantile(L, 0.80)].sum()) / S\n",
        "            feats[\"H0_tail_curvature_80_90\"] = q90 - q80\n",
        "        else:\n",
        "            feats[\"H0_tail_curvature_80_90\"] = 0.0\n",
        "\n",
        "        # 9) Centroid to energy\n",
        "        feats[\"H0_centroid_to_energy\"] = (\n",
        "            feats[\"H0_sum_centroid\"] / L2\n",
        "        ) if L2 > _EPS else 0.0\n",
        "\n",
        "        # 10) Gini\n",
        "        if L.size > 0 and S > _EPS:\n",
        "            xs = np.sort(L)\n",
        "            n = xs.size\n",
        "            cumx = np.cumsum(xs)\n",
        "            feats[\"H0_gini\"] = float(1.0 + 1.0/n - 2.0 * (cumx.sum() / (n * S)))\n",
        "        else:\n",
        "            feats[\"H0_gini\"] = 0.0\n",
        "\n",
        "        # Ensure all feature names are present\n",
        "        for name in FEATURE_NAMES:\n",
        "            feats.setdefault(name, 0.0)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    except Exception:\n",
        "        # On any failure, return zeros for all features\n",
        "        return {name: 0.0 for name in FEATURE_NAMES}\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 3. MAIN: ASK USER FOR FEATURE, THEN RUN NAB DETECTOR\n",
        "# ==========================================================\n",
        "def run():\n",
        "    # --- Ask user to pick a TDA feature ---\n",
        "    print(\"\\\\nAvailable TDA H0 features:\")\n",
        "    for idx, name in enumerate(FEATURE_NAMES):\n",
        "        print(f\"  {idx:2d} -> {name}\")\n",
        "    choice = input(\"Select feature by index or name (default: H0_bottleneck): \").strip()\n",
        "\n",
        "    selected_feature = \"H0_bottleneck\"\n",
        "\n",
        "    if choice == \"\":\n",
        "        pass\n",
        "    elif choice.isdigit():\n",
        "        idx = int(choice)\n",
        "        if 0 <= idx < len(FEATURE_NAMES):\n",
        "            selected_feature = FEATURE_NAMES[idx]\n",
        "        else:\n",
        "            print(f\"Index {idx} out of range, using default H0_bottleneck.\")\n",
        "    else:\n",
        "        if choice in FEATURE_NAMES:\n",
        "            selected_feature = choice\n",
        "        else:\n",
        "            print(f\"Feature '{choice}' not recognized, using default H0_bottleneck.\")\n",
        "\n",
        "    print(f\"\\\\n>>> Using TDA feature: {selected_feature}\\\\n\")\n",
        "\n",
        "    # --- Process all NAB CSV files ---\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"   Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = df[\"value\"].astype(float).values\n",
        "\n",
        "            # 1) Compute chosen TDA feature over sliding windows\n",
        "            scores = []\n",
        "            for i in range(len(vals)):\n",
        "                if i < WINDOW_SIZE:\n",
        "                    scores.append(0.0)\n",
        "                else:\n",
        "                    window = vals[i - WINDOW_SIZE : i]\n",
        "                    feats = compute_h0_features_for_window(window)\n",
        "                    scores.append(float(feats.get(selected_feature, 0.0)))\n",
        "\n",
        "            scores = pd.Series(scores, dtype=float).fillna(0.0)\n",
        "\n",
        "            # 2) VEAD-like step on the chosen TDA feature\n",
        "            v = scores.diff().fillna(0.0)\n",
        "            a = v.diff().fillna(0.0)\n",
        "            raw_final = (3.5 * np.abs(v)) * (3.5 * np.abs(a))\n",
        "            #raw_final = scores.abs()\n",
        "            # 3) Normalize anomaly scores into [0, 1]\n",
        "            max_val = raw_final.max()\n",
        "            if pd.isna(max_val) or max_val <= 0:\n",
        "                final_scores = raw_final * 0.0\n",
        "            else:\n",
        "                final_scores = (raw_final / max_val).clip(lower=0.0, upper=1.0)\n",
        "\n",
        "            # 4) Build output path: results/TDA_VEAD_Method/<category>/TDA_VEAD_Method_<file>.csv\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            # 5) Write result: timestamp + anomaly_score\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores.values\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"   -> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   !! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 5. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 5. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPJ5ahRNgChucz2vmZrHETt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}