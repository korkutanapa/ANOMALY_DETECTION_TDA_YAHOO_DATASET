{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korkutanapa/ANOMALY_DETECTION_TDA_YAHOO_DATASET/blob/main/TDA_CODES_for_NAB_ORIGINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VERSION 1"
      ],
      "metadata": {
        "id": "Qtj2mQa9ojxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# Remove old NAB clone if exists\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "# Clone NAB repository\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "\n",
        "# Install ripser for TDA\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "# Ensure config folder + empty thresholds.json (optimize will fill it)\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. WRITE TDA_VEAD_METHOD (my_algo.py)\n",
        "# ============================================================\n",
        "print(\"--- 3. WRITING TDA_VEAD_METHOD DETECTOR ---\")\n",
        "\n",
        "tda_code = \"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14   # sliding window length for raw time series\n",
        "TAU         = 1\n",
        "DIMENSION   = 7    # must satisfy (DIMENSION-1)*TAU < WINDOW_SIZE\n",
        "_EPS = 1e-12\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION (YOUR ORIGINAL SETTINGS)\n",
        "# ==========================================================\n",
        "KV   = 3.5          # Velocity gain\n",
        "KA   = 3.5          # Acceleration gain\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    \\\"\"\"\n",
        "    VEAD score for a 1D series (raw_vals).\n",
        "\n",
        "    Steps:\n",
        "      1. Interpolate NaNs.\n",
        "      2. Compute velocity v = diff(x).\n",
        "      3. Compute acceleration a = diff(v).\n",
        "      4. Robust Z-score with MAD for v and a.\n",
        "      5. Apply mode logic (strict / plateau / abs_plateau).\n",
        "      6. Final score = (kv * zv) * (ka * za).\n",
        "    \\\"\"\"\n",
        "    # 1) Convert to pandas Series & interpolate\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    # 2) Velocity and acceleration\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    # 3) Robust Z-score with MAD\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    # 4) Mode logic\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        # Only positive shocks\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        # Allow small negative dips, silence large negative drops\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        # Absolute shock magnitude (spikes + drops)\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    # 5) VEAD energy\n",
        "    score = (kv * zv) * (ka * za)\n",
        "\n",
        "    # 6) Clean NaNs / infs\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TDA FEATURE NAMES (H0-BASED)\n",
        "# ==========================================================\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_max\",\n",
        "    \"H0_ratio_auc_to_l2\",\n",
        "    \"H0_bottleneck\",\n",
        "    \"tail_share_q90\",\n",
        "    \"H0_sum_centroid\",\n",
        "    \"H0_L2_norm\",\n",
        "    \"PETE_p1.6_q0.5\",\n",
        "    \"H0_energy_concentration\",\n",
        "    \"H0_dominance_share\",\n",
        "    \"H0_tail_curvature_80_90\",\n",
        "    \"H0_centroid_to_energy\",\n",
        "    \"H0_gini\",\n",
        "]\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 2. TDA UTILITY FUNCTIONS (ADAPTED FROM YOUR CODE)\n",
        "# ==========================================================\n",
        "def takens_embed(window: np.ndarray, tau: int, m: int) -> np.ndarray:\n",
        "    \\\"\"\"\n",
        "    1D Takens embedding for a given window:\n",
        "      window: 1D array length N\n",
        "      tau: delay\n",
        "      m: embedding dimension\n",
        "    Returns shape (m, L) where L = N - (m-1)*tau, or None if not enough points.\n",
        "    \\\"\"\"\n",
        "    L = len(window) - (m - 1) * tau\n",
        "    if L <= 0:\n",
        "        return None\n",
        "    return np.stack([window[j : j + L * tau : tau] for j in range(m)], axis=1)\n",
        "\n",
        "\n",
        "def _clean_diag_h(diag_h):\n",
        "    if diag_h is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag_h, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    finite_mask = np.isfinite(arr).all(axis=1)\n",
        "    arr = arr[finite_mask]\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    ok = np.isfinite(d) & (d > b)\n",
        "    if not np.any(ok):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[ok], d[ok]], axis=1)\n",
        "\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "\n",
        "def _lifetimes(arr):\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0) if arr.size else np.empty((0,), float)\n",
        "\n",
        "\n",
        "def _bottleneck_amp(arr):\n",
        "    L = _lifetimes(arr)\n",
        "    return float(np.max(L)) if L.size else 0.0\n",
        "\n",
        "\n",
        "def h0_l2_norm(arr):\n",
        "    L = _lifetimes(arr)\n",
        "    return float(np.sqrt(np.sum(L**2))) if L.size else 0.0\n",
        "\n",
        "\n",
        "def _auc_tri_max(arr):\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    if arr.shape[0] == 1:\n",
        "        return 0.25 * ((arr[0, 1] - arr[0, 0])**2)\n",
        "\n",
        "    # Simple grid approximation for AUC\n",
        "    n_grid = 64\n",
        "    lo, hi = (float(np.min(arr[:, 0])), float(np.max(arr[:, 1]))) if arr.size else (0.0, 1.0)\n",
        "    grid = np.linspace(lo, hi, num=n_grid)\n",
        "    lam1 = np.zeros_like(grid, float)\n",
        "\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    for bj, dj in zip(b, d):\n",
        "        m = 0.5 * (bj + dj)\n",
        "        h = 0.5 * (dj - bj)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        mask = (grid >= bj) & (grid <= dj)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "\n",
        "        # Left side of triangle\n",
        "        l_mask = mask & (grid <= m)\n",
        "        if l_mask.any():\n",
        "            lam1[l_mask] = np.maximum(lam1[l_mask], (grid[l_mask] - bj) * (h / (m - bj + _EPS)))\n",
        "\n",
        "        # Right side of triangle\n",
        "        r_mask = mask & (grid > m)\n",
        "        if r_mask.any():\n",
        "            lam1[r_mask] = np.maximum(lam1[r_mask], (dj - grid[r_mask]) * (h / (dj - m + _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "\n",
        "def compute_h0_features_for_window(window: np.ndarray) -> dict:\n",
        "    \\\"\"\"\n",
        "    Compute all H0-based TDA features for one window.\n",
        "    Returns a dict with keys in FEATURE_NAMES.\n",
        "    \\\"\"\"\n",
        "    try:\n",
        "        emb = takens_embed(window, TAU, DIMENSION)\n",
        "        if emb is None:\n",
        "            # Not enough points\n",
        "            return {name: 0.0 for name in FEATURE_NAMES}\n",
        "\n",
        "        dgms = ripser(emb, maxdim=0)[\"dgms\"]\n",
        "        D0 = _clean_diag_h(dgms[0] if len(dgms) else None)\n",
        "\n",
        "        L = _lifetimes(D0)\n",
        "        S = float(L.sum())\n",
        "        A = _auc_tri_max(D0)\n",
        "        L2 = h0_l2_norm(D0)\n",
        "        mx = float(np.max(L)) if L.size else 0.0\n",
        "\n",
        "        feats = {}\n",
        "\n",
        "        # 1) Ratios\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = 0.0 if S <= _EPS else A / S\n",
        "        feats[\"H0_ratio_auc_to_max\"] = 0.0 if mx <= _EPS else A / mx\n",
        "        feats[\"H0_ratio_auc_to_l2\"] = 0.0 if L2 <= _EPS else A / L2\n",
        "\n",
        "        # 2) Bottleneck\n",
        "        feats[\"H0_bottleneck\"] = mx\n",
        "\n",
        "        # 3) Tail share q90\n",
        "        if S > _EPS and L.size > 0:\n",
        "            qv = float(np.quantile(L, 0.90))\n",
        "            feats[\"tail_share_q90\"] = float(L[L >= qv].sum()) / S\n",
        "        else:\n",
        "            feats[\"tail_share_q90\"] = 0.0\n",
        "\n",
        "        # 4) Centroid\n",
        "        if S > _EPS and D0.size > 0:\n",
        "            radial = np.abs((D0[:, 0] + D0[:, 1]) / np.sqrt(2.0))\n",
        "            feats[\"H0_sum_centroid\"] = float(np.sum(radial * L)) / S\n",
        "        else:\n",
        "            feats[\"H0_sum_centroid\"] = 0.0\n",
        "\n",
        "        # 5) L2 norm\n",
        "        feats[\"H0_L2_norm\"] = L2\n",
        "\n",
        "        # 6) PETE\n",
        "        if S > _EPS and D0.size > 0:\n",
        "            radial = (D0[:, 0] + D0[:, 1]) / np.sqrt(2.0)\n",
        "            num = np.sum((L ** 1.6) * (np.abs(radial) ** 0.5))\n",
        "            feats[\"PETE_p1.6_q0.5\"] = num / S\n",
        "        else:\n",
        "            feats[\"PETE_p1.6_q0.5\"] = 0.0\n",
        "\n",
        "        # 7) Energy concentration & dominance\n",
        "        feats[\"H0_energy_concentration\"] = (L2 / S) if S > _EPS else 0.0\n",
        "        feats[\"H0_dominance_share\"] = (mx / S) if S > _EPS else 0.0\n",
        "\n",
        "        # 8) Tail curvature\n",
        "        if S > _EPS and L.size > 0:\n",
        "            q90 = float(L[L >= np.quantile(L, 0.90)].sum()) / S\n",
        "            q80 = float(L[L >= np.quantile(L, 0.80)].sum()) / S\n",
        "            feats[\"H0_tail_curvature_80_90\"] = q90 - q80\n",
        "        else:\n",
        "            feats[\"H0_tail_curvature_80_90\"] = 0.0\n",
        "\n",
        "        # 9) Centroid to energy\n",
        "        feats[\"H0_centroid_to_energy\"] = (\n",
        "            feats[\"H0_sum_centroid\"] / L2\n",
        "        ) if L2 > _EPS else 0.0\n",
        "\n",
        "        # 10) Gini\n",
        "        if L.size > 0 and S > _EPS:\n",
        "            xs = np.sort(L)\n",
        "            n = xs.size\n",
        "            cumx = np.cumsum(xs)\n",
        "            feats[\"H0_gini\"] = float(1.0 + 1.0/n - 2.0 * (cumx.sum() / (n * S)))\n",
        "        else:\n",
        "            feats[\"H0_gini\"] = 0.0\n",
        "\n",
        "        # Ensure all feature names are present\n",
        "        for name in FEATURE_NAMES:\n",
        "            feats.setdefault(name, 0.0)\n",
        "\n",
        "        return feats\n",
        "\n",
        "    except Exception:\n",
        "        # On any failure, return zeros for all features\n",
        "        return {name: 0.0 for name in FEATURE_NAMES}\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 3. MAIN: ASK USER FOR FEATURE, THEN RUN NAB DETECTOR\n",
        "# ==========================================================\n",
        "def run():\n",
        "    # --- Ask user to pick a TDA feature ---\n",
        "    print(\"\\\\nAvailable TDA H0 features:\")\n",
        "    for idx, name in enumerate(FEATURE_NAMES):\n",
        "        print(f\"  {idx:2d} -> {name}\")\n",
        "    choice = input(\"Select feature by index or name (default: H0_bottleneck): \").strip()\n",
        "\n",
        "    selected_feature = \"H0_bottleneck\"\n",
        "\n",
        "    if choice == \"\":\n",
        "        pass\n",
        "    elif choice.isdigit():\n",
        "        idx = int(choice)\n",
        "        if 0 <= idx < len(FEATURE_NAMES):\n",
        "            selected_feature = FEATURE_NAMES[idx]\n",
        "        else:\n",
        "            print(f\"Index {idx} out of range, using default H0_bottleneck.\")\n",
        "    else:\n",
        "        if choice in FEATURE_NAMES:\n",
        "            selected_feature = choice\n",
        "        else:\n",
        "            print(f\"Feature '{choice}' not recognized, using default H0_bottleneck.\")\n",
        "\n",
        "    print(f\"\\\\n>>> Using TDA feature: {selected_feature}\")\n",
        "    print(f\">>> VEAD mode   : {MODE}\")\n",
        "    print(f\">>> KV, KA       : {KV}, {KA}\\\\n\")\n",
        "\n",
        "    # --- Process all NAB CSV files ---\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"   Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = df[\"value\"].astype(float).values\n",
        "\n",
        "            # 1) Compute chosen TDA feature over sliding windows\n",
        "            scores = []\n",
        "            for i in range(len(vals)):\n",
        "                if i < WINDOW_SIZE:\n",
        "                    scores.append(0.0)\n",
        "                else:\n",
        "                    window = vals[i - WINDOW_SIZE : i]\n",
        "                    feats = compute_h0_features_for_window(window)\n",
        "                    scores.append(float(feats.get(selected_feature, 0.0)))\n",
        "\n",
        "            scores = pd.Series(scores, dtype=float).fillna(0.0)\n",
        "\n",
        "            # 2) Apply full VEAD algorithm to the chosen TDA feature\n",
        "            vead_scores = _vead_series(scores.values, kv=KV, ka=KA, mode=MODE)\n",
        "            vead_series = pd.Series(vead_scores, index=df.index)\n",
        "\n",
        "            # 3) Normalize VEAD scores into [0, 1] for NAB\n",
        "            max_val = vead_series.max()\n",
        "            if pd.isna(max_val) or max_val <= 0:\n",
        "                final_scores = vead_series * 0.0\n",
        "            else:\n",
        "                final_scores = (vead_series / max_val).clip(lower=0.0, upper=1.0)\n",
        "\n",
        "            # 4) Build output path: results/TDA_VEAD_Method/<category>/TDA_VEAD_Method_<file>.csv\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            # 5) Write result: timestamp + anomaly_score\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores.values\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"   -> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   !! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 5. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 5. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-I8Y1yXooBA",
        "outputId": "b047bc58-6cc7-499f-8afb-ba78a7a7d1b8",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 7119 (delta 552), reused 495 (delta 495), pack-reused 6420 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.13 MiB | 22.46 MiB/s, done.\n",
            "Resolving deltas: 100% (5001/5001), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m842.1/842.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--- 3. WRITING TDA_VEAD_METHOD DETECTOR ---\n",
            "✅ my_algo.py written.\n",
            "--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\n",
            "\n",
            "Available TDA H0 features:\n",
            "   0 -> H0_ratio_auc_L1_to_sum\n",
            "   1 -> H0_ratio_auc_to_max\n",
            "   2 -> H0_ratio_auc_to_l2\n",
            "   3 -> H0_bottleneck\n",
            "   4 -> tail_share_q90\n",
            "   5 -> H0_sum_centroid\n",
            "   6 -> H0_L2_norm\n",
            "   7 -> PETE_p1.6_q0.5\n",
            "   8 -> H0_energy_concentration\n",
            "   9 -> H0_dominance_share\n",
            "  10 -> H0_tail_curvature_80_90\n",
            "  11 -> H0_centroid_to_energy\n",
            "  12 -> H0_gini\n",
            "Select feature by index or name (default: H0_bottleneck): 7\n",
            "\n",
            ">>> Using TDA feature: PETE_p1.6_q0.5\n",
            ">>> VEAD mode   : abs_plateau\n",
            ">>> KV, KA       : 3.5, 3.5\n",
            "\n",
            "   Found 58 data files in 'data'\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "--- 5. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -74.72853794890906 with anomaly threshold 0.9055822171465668.\n",
            "Optimizer found a max score of -80.65338884071619 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -152.05995954808674 with anomaly threshold 0.6888304237693663.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 17.79\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 15.24\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 22.97\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2 No VEAD"
      ],
      "metadata": {
        "id": "XmvtkQskRYmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# Remove old NAB clone if exists\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "# Clone NAB repository\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "\n",
        "# Install ripser for TDA\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "# Ensure config folder + empty thresholds.json (optimize will fill it)\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. WRITE TDA_METHOD (my_algo.py)  -- NO VEAD\n",
        "# ============================================================\n",
        "print(\"--- 3. WRITING TDA_METHOD DETECTOR (NO VEAD) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_Method_NoVEAD\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7    # must satisfy (DIMENSION-1)*TAU < WINDOW_SIZE\n",
        "_EPS = 1e-12\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TDA FEATURE NAMES (H0-BASED)\n",
        "# ==========================================================\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_max\",\n",
        "    \"H0_ratio_auc_to_l2\",\n",
        "    \"H0_bottleneck\",\n",
        "    \"tail_share_q90\",\n",
        "    \"H0_sum_centroid\",\n",
        "    \"H0_L2_norm\",\n",
        "    \"PETE_p1.6_q0.5\",\n",
        "    \"H0_energy_concentration\",\n",
        "    \"H0_dominance_share\",\n",
        "    \"H0_tail_curvature_80_90\",\n",
        "    \"H0_centroid_to_energy\",\n",
        "    \"H0_gini\",\n",
        "]\n",
        "\n",
        "# ==========================================================\n",
        "# 2. TDA UTILITY FUNCTIONS\n",
        "# ==========================================================\n",
        "def takens_embed(window: np.ndarray, tau: int, m: int) -> np.ndarray:\n",
        "    L = len(window) - (m - 1) * tau\n",
        "    if L <= 0:\n",
        "        return None\n",
        "    return np.stack([window[j : j + L * tau : tau] for j in range(m)], axis=1)\n",
        "\n",
        "def _clean_diag_h(diag_h):\n",
        "    if diag_h is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag_h, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    finite_mask = np.isfinite(arr).all(axis=1)\n",
        "    arr = arr[finite_mask]\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    ok = np.isfinite(d) & (d > b)\n",
        "    if not np.any(ok):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[ok], d[ok]], axis=1)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _lifetimes(arr):\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0) if arr.size else np.empty((0,), float)\n",
        "\n",
        "def h0_l2_norm(arr):\n",
        "    L = _lifetimes(arr)\n",
        "    return float(np.sqrt(np.sum(L**2))) if L.size else 0.0\n",
        "\n",
        "def _auc_tri_max(arr):\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    if arr.shape[0] == 1:\n",
        "        return 0.25 * ((arr[0, 1] - arr[0, 0])**2)\n",
        "\n",
        "    n_grid = 64\n",
        "    lo, hi = (float(np.min(arr[:, 0])), float(np.max(arr[:, 1])))\n",
        "    grid = np.linspace(lo, hi, num=n_grid)\n",
        "    lam1 = np.zeros_like(grid, float)\n",
        "\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    for bj, dj in zip(b, d):\n",
        "        m = 0.5 * (bj + dj)\n",
        "        h = 0.5 * (dj - bj)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        mask = (grid >= bj) & (grid <= dj)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "\n",
        "        l_mask = mask & (grid <= m)\n",
        "        if l_mask.any():\n",
        "            lam1[l_mask] = np.maximum(lam1[l_mask], (grid[l_mask] - bj) * (h / (m - bj + _EPS)))\n",
        "\n",
        "        r_mask = mask & (grid > m)\n",
        "        if r_mask.any():\n",
        "            lam1[r_mask] = np.maximum(lam1[r_mask], (dj - grid[r_mask]) * (h / (dj - m + _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def compute_h0_features_for_window(window: np.ndarray) -> dict:\n",
        "    try:\n",
        "        emb = takens_embed(window, TAU, DIMENSION)\n",
        "        if emb is None:\n",
        "            return {name: 0.0 for name in FEATURE_NAMES}\n",
        "\n",
        "        dgms = ripser(emb, maxdim=0)[\"dgms\"]\n",
        "        D0 = _clean_diag_h(dgms[0] if len(dgms) else None)\n",
        "\n",
        "        L = _lifetimes(D0)\n",
        "        S = float(L.sum())\n",
        "        A = _auc_tri_max(D0)\n",
        "        L2 = h0_l2_norm(D0)\n",
        "        mx = float(np.max(L)) if L.size else 0.0\n",
        "\n",
        "        feats = {}\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = 0.0 if S <= _EPS else A / S\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = 0.0 if mx <= _EPS else A / mx\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = 0.0 if L2 <= _EPS else A / L2\n",
        "        feats[\"H0_bottleneck\"]          = mx\n",
        "\n",
        "        if S > _EPS and L.size > 0:\n",
        "            qv = float(np.quantile(L, 0.90))\n",
        "            feats[\"tail_share_q90\"] = float(L[L >= qv].sum()) / S\n",
        "        else:\n",
        "            feats[\"tail_share_q90\"] = 0.0\n",
        "\n",
        "        if S > _EPS and D0.size > 0:\n",
        "            radial = np.abs((D0[:, 0] + D0[:, 1]) / np.sqrt(2.0))\n",
        "            feats[\"H0_sum_centroid\"] = float(np.sum(radial * L)) / S\n",
        "        else:\n",
        "            feats[\"H0_sum_centroid\"] = 0.0\n",
        "\n",
        "        feats[\"H0_L2_norm\"] = L2\n",
        "\n",
        "        if S > _EPS and D0.size > 0:\n",
        "            radial = (D0[:, 0] + D0[:, 1]) / np.sqrt(2.0)\n",
        "            num = np.sum((L ** 1.6) * (np.abs(radial) ** 0.5))\n",
        "            feats[\"PETE_p1.6_q0.5\"] = num / S\n",
        "        else:\n",
        "            feats[\"PETE_p1.6_q0.5\"] = 0.0\n",
        "\n",
        "        feats[\"H0_energy_concentration\"] = (L2 / S) if S > _EPS else 0.0\n",
        "        feats[\"H0_dominance_share\"]      = (mx / S) if S > _EPS else 0.0\n",
        "\n",
        "        if S > _EPS and L.size > 0:\n",
        "            q90 = float(L[L >= np.quantile(L, 0.90)].sum()) / S\n",
        "            q80 = float(L[L >= np.quantile(L, 0.80)].sum()) / S\n",
        "            feats[\"H0_tail_curvature_80_90\"] = q90 - q80\n",
        "        else:\n",
        "            feats[\"H0_tail_curvature_80_90\"] = 0.0\n",
        "\n",
        "        feats[\"H0_centroid_to_energy\"] = (feats[\"H0_sum_centroid\"] / L2) if L2 > _EPS else 0.0\n",
        "\n",
        "        if L.size > 0 and S > _EPS:\n",
        "            xs = np.sort(L)\n",
        "            n = xs.size\n",
        "            cumx = np.cumsum(xs)\n",
        "            feats[\"H0_gini\"] = float(1.0 + 1.0/n - 2.0 * (cumx.sum() / (n * S)))\n",
        "        else:\n",
        "            feats[\"H0_gini\"] = 0.0\n",
        "\n",
        "        for name in FEATURE_NAMES:\n",
        "            feats.setdefault(name, 0.0)\n",
        "\n",
        "        return feats\n",
        "    except Exception:\n",
        "        return {name: 0.0 for name in FEATURE_NAMES}\n",
        "\n",
        "def _minmax_01(x: np.ndarray) -> np.ndarray:\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    mn = float(np.min(x)) if x.size else 0.0\n",
        "    mx = float(np.max(x)) if x.size else 0.0\n",
        "    if (mx - mn) <= _EPS:\n",
        "        return np.zeros_like(x, dtype=float)\n",
        "    return (x - mn) / (mx - mn)\n",
        "\n",
        "# ==========================================================\n",
        "# 3. MAIN: ASK USER FOR FEATURE, THEN RUN NAB DETECTOR\n",
        "# ==========================================================\n",
        "def run():\n",
        "    print(\"\\nAvailable TDA H0 features:\")\n",
        "    for idx, name in enumerate(FEATURE_NAMES):\n",
        "        print(f\"  {idx:2d} -> {name}\")\n",
        "    choice = input(\"Select feature by index or name (default: H0_bottleneck): \").strip()\n",
        "\n",
        "    selected_feature = \"H0_bottleneck\"\n",
        "    if choice == \"\":\n",
        "        pass\n",
        "    elif choice.isdigit():\n",
        "        idx = int(choice)\n",
        "        if 0 <= idx < len(FEATURE_NAMES):\n",
        "            selected_feature = FEATURE_NAMES[idx]\n",
        "        else:\n",
        "            print(f\"Index {idx} out of range, using default H0_bottleneck.\")\n",
        "    else:\n",
        "        if choice in FEATURE_NAMES:\n",
        "            selected_feature = choice\n",
        "        else:\n",
        "            print(f\"Feature '{choice}' not recognized, using default H0_bottleneck.\")\n",
        "\n",
        "    print(f\"\\n>>> Using TDA feature (NO VEAD): {selected_feature}\\n\")\n",
        "\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"   Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = df[\"value\"].astype(float).values\n",
        "\n",
        "            # 1) Compute chosen TDA feature over sliding windows\n",
        "            feat_series = np.zeros(len(vals), dtype=float)\n",
        "            for i in range(len(vals)):\n",
        "                if i < WINDOW_SIZE:\n",
        "                    feat_series[i] = 0.0\n",
        "                else:\n",
        "                    window = vals[i - WINDOW_SIZE : i]\n",
        "                    feats = compute_h0_features_for_window(window)\n",
        "                    feat_series[i] = float(feats.get(selected_feature, 0.0))\n",
        "\n",
        "            # 2) Normalize feature directly into [0, 1] for NAB\n",
        "            final_scores = _minmax_01(feat_series)\n",
        "\n",
        "            # 3) Build output path\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            # 4) Write result: timestamp + anomaly_score\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"   -> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   !! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING TDA_Method_NoVEAD ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 5. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 5. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_Method_NoVEAD --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "M28FcSxZNm9w",
        "outputId": "602a752f-85ae-47f3-ea83-3b273cb255e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 7119 (delta 552), reused 495 (delta 495), pack-reused 6420 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.13 MiB | 23.23 MiB/s, done.\n",
            "Resolving deltas: 100% (5001/5001), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 3. WRITING TDA_METHOD DETECTOR (NO VEAD) ---\n",
            "✅ my_algo.py written.\n",
            "--- 4. RUNNING TDA_Method_NoVEAD ON ALL DATASETS ---\n",
            "\n",
            "Available TDA H0 features:\n",
            "   0 -> H0_ratio_auc_L1_to_sum\n",
            "   1 -> H0_ratio_auc_to_max\n",
            "   2 -> H0_ratio_auc_to_l2\n",
            "   3 -> H0_bottleneck\n",
            "   4 -> tail_share_q90\n",
            "   5 -> H0_sum_centroid\n",
            "   6 -> H0_L2_norm\n",
            "   7 -> PETE_p1.6_q0.5\n",
            "   8 -> H0_energy_concentration\n",
            "   9 -> H0_dominance_share\n",
            "  10 -> H0_tail_curvature_80_90\n",
            "  11 -> H0_centroid_to_energy\n",
            "  12 -> H0_gini\n",
            "Select feature by index or name (default: H0_bottleneck): 7\n",
            "\n",
            ">>> Using TDA feature (NO VEAD): PETE_p1.6_q0.5\n",
            "\n",
            "   Found 58 data files in 'data'\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAdExchange/TDA_Method_NoVEAD_exchange-4_cpc_results.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAdExchange/TDA_Method_NoVEAD_exchange-2_cpc_results.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAdExchange/TDA_Method_NoVEAD_exchange-2_cpm_results.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAdExchange/TDA_Method_NoVEAD_exchange-3_cpm_results.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAdExchange/TDA_Method_NoVEAD_exchange-4_cpm_results.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAdExchange/TDA_Method_NoVEAD_exchange-3_cpc_results.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_rds_cpu_utilization_e47b3b.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_fe7f93.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_24ae8d.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_disk_write_bytes_1ef3de.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_elb_request_count_8c0756.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_network_in_5abac7.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_5f5533.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_825cc2.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_grok_asg_anomaly.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_network_in_257a54.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_rds_cpu_utilization_cc0c53.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_ac20cd.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_53ea38.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_c6585a.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_disk_write_bytes_c0d644.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realAWSCloudwatch/TDA_Method_NoVEAD_ec2_cpu_utilization_77c1ca.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_GOOG.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_CRM.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_PFE.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_AAPL.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_KO.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_AMZN.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_UPS.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_IBM.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_FB.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTweets/TDA_Method_NoVEAD_Twitter_volume_CVS.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialWithAnomaly/TDA_Method_NoVEAD_art_daily_jumpsdown.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialWithAnomaly/TDA_Method_NoVEAD_art_daily_nojump.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialWithAnomaly/TDA_Method_NoVEAD_art_load_balancer_spikes.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialWithAnomaly/TDA_Method_NoVEAD_art_daily_jumpsup.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialWithAnomaly/TDA_Method_NoVEAD_art_increase_spike_density.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialWithAnomaly/TDA_Method_NoVEAD_art_daily_flatmiddle.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_rogue_agent_key_updown.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_machine_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_rogue_agent_key_hold.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_ec2_request_latency_system_failure.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_ambient_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_nyc_taxi.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realKnownCause/TDA_Method_NoVEAD_cpu_utilization_asg_misconfiguration.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_TravelTime_451.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_TravelTime_387.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_occupancy_6005.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_speed_6005.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_occupancy_t4013.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_speed_7578.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/realTraffic/TDA_Method_NoVEAD_speed_t4013.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialNoAnomaly/TDA_Method_NoVEAD_art_daily_perfect_square_wave.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialNoAnomaly/TDA_Method_NoVEAD_art_daily_small_noise.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialNoAnomaly/TDA_Method_NoVEAD_art_flatline.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialNoAnomaly/TDA_Method_NoVEAD_art_noisy.csv\n",
            "   -> Wrote: results/TDA_Method_NoVEAD/artificialNoAnomaly/TDA_Method_NoVEAD_art_daily_no_noise.csv\n",
            "--- 5. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_Method_NoVEAD'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -89.53357672965451 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -115.08572524619139 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -175.5335767296545 with anomaly threshold 1.0.\n",
            "\n",
            "Running scoring step\n",
            "TDA_Method_NoVEAD detector benchmark scores written to /content/NAB/results/TDA_Method_NoVEAD/TDA_Method_NoVEAD_standard_scores.csv\n",
            "TDA_Method_NoVEAD detector benchmark scores written to /content/NAB/results/TDA_Method_NoVEAD/TDA_Method_NoVEAD_reward_low_FP_rate_scores.csv\n",
            "TDA_Method_NoVEAD detector benchmark scores written to /content/NAB/results/TDA_Method_NoVEAD/TDA_Method_NoVEAD_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'Method_NoVEAD_standard' profile = 11.41\n",
            "Final score for 'TDA' detector on 'Method_NoVEAD_reward_low_FP_rate' profile = 0.39\n",
            "Final score for 'TDA' detector on 'Method_NoVEAD_reward_low_FN_rate' profile = 16.23\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 3 full flow w 77 FEATURES"
      ],
      "metadata": {
        "id": "HigiiFcrRdVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takens embedding: turns the 1D window into a point cloud (delay embedding).\n",
        "\n",
        "ripser: computes persistence diagrams H0 and H1 from that point cloud.\n",
        "\n",
        "Feature extraction: computes many statistics from diagrams (lifetimes, entropy, gini, Carlsson, etc.).\n",
        "\n",
        "Feature selection: you pick one of the 77 features.\n",
        "\n",
        "VEAD: treats that chosen feature as a time series, computes velocity + acceleration shocks with robust MAD Z-scores, then multiplies them.\n",
        "\n",
        "Normalize: required by NAB to output scores in [0,1].\n",
        "\n",
        "Write results: NAB expects results/<detector>/.../*.csv with timestamp, anomaly_score.\n",
        "\n",
        "run.py optimize/score: NAB finds thresholds and computes official NAB scores.\n",
        "\n",
        "If you want, I can also generate a single combined flowchart (Notebook + Detector + NAB scoring) in one diagram, or a very compact 8-box version for slides."
      ],
      "metadata": {
        "id": "uN7tBNbqayjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "# ============================================================\n",
        "print(\"--- 3. WRITING TDA_VEAD_METHOD DETECTOR (VEAD + 77 FEATURES) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "\n",
        "MAXDIM = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION (UNCHANGED)\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    # 1) Convert to pandas Series & interpolate\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    # 2) Velocity and acceleration\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    # 3) Robust Z-score with MAD\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    # 4) Mode logic\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "#    (COPIED/ALIGNED WITH YOUR 77-FEATURE CODE)\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    mask = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[mask], d[mask]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "# ==========================================================\n",
        "# 3. ROBUST FEATURES LIST (77)\n",
        "# ==========================================================\n",
        "ROBUST_FEATURES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "FEATURE_NAMES = ROBUST_FEATURES[:]  # now user can pick any of the 77\n",
        "\n",
        "# ==========================================================\n",
        "# 4. MAIN: PICK ONE FEATURE -> VEAD -> NORMALIZE -> WRITE\n",
        "# ==========================================================\n",
        "def run():\n",
        "    print(\"\\nAvailable TDA features (77):\")\n",
        "    for idx, name in enumerate(FEATURE_NAMES):\n",
        "        print(f\"  {idx:2d} -> {name}\")\n",
        "    choice = input(\"Select feature by index or name (default: H0_bottleneck): \").strip()\n",
        "\n",
        "    selected_feature = \"H0_bottleneck\"\n",
        "    if choice == \"\":\n",
        "        pass\n",
        "    elif choice.isdigit():\n",
        "        idx = int(choice)\n",
        "        if 0 <= idx < len(FEATURE_NAMES):\n",
        "            selected_feature = FEATURE_NAMES[idx]\n",
        "        else:\n",
        "            print(f\"Index {idx} out of range, using default H0_bottleneck.\")\n",
        "    else:\n",
        "        if choice in FEATURE_NAMES:\n",
        "            selected_feature = choice\n",
        "        else:\n",
        "            print(f\"Feature '{choice}' not recognized, using default H0_bottleneck.\")\n",
        "\n",
        "    print(f\"\\n>>> Using TDA feature: {selected_feature}\")\n",
        "    print(f\">>> VEAD mode   : {MODE}\")\n",
        "    print(f\">>> KV, KA      : {KV}, {KA}\\n\")\n",
        "\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"   Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ---- SELECT FEATURE SERIES (length n) ----\n",
        "            series = pd.to_numeric(full.get(selected_feature, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "            series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            # ---- VEAD on selected feature ----\n",
        "            vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "            vead_series = pd.Series(vead_scores, index=df.index)\n",
        "\n",
        "            # ---- Normalize to [0,1] for NAB ----\n",
        "            mx = float(np.max(vead_series.values)) if len(vead_series) else 0.0\n",
        "            if not np.isfinite(mx) or mx <= 0:\n",
        "                final_scores = np.zeros_like(vead_series.values, dtype=float)\n",
        "            else:\n",
        "                final_scores = np.clip(vead_series.values / mx, 0.0, 1.0)\n",
        "\n",
        "            # ---- Write output ----\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"   -> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   !! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 5. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 5. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5p4CFy8RhTJ",
        "outputId": "d3fbd0f7-1f4d-46e7-b3b8-9753201ea0d8",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (731/731), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 7119 (delta 564), reused 502 (delta 502), pack-reused 6388 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.16 MiB | 29.61 MiB/s, done.\n",
            "Resolving deltas: 100% (4983/4983), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m842.1/842.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--- 3. WRITING TDA_VEAD_METHOD DETECTOR (VEAD + 77 FEATURES) ---\n",
            "✅ my_algo.py written.\n",
            "--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\n",
            "\n",
            "Available TDA features (77):\n",
            "   0 -> H0_Carlsson_f1\n",
            "   1 -> H0_Carlsson_f3\n",
            "   2 -> H0_Carlsson_f5\n",
            "   3 -> H0_L1_lifetime\n",
            "   4 -> H0_L1_norm\n",
            "   5 -> H0_L2_lifetime\n",
            "   6 -> H0_L2_norm\n",
            "   7 -> H0_Linf_lifetime\n",
            "   8 -> H0_Linf_norm\n",
            "   9 -> H0_bottleneck\n",
            "  10 -> H0_centroid_to_energy\n",
            "  11 -> H0_centroid_y\n",
            "  12 -> H0_dominance_share\n",
            "  13 -> H0_energy_concentration\n",
            "  14 -> H0_gini\n",
            "  15 -> H0_max_diag_dist\n",
            "  16 -> H0_max_lifetime\n",
            "  17 -> H0_mean_death\n",
            "  18 -> H0_mean_diag_dist\n",
            "  19 -> H0_mean_lifetime\n",
            "  20 -> H0_median_lifetime\n",
            "  21 -> H0_min_lifetime\n",
            "  22 -> H0_persistence_entropy\n",
            "  23 -> H0_q50\n",
            "  24 -> H0_q75\n",
            "  25 -> H0_q90\n",
            "  26 -> H0_q95\n",
            "  27 -> H0_q99\n",
            "  28 -> H0_ratio_auc_L1_to_sum\n",
            "  29 -> H0_ratio_auc_to_l2\n",
            "  30 -> H0_ratio_auc_to_max\n",
            "  31 -> H0_std_death\n",
            "  32 -> H0_std_lifetime\n",
            "  33 -> H0_sum_centroid\n",
            "  34 -> H0_sum_diag_dist\n",
            "  35 -> H0_sum_lifetime\n",
            "  36 -> H0_tail_curvature_80_90\n",
            "  37 -> H0_tail_share_q80\n",
            "  38 -> H0_tail_share_q90\n",
            "  39 -> H0_tail_share_q95\n",
            "  40 -> H1_Carlsson_f1\n",
            "  41 -> H1_Carlsson_f2\n",
            "  42 -> H1_Carlsson_f3\n",
            "  43 -> H1_L1_lifetime\n",
            "  44 -> H1_L1_norm\n",
            "  45 -> H1_L2_lifetime\n",
            "  46 -> H1_L2_norm\n",
            "  47 -> H1_Linf_lifetime\n",
            "  48 -> H1_Linf_norm\n",
            "  49 -> H1_betti\n",
            "  50 -> H1_count_lifetime\n",
            "  51 -> H1_dominance_share\n",
            "  52 -> H1_energy_concentration\n",
            "  53 -> H1_gini\n",
            "  54 -> H1_max_diag_dist\n",
            "  55 -> H1_max_lifetime\n",
            "  56 -> H1_mean_diag_dist\n",
            "  57 -> H1_mean_lifetime\n",
            "  58 -> H1_median_lifetime\n",
            "  59 -> H1_min_lifetime\n",
            "  60 -> H1_persistence_entropy\n",
            "  61 -> H1_q50\n",
            "  62 -> H1_q75\n",
            "  63 -> H1_q90\n",
            "  64 -> H1_q95\n",
            "  65 -> H1_q99\n",
            "  66 -> H1_std_birth\n",
            "  67 -> H1_std_death\n",
            "  68 -> H1_std_lifetime\n",
            "  69 -> H1_sum_diag_dist\n",
            "  70 -> H1_sum_lifetime\n",
            "  71 -> H1_tail_share_q80\n",
            "  72 -> H1_tail_share_q90\n",
            "  73 -> H1_tail_share_q95\n",
            "  74 -> H1_to_H0_betti_ratio\n",
            "  75 -> H1_to_H0_entropy_ratio\n",
            "  76 -> PETE_p1.6_q0.5\n",
            "Select feature by index or name (default: H0_bottleneck): 26\n",
            "\n",
            ">>> Using TDA feature: H0_q95\n",
            ">>> VEAD mode   : abs_plateau\n",
            ">>> KV, KA      : 3.5, 3.5\n",
            "\n",
            "   Found 58 data files in 'data'\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "--- 5. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -56.20768431616171 with anomaly threshold 0.3457211234148364.\n",
            "Optimizer found a max score of -86.07128809943114 with anomaly threshold 0.5798116185198682.\n",
            "Optimizer found a max score of -106.03475784653192 with anomaly threshold 0.2453677658945846.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 25.77\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 12.90\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 36.20\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 4"
      ],
      "metadata": {
        "id": "5I4VSNU0WKxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "print(\"--- 3. WRITING TDA_VEAD_METHOD DETECTOR (77 FEATURES) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 10\n",
        "TAU         = 1\n",
        "DIMENSION   = 3\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1   # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION (SAME AS YOUR ORIGINAL)\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + _EPS\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1) TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2) DIAGRAM UTILITIES + FEATURE COMPUTATION (H0 + H1)\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    mask = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[mask], d[mask]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "                \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0}\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    # H0-only extras used in your 77-feature code\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats.get(\"H0_gini\", 0.0))\n",
        "\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "# ==========================================================\n",
        "# 3) THE 77 ROBUST FEATURES (EXACT LIST)\n",
        "# ==========================================================\n",
        "ROBUST_FEATURES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "def _zmad_vector(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    med = np.nanmedian(x)\n",
        "    mad = np.nanmedian(np.abs(x - med)) + _EPS\n",
        "    return (x - med) / mad\n",
        "\n",
        "def combine_features_to_score(feature_row: dict, feature_names):\n",
        "    # feature_row: {feature_name: value, ...} for ONE time index\n",
        "    vals = []\n",
        "    for c in feature_names:\n",
        "        v = float(feature_row.get(c, 0.0))\n",
        "        if not np.isfinite(v):\n",
        "            v = 0.0\n",
        "        vals.append(v)\n",
        "\n",
        "    # robust z across features? (not meaningful for a single row)\n",
        "    # Instead we follow your original approach: zMAD per feature over time.\n",
        "    # But to keep your exact logic, we will aggregate ABS(zMAD) computed later on a time-series matrix.\n",
        "    # -> This function is not used; kept for compatibility.\n",
        "    return float(np.nanmean(np.abs(np.asarray(vals, dtype=float))))\n",
        "\n",
        "def combine_features_matrix_to_score(feature_matrix: pd.DataFrame, feature_names):\n",
        "    cols = [c for c in feature_names if c in feature_matrix.columns]\n",
        "    if len(cols) == 0:\n",
        "        return np.zeros(len(feature_matrix), dtype=float)\n",
        "\n",
        "    Z = []\n",
        "    for c in cols:\n",
        "        v = pd.to_numeric(feature_matrix[c], errors=\"coerce\").astype(float).to_numpy()\n",
        "        v = np.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        z = _zmad_vector(v)\n",
        "        Z.append(np.abs(z))\n",
        "\n",
        "    Z = np.vstack(Z).T\n",
        "    raw = np.nanmean(Z, axis=1)\n",
        "    raw = np.nan_to_num(raw, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    mn, mx = float(np.min(raw)), float(np.max(raw))\n",
        "    if mx - mn <= _EPS:\n",
        "        return np.zeros_like(raw)\n",
        "    return (raw - mn) / (mx - mn + _EPS)\n",
        "\n",
        "# ==========================================================\n",
        "# 4) MAIN RUN\n",
        "# ==========================================================\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"   Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            # Build per-index feature rows\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # 1) Combine 77 features -> ONE raw score in [0,1]\n",
        "            tda_scores_01 = combine_features_matrix_to_score(full, ROBUST_FEATURES)\n",
        "\n",
        "            # 2) Apply VEAD on the combined TDA score\n",
        "            vead_scores = _vead_series(tda_scores_01, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "            # 3) Normalize VEAD output to [0,1] for NAB\n",
        "            mx = float(np.max(vead_scores)) if vead_scores.size else 0.0\n",
        "            if not np.isfinite(mx) or mx <= 0:\n",
        "                final_scores = np.zeros(n, dtype=float)\n",
        "            else:\n",
        "                final_scores = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "\n",
        "            # Output path\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"   -> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   !! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "print(\"--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "print(\"--- 5. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "h6nWHCbvTjR-",
        "outputId": "ba7c0815-6c9e-4107-e527-1583a4f6f715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 7119 (delta 552), reused 495 (delta 495), pack-reused 6420 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.13 MiB | 21.23 MiB/s, done.\n",
            "Resolving deltas: 100% (5001/5001), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 3. WRITING TDA_VEAD_METHOD DETECTOR (77 FEATURES) ---\n",
            "✅ my_algo.py written.\n",
            "--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\n",
            "   Found 58 data files in 'data'\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "--- 5. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -100.71295828358313 with anomaly threshold 0.934549841350186.\n",
            "Optimizer found a max score of -110.19417598321643 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -181.44363141934477 with anomaly threshold 0.5859839261379607.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 6.59\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 2.50\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 14.53\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 5 _ full flow w 5 top selected"
      ],
      "metadata": {
        "id": "eHgokEPtDhix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "# ============================================================\n",
        "print(\"--- 3. WRITING TDA_VEAD_METHOD DETECTOR (VEAD + 77 FEATURES) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "\n",
        "MAXDIM = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION (UNCHANGED)\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    # 1) Convert to pandas Series & interpolate\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    # 2) Velocity and acceleration\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    # 3) Robust Z-score with MAD\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    # 4) Mode logic\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    mask = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[mask], d[mask]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "# ==========================================================\n",
        "# 3. ROBUST FEATURES LIST (77)\n",
        "# ==========================================================\n",
        "ROBUST_FEATURES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "FEATURE_NAMES = ROBUST_FEATURES[:]\n",
        "\n",
        "# ==========================================================\n",
        "# 4. MAIN\n",
        "# ==========================================================\n",
        "def run():\n",
        "    print(\"\\nAvailable TDA features (77):\")\n",
        "    for idx, name in enumerate(FEATURE_NAMES):\n",
        "        print(f\"  {idx:2d} -> {name}\")\n",
        "    choice = input(\"Select feature by index or name (default: H0_bottleneck): \").strip()\n",
        "\n",
        "    selected_feature = \"H0_bottleneck\"\n",
        "    if choice == \"\":\n",
        "        pass\n",
        "    elif choice.isdigit():\n",
        "        idx = int(choice)\n",
        "        if 0 <= idx < len(FEATURE_NAMES):\n",
        "            selected_feature = FEATURE_NAMES[idx]\n",
        "        else:\n",
        "            print(f\"Index {idx} out of range, using default H0_bottleneck.\")\n",
        "    else:\n",
        "        if choice in FEATURE_NAMES:\n",
        "            selected_feature = choice\n",
        "        else:\n",
        "            print(f\"Feature '{choice}' not recognized, using default H0_bottleneck.\")\n",
        "\n",
        "    print(f\"\\n>>> Using TDA feature: {selected_feature}\")\n",
        "    print(f\">>> VEAD mode   : {MODE}\")\n",
        "    print(f\">>> KV, KA      : {KV}, {KA}\\n\")\n",
        "\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"   Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            series = pd.to_numeric(full.get(selected_feature, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "            series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "            vead_series = pd.Series(vead_scores, index=df.index)\n",
        "\n",
        "            # Normalize [0,1]\n",
        "            mx = float(np.max(vead_series.values)) if len(vead_series) else 0.0\n",
        "            if not np.isfinite(mx) or mx <= 0:\n",
        "                final_scores = np.zeros_like(vead_series.values, dtype=float)\n",
        "            else:\n",
        "                final_scores = np.clip(vead_series.values / mx, 0.0, 1.0)\n",
        "\n",
        "            # Keep only top-5 scores, silence others\n",
        "            k = 2\n",
        "            n_scores = len(final_scores)\n",
        "            if n_scores > 0 and np.max(final_scores) > 0:\n",
        "                k_eff = min(k, n_scores)\n",
        "                topk_idx = np.argpartition(final_scores, -k_eff)[-k_eff:]\n",
        "                sparse_scores = np.zeros_like(final_scores, dtype=float)\n",
        "                sparse_scores[topk_idx] = final_scores[topk_idx]\n",
        "                final_scores = sparse_scores\n",
        "\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"   -> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   !! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 5. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 5. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSt6QjBSDaOe",
        "outputId": "717680c5-0bf7-496f-e855-f56fbec22f51",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 7119 (delta 552), reused 495 (delta 495), pack-reused 6420 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.13 MiB | 29.12 MiB/s, done.\n",
            "Resolving deltas: 100% (5001/5001), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 3. WRITING TDA_VEAD_METHOD DETECTOR (VEAD + 77 FEATURES) ---\n",
            "✅ my_algo.py written.\n",
            "--- 4. RUNNING TDA_VEAD_Method ON ALL DATASETS ---\n",
            "\n",
            "Available TDA features (77):\n",
            "   0 -> H0_Carlsson_f1\n",
            "   1 -> H0_Carlsson_f3\n",
            "   2 -> H0_Carlsson_f5\n",
            "   3 -> H0_L1_lifetime\n",
            "   4 -> H0_L1_norm\n",
            "   5 -> H0_L2_lifetime\n",
            "   6 -> H0_L2_norm\n",
            "   7 -> H0_Linf_lifetime\n",
            "   8 -> H0_Linf_norm\n",
            "   9 -> H0_bottleneck\n",
            "  10 -> H0_centroid_to_energy\n",
            "  11 -> H0_centroid_y\n",
            "  12 -> H0_dominance_share\n",
            "  13 -> H0_energy_concentration\n",
            "  14 -> H0_gini\n",
            "  15 -> H0_max_diag_dist\n",
            "  16 -> H0_max_lifetime\n",
            "  17 -> H0_mean_death\n",
            "  18 -> H0_mean_diag_dist\n",
            "  19 -> H0_mean_lifetime\n",
            "  20 -> H0_median_lifetime\n",
            "  21 -> H0_min_lifetime\n",
            "  22 -> H0_persistence_entropy\n",
            "  23 -> H0_q50\n",
            "  24 -> H0_q75\n",
            "  25 -> H0_q90\n",
            "  26 -> H0_q95\n",
            "  27 -> H0_q99\n",
            "  28 -> H0_ratio_auc_L1_to_sum\n",
            "  29 -> H0_ratio_auc_to_l2\n",
            "  30 -> H0_ratio_auc_to_max\n",
            "  31 -> H0_std_death\n",
            "  32 -> H0_std_lifetime\n",
            "  33 -> H0_sum_centroid\n",
            "  34 -> H0_sum_diag_dist\n",
            "  35 -> H0_sum_lifetime\n",
            "  36 -> H0_tail_curvature_80_90\n",
            "  37 -> H0_tail_share_q80\n",
            "  38 -> H0_tail_share_q90\n",
            "  39 -> H0_tail_share_q95\n",
            "  40 -> H1_Carlsson_f1\n",
            "  41 -> H1_Carlsson_f2\n",
            "  42 -> H1_Carlsson_f3\n",
            "  43 -> H1_L1_lifetime\n",
            "  44 -> H1_L1_norm\n",
            "  45 -> H1_L2_lifetime\n",
            "  46 -> H1_L2_norm\n",
            "  47 -> H1_Linf_lifetime\n",
            "  48 -> H1_Linf_norm\n",
            "  49 -> H1_betti\n",
            "  50 -> H1_count_lifetime\n",
            "  51 -> H1_dominance_share\n",
            "  52 -> H1_energy_concentration\n",
            "  53 -> H1_gini\n",
            "  54 -> H1_max_diag_dist\n",
            "  55 -> H1_max_lifetime\n",
            "  56 -> H1_mean_diag_dist\n",
            "  57 -> H1_mean_lifetime\n",
            "  58 -> H1_median_lifetime\n",
            "  59 -> H1_min_lifetime\n",
            "  60 -> H1_persistence_entropy\n",
            "  61 -> H1_q50\n",
            "  62 -> H1_q75\n",
            "  63 -> H1_q90\n",
            "  64 -> H1_q95\n",
            "  65 -> H1_q99\n",
            "  66 -> H1_std_birth\n",
            "  67 -> H1_std_death\n",
            "  68 -> H1_std_lifetime\n",
            "  69 -> H1_sum_diag_dist\n",
            "  70 -> H1_sum_lifetime\n",
            "  71 -> H1_tail_share_q80\n",
            "  72 -> H1_tail_share_q90\n",
            "  73 -> H1_tail_share_q95\n",
            "  74 -> H1_to_H0_betti_ratio\n",
            "  75 -> H1_to_H0_entropy_ratio\n",
            "  76 -> PETE_p1.6_q0.5\n",
            "Select feature by index or name (default: H0_bottleneck): 26\n",
            "\n",
            ">>> Using TDA feature: H0_q95\n",
            ">>> VEAD mode   : abs_plateau\n",
            ">>> KV, KA      : 3.5, 3.5\n",
            "\n",
            "   Found 58 data files in 'data'\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "   -> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "--- 5. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -45.925311991322815 with anomaly threshold 0.1283219787398802.\n",
            "Optimizer found a max score of -49.828115897794966 with anomaly threshold 0.1283219787398802.\n",
            "Optimizer found a max score of -119.92531199132281 with anomaly threshold 0.1283219787398802.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 30.20\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 28.52\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 32.21\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 6 Majority Voting"
      ],
      "metadata": {
        "id": "RZJeNc-oS1Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "#    ENSEMBLE: top-2 per feature -> majority vote -> top-5 = 1 else 0\n",
        "# ============================================================\n",
        "print(\"--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    mask = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[mask], d[mask]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "# ==========================================================\n",
        "# 3. ROBUST FEATURES LIST (77)\n",
        "# ==========================================================\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "# ==========================================================\n",
        "# 4. MAIN: per file -> compute all features -> vote -> top-5 = 1 else 0\n",
        "# ==========================================================\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    # Voting config\n",
        "    K_PER_FEATURE = 3   # each feature votes for top-2 indices\n",
        "    TOP_FINAL     = 10  # final anomalies = top-5 voted indices\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            # ---- Build features for all windows ----\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ---- Majority voting across ALL features ----\n",
        "            votes = np.zeros(n, dtype=int)\n",
        "\n",
        "            for feat_name in FEATURE_NAMES:\n",
        "                series = pd.to_numeric(full.get(feat_name, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "                series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "                mx = float(np.max(vead_scores)) if len(vead_scores) else 0.0\n",
        "                if (not np.isfinite(mx)) or mx <= 0:\n",
        "                    continue\n",
        "\n",
        "                scores01 = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "                if np.max(scores01) <= 0:\n",
        "                    continue\n",
        "\n",
        "                k_eff = min(K_PER_FEATURE, n)\n",
        "                topk_idx = np.argpartition(scores01, -k_eff)[-k_eff:]\n",
        "\n",
        "                # deterministic ordering (by score desc, then index asc)\n",
        "                topk_idx = topk_idx[np.lexsort((topk_idx, -scores01[topk_idx]))]\n",
        "\n",
        "                votes[topk_idx] += 1\n",
        "\n",
        "            # ---- Take top-5 by vote count -> anomaly_score = 1 else 0 ----\n",
        "            final_scores = np.zeros(n, dtype=float)\n",
        "            if np.max(votes) > 0:\n",
        "                top_final_eff = min(TOP_FINAL, n)\n",
        "                order = np.lexsort((np.arange(n), -votes))  # votes desc, index asc\n",
        "                chosen = order[:top_final_eff]\n",
        "                final_scores[chosen] = 1.0\n",
        "\n",
        "            # ---- Write output ----\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"-> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cZGAq2sS5yZ",
        "outputId": "689c4d9b-6e4a-4983-d490-ffb435958dfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (713/713), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 7119 (delta 601), reused 545 (delta 545), pack-reused 6406 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.73 MiB | 26.52 MiB/s, done.\n",
            "Resolving deltas: 100% (5015/5015), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING) ---\n",
            "✅ my_algo.py written.\n",
            "--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\n",
            "Found 58 data files in 'data'\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "--- 4. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -11.202138510974809 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -37.69225875461228 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -54.202138510974805 with anomaly threshold 1.0.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 45.17\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 33.75\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 51.09\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9uY34q58Ym4orjdzWfYZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}