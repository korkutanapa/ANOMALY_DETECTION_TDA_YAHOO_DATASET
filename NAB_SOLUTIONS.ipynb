{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN4Knt8CkukQk+EOEVlxzIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korkutanapa/ANOMALY_DETECTION_TDA_YAHOO_DATASET/blob/main/NAB_SOLUTIONS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAB SOLUTION"
      ],
      "metadata": {
        "id": "335QGeuPp06L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize value of K_PER_FEATURE = 6\n",
        "\n",
        "TOP_FINAL = 12 by window_size 14 dimension 7\n",
        "\n",
        "gives 1 to selected features"
      ],
      "metadata": {
        "id": "CM8bX_I-2k1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title standart nab solution\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "#    ENSEMBLE: top-2 per feature -> majority vote -> top-5 = 1 else 0\n",
        "# ============================================================\n",
        "print(\"--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    mask = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[mask], d[mask]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "# ==========================================================\n",
        "# 3. ROBUST FEATURES LIST (77)\n",
        "# ==========================================================\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "# ==========================================================\n",
        "# 4. MAIN: per file -> compute all features -> vote -> top-5 = 1 else 0\n",
        "# ==========================================================\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "    ##############################################################################################################\n",
        "    # Voting config\n",
        "    K_PER_FEATURE = 6  # each feature votes for top-2 indices\n",
        "    TOP_FINAL     = 12  # final anomalies = top-5 voted indices\n",
        "    ########################################################################################################################333\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            # ---- Build features for all windows ----\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ---- Majority voting across ALL features ----\n",
        "            votes = np.zeros(n, dtype=int)\n",
        "\n",
        "            for feat_name in FEATURE_NAMES:\n",
        "                series = pd.to_numeric(full.get(feat_name, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "                series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "                mx = float(np.max(vead_scores)) if len(vead_scores) else 0.0\n",
        "                if (not np.isfinite(mx)) or mx <= 0:\n",
        "                    continue\n",
        "\n",
        "                scores01 = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "                if np.max(scores01) <= 0:\n",
        "                    continue\n",
        "\n",
        "                k_eff = min(K_PER_FEATURE, n)\n",
        "                topk_idx = np.argpartition(scores01, -k_eff)[-k_eff:]\n",
        "\n",
        "                # deterministic ordering (by score desc, then index asc)\n",
        "                topk_idx = topk_idx[np.lexsort((topk_idx, -scores01[topk_idx]))]\n",
        "\n",
        "                votes[topk_idx] += 1\n",
        "\n",
        "            # ---- Take top-5 by vote count -> anomaly_score = 1 else 0 ----\n",
        "            final_scores = np.zeros(n, dtype=float)\n",
        "            if np.max(votes) > 0:\n",
        "                top_final_eff = min(TOP_FINAL, n)\n",
        "                order = np.lexsort((np.arange(n), -votes))  # votes desc, index asc\n",
        "                chosen = order[:top_final_eff]\n",
        "                final_scores[chosen] = 1.0\n",
        "\n",
        "            # ---- Write output ----\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"-> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "id": "6cZGAq2sS5yZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97276c5a-bbe5-4f38-e1d1-572ae61af0cd",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 7119 (delta 552), reused 495 (delta 495), pack-reused 6420 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.13 MiB | 37.66 MiB/s, done.\n",
            "Resolving deltas: 100% (5001/5001), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING) ---\n",
            "✅ my_algo.py written.\n",
            "--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\n",
            "Found 58 data files in 'data'\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "--- 4. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -0.6050732105935437 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -35.047318925127115 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -33.605073210593545 with anomaly threshold 1.0.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 49.74\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 34.89\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 57.01\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title improvement of infinite death points\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "#    ENSEMBLE: top-2 per feature -> majority vote -> top-5 = 1 else 0\n",
        "# ============================================================\n",
        "print(\"--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "\n",
        "    # keep finite births; allow finite deaths OR +inf deaths\n",
        "    mask = np.isfinite(b) & (np.isfinite(d) | np.isposinf(d))\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    b = b[mask]\n",
        "    d = d[mask]\n",
        "\n",
        "    # per-window dmax from finite deaths\n",
        "    finite_d = d[np.isfinite(d)]\n",
        "    if finite_d.size > 0:\n",
        "        dmax = float(np.max(finite_d))\n",
        "    else:\n",
        "        # rare fallback: if everything is +inf, set something > max birth\n",
        "        dmax = float(np.max(b) + 1.0)\n",
        "\n",
        "    # cap +inf deaths\n",
        "    d = np.where(np.isposinf(d), dmax, d)\n",
        "\n",
        "    # remove degenerate points\n",
        "    good = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(good):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    return np.stack([b[good], d[good]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    # Voting config\n",
        "    K_PER_FEATURE = 6\n",
        "    TOP_FINAL     = 12\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            # ---- Build features for all windows ----\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ---- Majority voting across ALL features ----\n",
        "            votes = np.zeros(n, dtype=int)\n",
        "\n",
        "            for feat_name in FEATURE_NAMES:\n",
        "                series = pd.to_numeric(full.get(feat_name, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "                series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "                mx = float(np.max(vead_scores)) if len(vead_scores) else 0.0\n",
        "                if (not np.isfinite(mx)) or mx <= 0:\n",
        "                    continue\n",
        "\n",
        "                scores01 = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "                if np.max(scores01) <= 0:\n",
        "                    continue\n",
        "\n",
        "                k_eff = min(K_PER_FEATURE, n)\n",
        "                topk_idx = np.argpartition(scores01, -k_eff)[-k_eff:]\n",
        "                topk_idx = topk_idx[np.lexsort((topk_idx, -scores01[topk_idx]))]  # score desc, index asc\n",
        "                votes[topk_idx] += 1\n",
        "\n",
        "            # ---- Take TOP_FINAL by vote count -> anomaly_score = 1 else 0 ----\n",
        "            final_scores = np.zeros(n, dtype=float)\n",
        "            if np.max(votes) > 0:\n",
        "                top_final_eff = min(TOP_FINAL, n)\n",
        "                order = np.lexsort((np.arange(n), -votes))  # votes desc, index asc\n",
        "                chosen = order[:top_final_eff]\n",
        "                final_scores[chosen] = 1.0\n",
        "\n",
        "            # ---- Write output ----\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"-> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c1e652-47af-4124-9ab7-5fd7045d5cfb",
        "collapsed": true,
        "cellView": "form",
        "id": "zbPUVKk5DoSl"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (713/713), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 7119 (delta 601), reused 545 (delta 545), pack-reused 6406 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.73 MiB | 41.15 MiB/s, done.\n",
            "Resolving deltas: 100% (5015/5015), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING) ---\n",
            "✅ my_algo.py written.\n",
            "--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\n",
            "Found 58 data files in 'data'\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "--- 4. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of 2.865612162042114 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -32.1681780704018 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -27.134387837957885 with anomaly threshold 1.0.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 51.24\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 36.13\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 58.87\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FNN CODES\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser scikit-learn\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "#    ENSEMBLE: top-2 per feature -> majority vote -> top-5 = 1 else 0\n",
        "#    + FNN per-file embedding dimension selection\n",
        "# ============================================================\n",
        "print(\"--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING + FNN DIM) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "from sklearn.neighbors import KDTree\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (WINDOW_SIZE, TAU fixed; DIM picked by FNN per file)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7          # default / fallback\n",
        "DEFAULT_DIMENSION = DIMENSION\n",
        "\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 1b. FNN-based embedding dimension selection (per file)\n",
        "# ==========================================================\n",
        "FNN_R_TOL = 10.0\n",
        "FNN_A_TOL = 2.0\n",
        "FNN_THRESHOLD = 0.05  # 5%\n",
        "FNN_MAX_M = 20\n",
        "\n",
        "def _takens_embedding_full(x, m, tau):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    M = len(x) - (m - 1) * tau\n",
        "    if M <= 5:\n",
        "        return None\n",
        "    emb = np.zeros((M, m), dtype=float)\n",
        "    for i in range(m):\n",
        "        emb[:, i] = x[i * tau : i * tau + M]\n",
        "    return emb\n",
        "\n",
        "def _compute_fnn_ratio(x, m, tau, R_tol=FNN_R_TOL, A_tol=FNN_A_TOL):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "\n",
        "    med = np.nanmedian(x) if np.isfinite(np.nanmedian(x)) else 0.0\n",
        "    x = np.nan_to_num(x, nan=float(med), posinf=float(med), neginf=float(med))\n",
        "\n",
        "    N = len(x)\n",
        "    M_m   = N - (m - 1) * tau\n",
        "    M_mp1 = N - m * tau\n",
        "    if M_m <= 5 or M_mp1 <= 5:\n",
        "        return np.nan\n",
        "\n",
        "    emb_m   = _takens_embedding_full(x, m, tau)\n",
        "    emb_mp1 = _takens_embedding_full(x, m + 1, tau)\n",
        "    if emb_m is None or emb_mp1 is None:\n",
        "        return np.nan\n",
        "\n",
        "    M = min(len(emb_m), len(emb_mp1))\n",
        "    emb_m   = emb_m[:M]\n",
        "    emb_mp1 = emb_mp1[:M]\n",
        "\n",
        "    tree = KDTree(emb_m)\n",
        "    dist, idx = tree.query(emb_m, k=2)\n",
        "\n",
        "    Rm = dist[:, 1]\n",
        "    j  = idx[:, 1]\n",
        "\n",
        "    x_i = x[m * tau : m * tau + M]\n",
        "    x_j = x[j + m * tau]\n",
        "    delta = np.abs(x_i - x_j)\n",
        "\n",
        "    Rm = np.maximum(Rm, 1e-12)\n",
        "    stdx = np.std(x) + 1e-12\n",
        "\n",
        "    fnn_1 = (delta / Rm) > R_tol\n",
        "    fnn_2 = (np.sqrt(Rm**2 + delta**2) / stdx) > A_tol\n",
        "    fnn = np.logical_or(fnn_1, fnn_2)\n",
        "\n",
        "    return float(np.mean(fnn))\n",
        "\n",
        "def pick_embedding_dimension_fnn(x, tau, window_size,\n",
        "                                 default_m=DEFAULT_DIMENSION,\n",
        "                                 threshold=FNN_THRESHOLD,\n",
        "                                 max_m=FNN_MAX_M):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    n = len(x)\n",
        "\n",
        "    max_m_allowed = int((window_size - 1) // max(int(tau), 1))\n",
        "    if max_m_allowed < 2:\n",
        "        return int(default_m)\n",
        "\n",
        "    if n < max(60, 5 * window_size):\n",
        "        return int(min(default_m, max_m_allowed))\n",
        "\n",
        "    m_hi = min(max_m, max_m_allowed, 20)\n",
        "    if m_hi < 2:\n",
        "        return int(min(default_m, max_m_allowed))\n",
        "\n",
        "    m_range = range(2, m_hi + 1)\n",
        "    ratios = []\n",
        "    for m in m_range:\n",
        "        ratios.append(_compute_fnn_ratio(x, m, tau))\n",
        "    ratios = np.array(ratios, dtype=float)\n",
        "\n",
        "    if not np.any(np.isfinite(ratios)):\n",
        "        return int(min(default_m, max_m_allowed))\n",
        "\n",
        "    good = np.where((ratios < threshold) & np.isfinite(ratios))[0]\n",
        "    if len(good) > 0:\n",
        "        best_m = list(m_range)[int(good[0])]\n",
        "    else:\n",
        "        best_m = list(m_range)[int(np.nanargmin(ratios))]\n",
        "\n",
        "    best_m = int(np.clip(best_m, 2, max_m_allowed))\n",
        "    return best_m\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "\n",
        "    mask = np.isfinite(b) & (np.isfinite(d) | np.isposinf(d))\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    b = b[mask]\n",
        "    d = d[mask]\n",
        "\n",
        "    finite_d = d[np.isfinite(d)]\n",
        "    if finite_d.size > 0:\n",
        "        dmax = float(np.max(finite_d))\n",
        "    else:\n",
        "        dmax = float(np.max(b) + 1.0)\n",
        "\n",
        "    d = np.where(np.isposinf(d), dmax, d)\n",
        "\n",
        "    good = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(good):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    return np.stack([b[good], d[good]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    # Voting config\n",
        "    K_PER_FEATURE = 6\n",
        "    TOP_FINAL     = 12\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "            if n == 0:\n",
        "                continue\n",
        "\n",
        "            # ==========================================================\n",
        "            # FNN: pick embedding dimension per file, clamped by WINDOW_SIZE\n",
        "            # ==========================================================\n",
        "            best_m = pick_embedding_dimension_fnn(vals, TAU, WINDOW_SIZE, default_m=DEFAULT_DIMENSION)\n",
        "\n",
        "            # ---- Build features for all windows ----\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, best_m)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ---- Majority voting across ALL features ----\n",
        "            votes = np.zeros(n, dtype=int)\n",
        "\n",
        "            for feat_name in FEATURE_NAMES:\n",
        "                series = pd.to_numeric(full.get(feat_name, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "                series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "                mx = float(np.max(vead_scores)) if len(vead_scores) else 0.0\n",
        "                if (not np.isfinite(mx)) or mx <= 0:\n",
        "                    continue\n",
        "\n",
        "                scores01 = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "                if np.max(scores01) <= 0:\n",
        "                    continue\n",
        "\n",
        "                k_eff = min(K_PER_FEATURE, n)\n",
        "                topk_idx = np.argpartition(scores01, -k_eff)[-k_eff:]\n",
        "                topk_idx = topk_idx[np.lexsort((topk_idx, -scores01[topk_idx]))]\n",
        "                votes[topk_idx] += 1\n",
        "\n",
        "            # ---- Take TOP_FINAL by vote count -> anomaly_score = 1 else 0 ----\n",
        "            final_scores = np.zeros(n, dtype=float)\n",
        "            if np.max(votes) > 0:\n",
        "                top_final_eff = min(TOP_FINAL, n)\n",
        "                order = np.lexsort((np.arange(n), -votes))  # votes desc, index asc\n",
        "                chosen = order[:top_final_eff]\n",
        "                final_scores[chosen] = 1.0\n",
        "\n",
        "            # ---- Write output ----\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"-> Wrote: {out_path}  (best_m={best_m})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE + FNN DIM) ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "paizC0nMLcIJ",
        "outputId": "cf4e2a28-ff14-4676-88ca-755dbf8158ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (731/731), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 7119 (delta 564), reused 502 (delta 502), pack-reused 6388 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.16 MiB | 37.10 MiB/s, done.\n",
            "Resolving deltas: 100% (4983/4983), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING + FNN DIM) ---\n",
            "✅ my_algo.py written.\n",
            "--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE + FNN DIM) ON ALL DATASETS ---\n",
            "Found 58 data files in 'data'\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv  (best_m=10)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv  (best_m=8)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv  (best_m=12)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv  (best_m=9)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv  (best_m=11)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv  (best_m=2)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv  (best_m=2)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv  (best_m=12)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv  (best_m=11)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv  (best_m=13)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv  (best_m=9)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv  (best_m=7)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv  (best_m=13)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv  (best_m=13)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv  (best_m=11)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv  (best_m=13)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv  (best_m=13)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv  (best_m=12)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv  (best_m=7)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv  (best_m=12)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv  (best_m=6)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv  (best_m=2)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv  (best_m=3)\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv  (best_m=4)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv  (best_m=7)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv  (best_m=5)\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv  (best_m=5)\n",
            "--- 4. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -20.96978927472128 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -57.42407520813501 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -62.96978927472128 with anomaly threshold 1.0.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 40.96\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 25.25\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 48.57\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPROVEMENT 1"
      ],
      "metadata": {
        "id": "IGEJzVr891nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TDA_VEAD_Method (Ensemble Voting) + Per-file & Per-dataset anomaly counts\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. WRITE TDA_VEAD_METHOD (my_algo.py)  [VEAD + 77 FEATURES]\n",
        "#    ENSEMBLE: top-K per feature -> majority vote -> top-N = 1 else 0\n",
        "#    + PRINT anomaly counts per file + totals per dataset folder\n",
        "# ============================================================\n",
        "print(\"--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING + COUNTS) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# Helper: count contiguous anomaly segments (events)\n",
        "# ==========================================================\n",
        "def _count_runs(mask: np.ndarray) -> int:\n",
        "    mask = np.asarray(mask, dtype=bool)\n",
        "    if mask.size == 0:\n",
        "        return 0\n",
        "    return int(np.sum(mask & ~np.r_[False, mask[:-1]]))\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    mask = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[mask], d[mask]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "# ==========================================================\n",
        "# 3. ROBUST FEATURES LIST (77)\n",
        "# ==========================================================\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "# ==========================================================\n",
        "# 4. MAIN: per file -> compute all features -> vote -> top-N = 1 else 0\n",
        "# ==========================================================\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    # Voting config\n",
        "    K_PER_FEATURE = 6   # each feature votes for top-K indices\n",
        "    TOP_FINAL     = 12  # final anomalies = top-N voted indices\n",
        "\n",
        "    # Per-dataset aggregation\n",
        "    dataset_points = defaultdict(int)\n",
        "    dataset_events = defaultdict(int)\n",
        "    dataset_files  = defaultdict(int)\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            # ---- Build features for all windows ----\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ---- Majority voting across ALL features ----\n",
        "            votes = np.zeros(n, dtype=int)\n",
        "\n",
        "            for feat_name in FEATURE_NAMES:\n",
        "                series = pd.to_numeric(full.get(feat_name, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "                series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "                mx = float(np.max(vead_scores)) if len(vead_scores) else 0.0\n",
        "                if (not np.isfinite(mx)) or mx <= 0:\n",
        "                    continue\n",
        "\n",
        "                scores01 = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "                if np.max(scores01) <= 0:\n",
        "                    continue\n",
        "\n",
        "                k_eff = min(K_PER_FEATURE, n)\n",
        "                topk_idx = np.argpartition(scores01, -k_eff)[-k_eff:]\n",
        "                topk_idx = topk_idx[np.lexsort((topk_idx, -scores01[topk_idx]))]  # score desc, index asc\n",
        "                votes[topk_idx] += 1\n",
        "\n",
        "            # ---- Take top-N by vote count -> anomaly_score = 1 else 0 ----\n",
        "            final_scores = np.zeros(n, dtype=float)\n",
        "            if np.max(votes) > 0:\n",
        "                top_final_eff = min(TOP_FINAL, n)\n",
        "                order = np.lexsort((np.arange(n), -votes))  # votes desc, index asc\n",
        "                chosen = order[:top_final_eff]\n",
        "                final_scores[chosen] = 1.0\n",
        "\n",
        "            # ==========================================================\n",
        "            # ✅ NEW: PRINT anomaly counts for this file\n",
        "            # ==========================================================\n",
        "            mask = final_scores > 0\n",
        "            anomaly_points = int(mask.sum())\n",
        "            anomaly_events = _count_runs(mask)\n",
        "\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            cat_path = os.path.dirname(rel)\n",
        "            dataset = cat_path.split(os.sep)[0] if cat_path else \"root\"\n",
        "\n",
        "            print(f\"[SUMMARY] {rel} -> points={anomaly_points}, events={anomaly_events}, \"\n",
        "                  f\"vote_max={int(votes.max())}, voted_idx={(votes>0).sum()}\")\n",
        "\n",
        "            dataset_points[dataset] += anomaly_points\n",
        "            dataset_events[dataset] += anomaly_events\n",
        "            dataset_files[dataset]  += 1\n",
        "\n",
        "            # ---- Write output ----\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"-> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # ==========================================================\n",
        "    # ✅ NEW: PRINT totals per dataset folder\n",
        "    # ==========================================================\n",
        "    print(\"\\n=== DATASET TOTALS (this run) ===\")\n",
        "    for ds in sorted(dataset_files.keys()):\n",
        "        print(f\"{ds:25s} files={dataset_files[ds]:3d}  points={dataset_points[ds]:6d}  events={dataset_events[ds]:6d}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFb6CB9ZUWK3",
        "outputId": "44e8de50-86b5-4ab6-ce98-f2a11ce9a7ff",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (699/699), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 7119 (delta 552), reused 495 (delta 495), pack-reused 6420 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.13 MiB | 35.49 MiB/s, done.\n",
            "Resolving deltas: 100% (5001/5001), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 2. WRITING TDA_VEAD_METHOD (ENSEMBLE VOTING + COUNTS) ---\n",
            "✅ my_algo.py written.\n",
            "--- 3. RUNNING TDA_VEAD_Method (ENSEMBLE) ON ALL DATASETS ---\n",
            "Found 58 data files in 'data'\n",
            "[SUMMARY] realAdExchange/exchange-2_cpm_results.csv -> points=12, events=9, vote_max=36, voted_idx=69\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "[SUMMARY] realAdExchange/exchange-3_cpc_results.csv -> points=12, events=9, vote_max=35, voted_idx=67\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "[SUMMARY] realAdExchange/exchange-4_cpc_results.csv -> points=12, events=12, vote_max=36, voted_idx=79\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "[SUMMARY] realAdExchange/exchange-4_cpm_results.csv -> points=12, events=11, vote_max=35, voted_idx=76\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "[SUMMARY] realAdExchange/exchange-3_cpm_results.csv -> points=12, events=10, vote_max=36, voted_idx=71\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "[SUMMARY] realAdExchange/exchange-2_cpc_results.csv -> points=12, events=10, vote_max=37, voted_idx=74\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_CVS.csv -> points=12, events=10, vote_max=30, voted_idx=88\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_AAPL.csv -> points=12, events=12, vote_max=29, voted_idx=95\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_IBM.csv -> points=12, events=9, vote_max=22, voted_idx=98\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_UPS.csv -> points=12, events=11, vote_max=29, voted_idx=101\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_AMZN.csv -> points=12, events=9, vote_max=36, voted_idx=78\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_CRM.csv -> points=12, events=6, vote_max=35, voted_idx=81\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_PFE.csv -> points=12, events=11, vote_max=31, voted_idx=99\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_KO.csv -> points=12, events=9, vote_max=29, voted_idx=92\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_GOOG.csv -> points=12, events=11, vote_max=24, voted_idx=90\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "[SUMMARY] realTweets/Twitter_volume_FB.csv -> points=12, events=8, vote_max=33, voted_idx=76\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "[SUMMARY] artificialNoAnomaly/art_flatline.csv -> points=0, events=0, vote_max=0, voted_idx=0\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "[SUMMARY] artificialNoAnomaly/art_daily_perfect_square_wave.csv -> points=12, events=12, vote_max=21, voted_idx=43\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "[SUMMARY] artificialNoAnomaly/art_daily_no_noise.csv -> points=12, events=12, vote_max=17, voted_idx=55\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "[SUMMARY] artificialNoAnomaly/art_daily_small_noise.csv -> points=12, events=12, vote_max=23, voted_idx=99\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "[SUMMARY] artificialNoAnomaly/art_noisy.csv -> points=12, events=12, vote_max=37, voted_idx=97\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv -> points=12, events=9, vote_max=27, voted_idx=87\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "[SUMMARY] realAWSCloudwatch/elb_request_count_8c0756.csv -> points=12, events=12, vote_max=33, voted_idx=98\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv -> points=12, events=9, vote_max=36, voted_idx=74\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_network_in_5abac7.csv -> points=12, events=10, vote_max=25, voted_idx=98\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_network_in_257a54.csv -> points=12, events=6, vote_max=29, voted_idx=79\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "[SUMMARY] realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv -> points=12, events=9, vote_max=36, voted_idx=80\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv -> points=12, events=12, vote_max=25, voted_idx=102\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv -> points=12, events=9, vote_max=35, voted_idx=72\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "[SUMMARY] realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv -> points=12, events=8, vote_max=41, voted_idx=75\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv -> points=12, events=12, vote_max=28, voted_idx=109\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv -> points=12, events=11, vote_max=22, voted_idx=95\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "[SUMMARY] realAWSCloudwatch/grok_asg_anomaly.csv -> points=12, events=11, vote_max=28, voted_idx=94\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv -> points=12, events=10, vote_max=37, voted_idx=83\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv -> points=12, events=10, vote_max=27, voted_idx=112\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv -> points=12, events=12, vote_max=32, voted_idx=97\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "[SUMMARY] realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv -> points=12, events=5, vote_max=37, voted_idx=68\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "[SUMMARY] realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv -> points=12, events=12, vote_max=31, voted_idx=100\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "[SUMMARY] realKnownCause/rogue_agent_key_hold.csv -> points=12, events=11, vote_max=30, voted_idx=98\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "[SUMMARY] realKnownCause/ambient_temperature_system_failure.csv -> points=12, events=10, vote_max=36, voted_idx=78\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "[SUMMARY] realKnownCause/ec2_request_latency_system_failure.csv -> points=12, events=10, vote_max=35, voted_idx=81\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "[SUMMARY] realKnownCause/machine_temperature_system_failure.csv -> points=12, events=10, vote_max=31, voted_idx=77\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "[SUMMARY] realKnownCause/cpu_utilization_asg_misconfiguration.csv -> points=12, events=12, vote_max=28, voted_idx=88\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "[SUMMARY] realKnownCause/nyc_taxi.csv -> points=12, events=11, vote_max=34, voted_idx=81\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "[SUMMARY] realKnownCause/rogue_agent_key_updown.csv -> points=12, events=10, vote_max=30, voted_idx=79\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "[SUMMARY] artificialWithAnomaly/art_increase_spike_density.csv -> points=12, events=11, vote_max=33, voted_idx=74\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "[SUMMARY] artificialWithAnomaly/art_daily_jumpsup.csv -> points=12, events=12, vote_max=36, voted_idx=85\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "[SUMMARY] artificialWithAnomaly/art_daily_flatmiddle.csv -> points=12, events=12, vote_max=22, voted_idx=96\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "[SUMMARY] artificialWithAnomaly/art_daily_jumpsdown.csv -> points=12, events=12, vote_max=23, voted_idx=98\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "[SUMMARY] artificialWithAnomaly/art_load_balancer_spikes.csv -> points=12, events=11, vote_max=22, voted_idx=84\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "[SUMMARY] artificialWithAnomaly/art_daily_nojump.csv -> points=12, events=12, vote_max=23, voted_idx=94\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "[SUMMARY] realTraffic/occupancy_6005.csv -> points=12, events=11, vote_max=37, voted_idx=81\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "[SUMMARY] realTraffic/speed_6005.csv -> points=12, events=10, vote_max=37, voted_idx=91\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "[SUMMARY] realTraffic/speed_t4013.csv -> points=12, events=11, vote_max=33, voted_idx=93\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "[SUMMARY] realTraffic/TravelTime_451.csv -> points=12, events=12, vote_max=31, voted_idx=77\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "[SUMMARY] realTraffic/TravelTime_387.csv -> points=12, events=12, vote_max=28, voted_idx=71\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "[SUMMARY] realTraffic/speed_7578.csv -> points=12, events=9, vote_max=31, voted_idx=85\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "[SUMMARY] realTraffic/occupancy_t4013.csv -> points=12, events=12, vote_max=31, voted_idx=93\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "\n",
            "=== DATASET TOTALS (this run) ===\n",
            "artificialNoAnomaly       files=  5  points=    48  events=    48\n",
            "artificialWithAnomaly     files=  6  points=    72  events=    70\n",
            "realAWSCloudwatch         files= 17  points=   204  events=   167\n",
            "realAdExchange            files=  6  points=    72  events=    61\n",
            "realKnownCause            files=  7  points=    84  events=    74\n",
            "realTraffic               files=  7  points=    84  events=    77\n",
            "realTweets                files= 10  points=   120  events=    96\n",
            "--- 4. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -0.6050732105935437 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -35.047318925127115 with anomaly threshold 1.0.\n",
            "Optimizer found a max score of -33.605073210593545 with anomaly threshold 1.0.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 49.74\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 34.89\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 57.01\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title improvement of infinite death points + AVERAGE FEATURE ANOMALY SCORE (NO POINT SELECTION)\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN START & CLONE NAB\n",
        "# ============================================================\n",
        "print(\"--- 1. CLEAN START ---\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "if os.path.exists(\"NAB\"):\n",
        "    shutil.rmtree(\"NAB\")\n",
        "\n",
        "!git clone https://github.com/numenta/NAB.git\n",
        "!pip install -q ripser\n",
        "\n",
        "os.chdir(\"/content/NAB\")\n",
        "\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "thr_path = os.path.join(\"config\", \"thresholds.json\")\n",
        "if not os.path.exists(thr_path):\n",
        "    with open(thr_path, \"w\") as f:\n",
        "        f.write(\"{}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. WRITE TDA_VEAD_METHOD (my_algo.py)\n",
        "#    CHANGE: remove top-k voting & point selection\n",
        "#            output anomaly_score = average of per-feature normalized VEAD scores\n",
        "# ============================================================\n",
        "print(\"--- 2. WRITING TDA_VEAD_METHOD (AVERAGE FEATURE SCORES) ---\")\n",
        "\n",
        "tda_code = r\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DETECTOR_NAME = \"TDA_VEAD_Method\"\n",
        "INPUT_DIR = \"data\"\n",
        "OUTPUT_DIR = os.path.join(\"results\", DETECTOR_NAME)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Embedding parameters (fixed in NAB for fairness)\n",
        "# ----------------------------------------------------------\n",
        "WINDOW_SIZE = 14\n",
        "TAU         = 1\n",
        "DIMENSION   = 7\n",
        "_EPS        = 1e-12\n",
        "MAXDIM      = 1  # H0 + H1\n",
        "\n",
        "# ==========================================================\n",
        "# 0. VEAD CONFIGURATION\n",
        "# ==========================================================\n",
        "KV   = 3.5\n",
        "KA   = 3.5\n",
        "MODE = \"abs_plateau\"  # \"strict\" | \"plateau\" | \"abs_plateau\"\n",
        "\n",
        "def _vead_series(raw_vals, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(raw_vals, dtype=float), errors=\"coerce\") \\\n",
        "            .interpolate(limit_direction=\"both\")\n",
        "\n",
        "    v = s.diff(1)\n",
        "    a = v.diff(1)\n",
        "\n",
        "    def _zmad(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zmad(v.values)\n",
        "    za = _zmad(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv)\n",
        "        za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv)\n",
        "        za = np.abs(za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ==========================================================\n",
        "# 1. TAKENS EMBEDDING\n",
        "# ==========================================================\n",
        "def takens_embed(window, time_delay, dimension):\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens parameters too large for this window.\")\n",
        "    return np.stack(\n",
        "        [window[j:j + m * time_delay:time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# ==========================================================\n",
        "# 2. PERSISTENCE DIAGRAM UTILITIES + FEATURE FUNCTIONS\n",
        "# ==========================================================\n",
        "def _clean_diag(diag):\n",
        "\n",
        "    if diag is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    arr = np.asarray(diag, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "\n",
        "    # keep finite births; allow finite deaths OR +inf deaths\n",
        "    mask = np.isfinite(b) & (np.isfinite(d) | np.isposinf(d))\n",
        "    if not np.any(mask):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    b = b[mask]\n",
        "    d = d[mask]\n",
        "\n",
        "    # per-window dmax from finite deaths\n",
        "    finite_d = d[np.isfinite(d)]\n",
        "    if finite_d.size > 0:\n",
        "        dmax = float(np.max(finite_d))\n",
        "    else:\n",
        "        # fallback: if everything is +inf, set something > max birth\n",
        "        dmax = float(np.max(b) + 1.0)\n",
        "\n",
        "    # cap +inf deaths\n",
        "    d = np.where(np.isposinf(d), dmax, d)\n",
        "\n",
        "    # remove degenerate points\n",
        "    good = np.isfinite(b) & np.isfinite(d) & (d > b)\n",
        "    if not np.any(good):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "\n",
        "    return np.stack([b[good], d[good]], axis=1)\n",
        "\n",
        "def _lifetimes(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return np.empty(0, dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return float(a) / float(b + _EPS)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "def _auc_tri_max(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b_all, d_all = arr[:, 0], arr[:, 1]\n",
        "    if b_all.min() == d_all.max():\n",
        "        return 0.0\n",
        "\n",
        "    grid = np.linspace(b_all.min(), d_all.max(), 64)\n",
        "    lam1 = np.zeros_like(grid)\n",
        "\n",
        "    for b, d in arr:\n",
        "        m = 0.5 * (b + d)\n",
        "        h = 0.5 * (d - b)\n",
        "        if h <= 0:\n",
        "            continue\n",
        "\n",
        "        left = (grid >= b) & (grid <= m)\n",
        "        right = (grid >= m) & (grid <= d)\n",
        "\n",
        "        lam1[left] = np.maximum(lam1[left], (grid[left] - b) * (h / max(m - b, _EPS)))\n",
        "        lam1[right] = np.maximum(lam1[right], (d - grid[right]) * (h / max(d - m, _EPS)))\n",
        "\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "def _persistence_entropy(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    p = L / (S + _EPS)\n",
        "    return float(-np.sum(p * np.log(p + _EPS)))\n",
        "\n",
        "def _gini_from_lifetimes(L):\n",
        "    L = np.sort(L)\n",
        "    n = len(L)\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    cumL = np.cumsum(L)\n",
        "    return float(1 + 1/n - 2*np.sum(cumL/(n*S)))\n",
        "\n",
        "def _tail_share_q(diag, q):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    qv = np.quantile(L, q)\n",
        "    return _safe_div(L[L >= qv].sum(), L.sum())\n",
        "\n",
        "def _birth_death_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_birth\": 0.0, \"mean_death\": 0.0, \"std_birth\": 0.0, \"std_death\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    return {\n",
        "        \"mean_birth\": float(b.mean()),\n",
        "        \"mean_death\": float(d.mean()),\n",
        "        \"std_birth\": float(b.std(ddof=0)),\n",
        "        \"std_death\": float(d.std(ddof=0)),\n",
        "    }\n",
        "\n",
        "def _diag_distance_stats(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean_diag_dist\": 0.0, \"max_diag_dist\": 0.0, \"sum_diag_dist\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    dist = (d - b) / np.sqrt(2.0)\n",
        "    return {\n",
        "        \"mean_diag_dist\": float(dist.mean()),\n",
        "        \"max_diag_dist\": float(dist.max()),\n",
        "        \"sum_diag_dist\": float(dist.sum()),\n",
        "    }\n",
        "\n",
        "def _centroid_xy(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {\"centroid_x\": 0.0, \"centroid_y\": 0.0}\n",
        "    return {\n",
        "        \"centroid_x\": float(np.sum(b * L) / (S + _EPS)),\n",
        "        \"centroid_y\": float(np.sum(d * L) / (S + _EPS)),\n",
        "    }\n",
        "\n",
        "def _lifetimes_stats(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\n",
        "            \"count\": 0, \"sum\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"std\": 0.0,\n",
        "            \"min\": 0.0, \"max\": 0.0, \"L1\": 0.0, \"L2\": 0.0, \"Linf\": 0.0\n",
        "        }\n",
        "    return {\n",
        "        \"count\": int(L.size),\n",
        "        \"sum\": float(L.sum()),\n",
        "        \"mean\": float(L.mean()),\n",
        "        \"median\": float(np.median(L)),\n",
        "        \"std\": float(L.std(ddof=0)),\n",
        "        \"min\": float(L.min()),\n",
        "        \"max\": float(L.max()),\n",
        "        \"L1\": float(np.sum(np.abs(L))),\n",
        "        \"L2\": float(np.sqrt(np.sum(L**2))),\n",
        "        \"Linf\": float(np.max(np.abs(L))),\n",
        "    }\n",
        "\n",
        "def _lifetimes_quantiles(diag):\n",
        "    L = _lifetimes(diag)\n",
        "    if L.size == 0:\n",
        "        return {\"q50\": 0.0, \"q75\": 0.0, \"q90\": 0.0, \"q95\": 0.0, \"q99\": 0.0}\n",
        "    return {\n",
        "        \"q50\": float(np.quantile(L, 0.50)),\n",
        "        \"q75\": float(np.quantile(L, 0.75)),\n",
        "        \"q90\": float(np.quantile(L, 0.90)),\n",
        "        \"q95\": float(np.quantile(L, 0.95)),\n",
        "        \"q99\": float(np.quantile(L, 0.99)),\n",
        "    }\n",
        "\n",
        "def _carlsson_coordinates(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return {f\"f{k}\": 0.0 for k in range(1, 6)}\n",
        "    return {\n",
        "        \"f1\": float(L.sum()),\n",
        "        \"f2\": float(np.sum(b * L)),\n",
        "        \"f3\": float(np.sum(d * L)),\n",
        "        \"f4\": float(np.sum(b**2 * L)),\n",
        "        \"f5\": float(np.sum(d**2 * L)),\n",
        "    }\n",
        "\n",
        "def _sum_centroid_radial(diag):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum(np.abs(radial) * L), S)\n",
        "\n",
        "def _pete(diag, p=1.6, q=0.5):\n",
        "    arr = _clean_diag(diag)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0)\n",
        "    S = L.sum()\n",
        "    if S <= 0:\n",
        "        return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    return _safe_div(np.sum((L**p) * (np.abs(radial)**q)), S)\n",
        "\n",
        "def compute_features_for_diag(diag, prefix):\n",
        "    feats = {}\n",
        "\n",
        "    Ls = _lifetimes_stats(diag)\n",
        "    feats[f\"{prefix}count_lifetime\"] = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}sum_lifetime\"]   = float(Ls[\"sum\"])\n",
        "    feats[f\"{prefix}mean_lifetime\"]  = float(Ls[\"mean\"])\n",
        "    feats[f\"{prefix}median_lifetime\"]= float(Ls[\"median\"])\n",
        "    feats[f\"{prefix}std_lifetime\"]   = float(Ls[\"std\"])\n",
        "    feats[f\"{prefix}min_lifetime\"]   = float(Ls[\"min\"])\n",
        "    feats[f\"{prefix}max_lifetime\"]   = float(Ls[\"max\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_lifetime\"]    = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_lifetime\"]    = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_lifetime\"]  = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}L1_norm\"]        = float(Ls[\"L1\"])\n",
        "    feats[f\"{prefix}L2_norm\"]        = float(Ls[\"L2\"])\n",
        "    feats[f\"{prefix}Linf_norm\"]      = float(Ls[\"Linf\"])\n",
        "\n",
        "    feats[f\"{prefix}betti\"]          = float(Ls[\"count\"])\n",
        "    feats[f\"{prefix}energy_concentration\"] = _safe_div(Ls[\"L2\"], Ls[\"L1\"])\n",
        "    feats[f\"{prefix}dominance_share\"]      = _safe_div(Ls[\"Linf\"], Ls[\"L1\"])\n",
        "\n",
        "    feats[f\"{prefix}persistence_entropy\"]  = _persistence_entropy(diag)\n",
        "\n",
        "    bd = _birth_death_stats(diag)\n",
        "    for k, v in bd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    dd = _diag_distance_stats(diag)\n",
        "    for k, v in dd.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    cxy = _centroid_xy(diag)\n",
        "    feats[f\"{prefix}centroid_x\"] = float(cxy[\"centroid_x\"])\n",
        "    feats[f\"{prefix}centroid_y\"] = float(cxy[\"centroid_y\"])\n",
        "\n",
        "    q = _lifetimes_quantiles(diag)\n",
        "    for k, v in q.items():\n",
        "        feats[f\"{prefix}{k}\"] = float(v)\n",
        "\n",
        "    tail80 = _tail_share_q(diag, 0.80)\n",
        "    tail90 = _tail_share_q(diag, 0.90)\n",
        "    tail95 = _tail_share_q(diag, 0.95)\n",
        "\n",
        "    feats[f\"{prefix}tail_share_q80\"] = float(tail80)\n",
        "    feats[f\"{prefix}tail_share_q90\"] = float(tail90)\n",
        "    feats[f\"{prefix}tail_share_q95\"] = float(tail95)\n",
        "    feats[f\"{prefix}tail_curvature_80_90\"] = float(tail90 - tail80)\n",
        "\n",
        "    L = _lifetimes(diag)\n",
        "    feats[f\"{prefix}gini\"] = float(_gini_from_lifetimes(L))\n",
        "\n",
        "    cc = _carlsson_coordinates(diag)\n",
        "    feats[f\"{prefix}Carlsson_f1\"] = float(cc[\"f1\"])\n",
        "    feats[f\"{prefix}Carlsson_f2\"] = float(cc[\"f2\"])\n",
        "    feats[f\"{prefix}Carlsson_f3\"] = float(cc[\"f3\"])\n",
        "    feats[f\"{prefix}Carlsson_f4\"] = float(cc[\"f4\"])\n",
        "    feats[f\"{prefix}Carlsson_f5\"] = float(cc[\"f5\"])\n",
        "\n",
        "    if prefix == \"H0_\":\n",
        "        A = _auc_tri_max(diag)\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"] = _safe_div(A, Ls[\"sum\"])\n",
        "        feats[\"H0_ratio_auc_to_max\"]    = _safe_div(A, Ls[\"max\"])\n",
        "        feats[\"H0_ratio_auc_to_l2\"]     = _safe_div(A, Ls[\"L2\"])\n",
        "        feats[\"H0_bottleneck\"]          = float(Ls[\"max\"])\n",
        "        feats[\"H0_sum_centroid\"]        = float(_sum_centroid_radial(diag))\n",
        "        feats[\"PETE_p1.6_q0.5\"]         = float(_pete(diag, p=1.6, q=0.5))\n",
        "        feats[\"H0_energy_concentration\"]= _safe_div(Ls[\"L2\"], Ls[\"sum\"])\n",
        "        feats[\"H0_dominance_share\"]     = _safe_div(Ls[\"Linf\"], Ls[\"sum\"])\n",
        "        feats[\"H0_tail_curvature_80_90\"]= float(tail90 - tail80)\n",
        "        feats[\"H0_centroid_to_energy\"]  = _safe_div(feats[\"H0_sum_centroid\"], Ls[\"L2\"])\n",
        "        feats[\"H0_gini\"]                = float(feats[\"H0_gini\"])\n",
        "    return feats\n",
        "\n",
        "def compute_cross_dim_features(feats_H0, feats_H1):\n",
        "    out = {}\n",
        "    def g(d, k): return float(d.get(k, 0.0))\n",
        "    out[\"H1_to_H0_betti_ratio\"]   = _safe_div(g(feats_H1, \"H1_betti\"), g(feats_H0, \"H0_betti\"))\n",
        "    out[\"H1_to_H0_entropy_ratio\"] = _safe_div(g(feats_H1, \"H1_persistence_entropy\"), g(feats_H0, \"H0_persistence_entropy\"))\n",
        "    return out\n",
        "\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_Carlsson_f1\",\"H0_Carlsson_f3\",\"H0_Carlsson_f5\",\n",
        "    \"H0_L1_lifetime\",\"H0_L1_norm\",\"H0_L2_lifetime\",\"H0_L2_norm\",\n",
        "    \"H0_Linf_lifetime\",\"H0_Linf_norm\",\"H0_bottleneck\",\"H0_centroid_to_energy\",\n",
        "    \"H0_centroid_y\",\"H0_dominance_share\",\"H0_energy_concentration\",\"H0_gini\",\n",
        "    \"H0_max_diag_dist\",\"H0_max_lifetime\",\"H0_mean_death\",\"H0_mean_diag_dist\",\n",
        "    \"H0_mean_lifetime\",\"H0_median_lifetime\",\"H0_min_lifetime\",\"H0_persistence_entropy\",\n",
        "    \"H0_q50\",\"H0_q75\",\"H0_q90\",\"H0_q95\",\"H0_q99\",\"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_l2\",\"H0_ratio_auc_to_max\",\"H0_std_death\",\"H0_std_lifetime\",\n",
        "    \"H0_sum_centroid\",\"H0_sum_diag_dist\",\"H0_sum_lifetime\",\"H0_tail_curvature_80_90\",\n",
        "    \"H0_tail_share_q80\",\"H0_tail_share_q90\",\"H0_tail_share_q95\",\n",
        "    \"H1_Carlsson_f1\",\"H1_Carlsson_f2\",\"H1_Carlsson_f3\",\n",
        "    \"H1_L1_lifetime\",\"H1_L1_norm\",\"H1_L2_lifetime\",\"H1_L2_norm\",\n",
        "    \"H1_Linf_lifetime\",\"H1_Linf_norm\",\"H1_betti\",\"H1_count_lifetime\",\n",
        "    \"H1_dominance_share\",\"H1_energy_concentration\",\"H1_gini\",\n",
        "    \"H1_max_diag_dist\",\"H1_max_lifetime\",\"H1_mean_diag_dist\",\"H1_mean_lifetime\",\n",
        "    \"H1_median_lifetime\",\"H1_min_lifetime\",\"H1_persistence_entropy\",\n",
        "    \"H1_q50\",\"H1_q75\",\"H1_q90\",\"H1_q95\",\"H1_q99\",\n",
        "    \"H1_std_birth\",\"H1_std_death\",\"H1_std_lifetime\",\n",
        "    \"H1_sum_diag_dist\",\"H1_sum_lifetime\",\n",
        "    \"H1_tail_share_q80\",\"H1_tail_share_q90\",\"H1_tail_share_q95\",\n",
        "    \"H1_to_H0_betti_ratio\",\"H1_to_H0_entropy_ratio\",\n",
        "    \"PETE_p1.6_q0.5\"\n",
        "]\n",
        "\n",
        "def run():\n",
        "    files = glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True)\n",
        "    print(f\"Found {len(files)} data files in '{INPUT_DIR}'\")\n",
        "\n",
        "    for filepath in files:\n",
        "        if \".ipynb_checkpoints\" in filepath:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            df.columns = [c.strip().lower() for c in df.columns]\n",
        "            if \"value\" not in df.columns or \"timestamp\" not in df.columns:\n",
        "                continue\n",
        "\n",
        "            vals = pd.to_numeric(df[\"value\"], errors=\"coerce\").astype(float).to_numpy()\n",
        "            n = len(vals)\n",
        "\n",
        "            # ---- Build features for all windows ----\n",
        "            rows = []\n",
        "            for i in range(WINDOW_SIZE - 1, n):\n",
        "                w = vals[i - WINDOW_SIZE + 1 : i + 1]\n",
        "                try:\n",
        "                    emb = takens_embed(w, TAU, DIMENSION)\n",
        "                    dgms = ripser(emb, maxdim=MAXDIM)[\"dgms\"]\n",
        "                except Exception:\n",
        "                    dgms = [np.empty((0, 2)), np.empty((0, 2))]\n",
        "\n",
        "                D0 = dgms[0] if len(dgms) > 0 else np.empty((0, 2))\n",
        "                D1 = dgms[1] if (MAXDIM >= 1 and len(dgms) > 1) else np.empty((0, 2))\n",
        "\n",
        "                feats_H0 = compute_features_for_diag(D0, \"H0_\")\n",
        "                feats_H1 = compute_features_for_diag(D1, \"H1_\")\n",
        "                cross    = compute_cross_dim_features(feats_H0, feats_H1)\n",
        "\n",
        "                merged = {}\n",
        "                merged.update(feats_H0)\n",
        "                merged.update(feats_H1)\n",
        "                merged.update(cross)\n",
        "                merged[\"index\"] = i\n",
        "                rows.append(merged)\n",
        "\n",
        "            feat_df = pd.DataFrame(rows)\n",
        "            full = pd.DataFrame(index=np.arange(n))\n",
        "            if not feat_df.empty:\n",
        "                feat_df = feat_df.set_index(\"index\")\n",
        "                full = full.join(feat_df, how=\"left\")\n",
        "\n",
        "            full = full.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            # ==========================================================\n",
        "            # NEW: anomaly_score = average of normalized per-feature VEAD scores\n",
        "            # ==========================================================\n",
        "            sum_scores = np.zeros(n, dtype=float)\n",
        "            cnt_scores = np.zeros(n, dtype=int)\n",
        "\n",
        "            for feat_name in FEATURE_NAMES:\n",
        "                series = pd.to_numeric(full.get(feat_name, 0.0), errors=\"coerce\").astype(float).to_numpy()\n",
        "                series = np.nan_to_num(series, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                vead_scores = _vead_series(series, kv=KV, ka=KA, mode=MODE)\n",
        "\n",
        "                mx = float(np.max(vead_scores)) if len(vead_scores) else 0.0\n",
        "                if (not np.isfinite(mx)) or mx <= 0:\n",
        "                    continue\n",
        "\n",
        "                scores01 = np.clip(vead_scores / mx, 0.0, 1.0)\n",
        "\n",
        "                # accumulate for average\n",
        "                sum_scores += scores01\n",
        "                cnt_scores += 1\n",
        "\n",
        "            # final average (if no valid features, all zeros)\n",
        "            final_scores = np.zeros(n, dtype=float)\n",
        "            good = cnt_scores > 0\n",
        "            final_scores[good] = sum_scores[good] / cnt_scores[good]\n",
        "\n",
        "            # ---- Write output ----\n",
        "            rel = os.path.relpath(filepath, INPUT_DIR)\n",
        "            category = os.path.dirname(rel)\n",
        "            base_name = os.path.basename(rel)\n",
        "\n",
        "            out_dir = os.path.join(OUTPUT_DIR, category)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_name = f\"{DETECTOR_NAME}_\" + base_name\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "\n",
        "            out_df = pd.DataFrame({\n",
        "                \"timestamp\": df[\"timestamp\"],\n",
        "                \"anomaly_score\": final_scores\n",
        "            })\n",
        "            out_df.to_csv(out_path, index=False)\n",
        "            print(f\"-> Wrote: {out_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! Error processing {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"my_algo.py\", \"w\") as f:\n",
        "    f.write(tda_code)\n",
        "\n",
        "print(\"✅ my_algo.py written.\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. RUN YOUR DETECTOR ON ALL NAB DATA\n",
        "# ============================================================\n",
        "print(\"--- 3. RUNNING TDA_VEAD_Method (AVERAGE FEATURE SCORES) ON ALL DATASETS ---\")\n",
        "!python my_algo.py\n",
        "\n",
        "# ============================================================\n",
        "# 4. RUN NAB OPTIMIZE + SCORE FOR THIS DETECTOR\n",
        "# ============================================================\n",
        "print(\"--- 4. RUNNING NAB OPTIMIZE + SCORE ---\")\n",
        "!python run.py --optimize --score --detectors TDA_VEAD_Method --normalize\n"
      ],
      "metadata": {
        "id": "OcXxe5cfWS7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "outputId": "3ed8fa8e-7dc0-4ecb-bf1d-b3c71230f4ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. CLEAN START ---\n",
            "Cloning into 'NAB'...\n",
            "remote: Enumerating objects: 7119, done.\u001b[K\n",
            "remote: Counting objects: 100% (713/713), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 7119 (delta 601), reused 545 (delta 545), pack-reused 6406 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7119/7119), 86.73 MiB | 39.81 MiB/s, done.\n",
            "Resolving deltas: 100% (5015/5015), done.\n",
            "Updating files: 100% (1186/1186), done.\n",
            "--- 2. WRITING TDA_VEAD_METHOD (AVERAGE FEATURE SCORES) ---\n",
            "✅ my_algo.py written.\n",
            "--- 3. RUNNING TDA_VEAD_Method (AVERAGE FEATURE SCORES) ON ALL DATASETS ---\n",
            "Found 58 data files in 'data'\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-4_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-3_cpm_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAdExchange/TDA_VEAD_Method_exchange-2_cpc_results.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CVS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AAPL.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_IBM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_UPS.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_AMZN.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_CRM.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_PFE.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_KO.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_GOOG.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTweets/TDA_VEAD_Method_Twitter_volume_FB.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_flatline.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_perfect_square_wave.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_no_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_daily_small_noise.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialNoAnomaly/TDA_VEAD_Method_art_noisy.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_1ef3de.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_elb_request_count_8c0756.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_ac20cd.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_5abac7.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_network_in_257a54.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_cc0c53.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_77c1ca.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_825cc2.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_24ae8d.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_disk_write_bytes_c0d644.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_grok_asg_anomaly.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_5f5533.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_c6585a.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_fe7f93.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_rds_cpu_utilization_e47b3b.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realAWSCloudwatch/TDA_VEAD_Method_ec2_cpu_utilization_53ea38.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_hold.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ambient_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_ec2_request_latency_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_machine_temperature_system_failure.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_cpu_utilization_asg_misconfiguration.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_nyc_taxi.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realKnownCause/TDA_VEAD_Method_rogue_agent_key_updown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_increase_spike_density.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsup.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_flatmiddle.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_jumpsdown.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_load_balancer_spikes.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/artificialWithAnomaly/TDA_VEAD_Method_art_daily_nojump.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_6005.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_t4013.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_451.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_TravelTime_387.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_speed_7578.csv\n",
            "-> Wrote: results/TDA_VEAD_Method/realTraffic/TDA_VEAD_Method_occupancy_t4013.csv\n",
            "--- 4. RUNNING NAB OPTIMIZE + SCORE ---\n",
            "{'dataDir': 'data',\n",
            " 'detect': False,\n",
            " 'detectors': ['TDA_VEAD_Method'],\n",
            " 'normalize': True,\n",
            " 'numCPUs': None,\n",
            " 'optimize': True,\n",
            " 'profilesFile': 'config/profiles.json',\n",
            " 'resultsDir': 'results',\n",
            " 'score': True,\n",
            " 'skipConfirmation': False,\n",
            " 'thresholdsFile': 'config/thresholds.json',\n",
            " 'windowsFile': 'labels/combined_windows.json'}\n",
            "Proceed? (y/n): y\n",
            "\n",
            "Running optimize step\n",
            "Optimizer found a max score of -39.60979367592889 with anomaly threshold 0.2379744535180599.\n",
            "Optimizer found a max score of -75.00399710671145 with anomaly threshold 0.3077381913004652.\n",
            "Optimizer found a max score of -88.69573467377447 with anomaly threshold 0.2049836291012177.\n",
            "\n",
            "Running scoring step\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_standard_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FP_rate_scores.csv\n",
            "TDA_VEAD_Method detector benchmark scores written to /content/NAB/results/TDA_VEAD_Method/TDA_VEAD_Method_reward_low_FN_rate_scores.csv\n",
            "\n",
            "Running score normalization step\n",
            "Final score for 'TDA' detector on 'VEAD_Method_standard' profile = 32.93\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FP_rate' profile = 17.67\n",
            "Final score for 'TDA' detector on 'VEAD_Method_reward_low_FN_rate' profile = 41.18\n",
            "Final scores have been written to /content/NAB/results/final_results.json.\n"
          ]
        }
      ]
    }
  ]
}