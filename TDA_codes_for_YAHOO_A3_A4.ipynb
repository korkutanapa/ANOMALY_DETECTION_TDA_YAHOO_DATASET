{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "8O12D8IO8HQX"
      ],
      "authorship_tag": "ABX9TyOLe6neFdOvR6wiHtUuFk8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korkutanapa/ANOMALY_DETECTION_TDA_YAHOO_DATASET/blob/main/TDA_codes_for_YAHOO_A3_A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FULL CODE FOR UNSUPERVISED ANOMALY DETECTION VIA TDA"
      ],
      "metadata": {
        "id": "6FulMgvf8ilJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAWh57YHKz-I",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title necessary libraries\n",
        "# Install\n",
        "!pip install -U \"numpy>=2.0,<2.3\" \"scipy>=1.11\" ripser persim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title necessary libraries\n",
        "!pip install ruptures"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "4qY5P2LqFQqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN CODE TDA AND VAAD"
      ],
      "metadata": {
        "id": "1UHY1qGb8Pwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title segmentation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ruptures as rpt\n",
        "import sys\n",
        "\n",
        "# Define the file path pattern\n",
        "DATA_PATTERN = \"/content/A3Benchmark-TS{ts_id}.csv\"\n",
        "window_size = 100\n",
        "\n",
        "print(f\"Starting batch changepoint detection for 100 files...\")\n",
        "print(f\"Using window size: {window_size}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Loop from 1 to 100 (inclusive)\n",
        "for ts_id in range(1, 101):\n",
        "    filepath = DATA_PATTERN.format(ts_id=ts_id)\n",
        "    print(f\"\\nProcessing file: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        # --- Load the dataset ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # --- 1. Calculate Rolling Statistics ---\n",
        "        df['rolling_mean'] = df['value'].rolling(window=window_size, center=True).mean()\n",
        "        df['rolling_variance'] = df['value'].rolling(window=window_size, center=True).var()\n",
        "\n",
        "        # --- 2. Run Changepoint Detection ---\n",
        "\n",
        "        # Create the signal from rolling variance\n",
        "        signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n",
        "\n",
        "        # Check for bad signal (e.g., file shorter than window size)\n",
        "        if np.all(np.isnan(signal)):\n",
        "            print(\"Signal is all NaN. Skipping detection.\")\n",
        "            continue\n",
        "\n",
        "        # Use 'Pelt' algorithm\n",
        "        algo = rpt.Pelt(model=\"l2\").fit(signal)\n",
        "\n",
        "        # Define a penalty value\n",
        "        n_samples = len(signal)\n",
        "        sigma_sq = np.nanvar(signal) # Use nan-safe variance\n",
        "\n",
        "        # Avoid division by zero or invalid penalty if variance is 0\n",
        "        if sigma_sq == 0:\n",
        "            print(\"Signal variance is zero. Skipping detection.\")\n",
        "            continue\n",
        "\n",
        "        # Using the 50* multiplier from your code\n",
        "        penalty_value = 50 * np.log(n_samples) * sigma_sq\n",
        "\n",
        "        # Predict the changepoints using the penalty\n",
        "        new_changepoints_idx = algo.predict(pen=penalty_value)\n",
        "\n",
        "        # Clean up the list (remove the end-of-signal index)\n",
        "        new_changepoints = [idx for idx in new_changepoints_idx if idx < len(signal)]\n",
        "\n",
        "        # --- 3. Print Ground Truth and Detection Results ---\n",
        "\n",
        "        # Get the original changepoints from the file\n",
        "        original_changepoints = []\n",
        "        if 'changepoint' in df.columns:\n",
        "            original_changepoints = df.index[df['changepoint'] == 1].tolist()\n",
        "\n",
        "        print(\"\\n--- Detection Results ---\")\n",
        "        print(f\"Original 'Ground Truth' changepoints: {original_changepoints}\")\n",
        "        print(f\"Found {len(new_changepoints)} new changepoints: {new_changepoints}\")\n",
        "\n",
        "        # --- 4. Add new column and Overwrite file ---\n",
        "        df['new_cp'] = 0\n",
        "        df.loc[new_changepoints, 'new_cp'] = 1\n",
        "\n",
        "        print(f\"Updating and overwriting {filepath}...\")\n",
        "        # Save the file, overwriting the original. index=False stops\n",
        "        # pandas from adding an extra 'Unnamed: 0' column.\n",
        "        df.to_csv(filepath, index=False)\n",
        "        print(\"File update complete.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filepath}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        # Catch any other errors during processing\n",
        "        print(f\"An error occurred processing {filepath}: {e}\", file=sys.stderr)\n",
        "        print(\"Skipping this file.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Batch processing complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "3KDcwQmcOOCL",
        "outputId": "ee0b7dd9-8160-4c0e-f4ed-2fa59a495509"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting batch changepoint detection for 100 files...\n",
            "Using window size: 100\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS1.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS2.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS3.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS3.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS4.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS4.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS5.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS5.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS6.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS6.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS7.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS7.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS8.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS8.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS9.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS9.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS10.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS10.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS11.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 1 new changepoints: [200]\n",
            "Updating and overwriting /content/A3Benchmark-TS11.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS12.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS12.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS13.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS13.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS14.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS14.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS15.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS15.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS16.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS16.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS17.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS17.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS18.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS18.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS19.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS19.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS20.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS20.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS21.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS21.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS22.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS22.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS23.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS23.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS24.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS24.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS25.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS25.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS26.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS26.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS27.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS27.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS28.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS28.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS29.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS29.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS30.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS30.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS31.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 2 new changepoints: [660, 765]\n",
            "Updating and overwriting /content/A3Benchmark-TS31.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS32.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS32.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS33.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS33.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS34.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS34.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS35.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS35.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS36.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS36.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS37.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS37.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS38.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS38.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS39.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS39.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS40.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS40.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS41.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS41.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS42.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS42.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS43.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS43.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS44.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS44.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS45.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS45.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS46.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS46.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS47.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS47.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS48.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS48.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS49.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS49.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS50.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS50.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS51.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS51.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS52.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 1 new changepoints: [840]\n",
            "Updating and overwriting /content/A3Benchmark-TS52.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS53.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS53.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS54.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS54.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS55.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS55.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS56.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS56.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS57.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS57.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS58.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS58.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS59.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS59.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS60.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS60.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS61.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS61.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS62.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS62.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS63.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS63.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS64.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 1 new changepoints: [1580]\n",
            "Updating and overwriting /content/A3Benchmark-TS64.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS65.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS65.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS66.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS66.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS67.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS67.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS68.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS68.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS69.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS69.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS70.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS70.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS71.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS71.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS72.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS72.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS73.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS73.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS74.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS74.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS75.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS75.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS76.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS76.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS77.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS77.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS78.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 2 new changepoints: [135, 1555]\n",
            "Updating and overwriting /content/A3Benchmark-TS78.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS79.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS79.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS80.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS80.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS81.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 1 new changepoints: [985]\n",
            "Updating and overwriting /content/A3Benchmark-TS81.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS82.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS82.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS83.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 1 new changepoints: [1110]\n",
            "Updating and overwriting /content/A3Benchmark-TS83.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS84.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS84.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS85.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS85.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS86.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS86.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS87.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS87.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS88.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS88.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS89.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 1 new changepoints: [245]\n",
            "Updating and overwriting /content/A3Benchmark-TS89.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS90.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS90.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS91.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS91.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS92.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS92.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS93.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS93.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS94.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS94.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS95.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS95.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS96.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS96.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS97.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS97.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS98.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS98.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS99.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS99.csv...\n",
            "File update complete.\n",
            "\n",
            "Processing file: /content/A3Benchmark-TS100.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-209269173.py:31: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  signal = df['rolling_variance'].fillna(method='bfill').fillna(method='ffill').values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detection Results ---\n",
            "Original 'Ground Truth' changepoints: []\n",
            "Found 0 new changepoints: []\n",
            "Updating and overwriting /content/A3Benchmark-TS100.csv...\n",
            "File update complete.\n",
            "--------------------------------------------------\n",
            "Batch processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TDA Feature Extractor (RAW values only  no VAAD/scoring)\n",
        "# Reads A3Benchmark-TS{1..100}.csv, computes H0-based features per sliding window,\n",
        "# aligns each window to its right-edge point, and saves raw feature values to CSV.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "\n",
        "# ---------------- Tunables ----------------\n",
        "DATA_PATTERN = \"/content/A3Benchmark-TS{ts_id}.csv\"   # change if needed\n",
        "OUT_CSV      = \"/content/tda_features_a3.csv\"         # output CSV with RAW feature values\n",
        "\n",
        "VALUE_COL = \"value\"\n",
        "LABEL_COL = \"anomaly\"\n",
        "CP_CANDIDATES = [\"change_point\", \"changepoint\", \"cp\", \"cp_flag\"]\n",
        "\n",
        "# Sliding window + Takens\n",
        "window_size = 14\n",
        "time_delay  = 1\n",
        "dimension   = 7\n",
        "assert window_size - (dimension - 1) * time_delay > 0, \\\n",
        "    \"Choose smaller time_delay*dimension or larger window_size.\"\n",
        "\n",
        "# Which TDA feature streams to export (column names will match these keys)\n",
        "FEATURE_NAMES = [\n",
        "    \"H0_ratio_auc_L1_to_sum\",\n",
        "    \"H0_ratio_auc_to_max\",\n",
        "    \"H0_ratio_auc_to_l2\",\n",
        "\n",
        "    \"H0_bottleneck\",\n",
        "    \"tail_share_q90\",\n",
        "    \"H0_sum_centroid\",\n",
        "    \"H0_L2_norm\",\n",
        "    \"PETE_p1.6_q0.5\",\n",
        "\n",
        "    \"H0_energy_concentration\",   # L2 / sumL\n",
        "    \"H0_dominance_share\",        # maxL / sumL\n",
        "    \"H0_tail_curvature_80_90\",   # tail90 - tail80\n",
        "    \"H0_centroid_to_energy\",     # centroid / L2\n",
        "    \"H0_gini\",                   # inequality of lifetimes\n",
        "]\n",
        "\n",
        "# ------------- Utilities (TDA + features) -------------\n",
        "def takens_embed(window: np.ndarray, time_delay: int, dimension: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return Takens embedding with shape (m, dimension): m points  embedding dimension.\n",
        "    \"\"\"\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens params too large for this window_size.\")\n",
        "    # Each slice has length m; stacking along axis=1 yields (m, dimension)\n",
        "    return np.stack(\n",
        "        [window[j : j + m * time_delay : time_delay] for j in range(dimension)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "def _clean_diag_h(diag_h):\n",
        "    if diag_h is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag_h, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    finite_mask = np.isfinite(arr).all(axis=1)\n",
        "    arr = arr[finite_mask]\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    ok = np.isfinite(d) & (d > b)\n",
        "    if not np.any(ok):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[ok], d[ok]], axis=1)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "_EPS = 1e-12\n",
        "def _safe_div(a, b, eps=_EPS): return float(a) / float(b + eps)\n",
        "\n",
        "def _build_grid_for_time_funcs(arr, n_grid=128):\n",
        "    if arr.size == 0: return np.linspace(0.0, 1.0, num=n_grid)\n",
        "    lo = float(np.min(arr[:, 0])); hi = float(np.max(arr[:, 1]))\n",
        "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo: lo, hi = 0.0, 1.0\n",
        "    return np.linspace(lo, hi, num=n_grid)\n",
        "\n",
        "def _lifetimes(arr):\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0,), dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _bottleneck_amp(diag_h):\n",
        "    \"\"\"Max lifetime.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    if L.size == 0: return 0.0\n",
        "    return float(np.max(L))\n",
        "\n",
        "def h0_l2_norm(diag_h):\n",
        "    \"\"\"sqrt(sum L_i^2).\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    return float(np.sqrt(np.sum(L**2)))\n",
        "\n",
        "def _auc_tri_max(diag_h):\n",
        "    \"\"\"Area under the pointwise max of triangular bars over the birthdeath axis (discretized).\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    # Fast path: one bar  exact\n",
        "    if arr.shape[0] == 1:\n",
        "        L = float(arr[0,1] - arr[0,0])\n",
        "        return 0.25 * (L * L)\n",
        "\n",
        "    grid = _build_grid_for_time_funcs(arr, n_grid=64)\n",
        "    lam1 = np.zeros_like(grid, dtype=float)\n",
        "    b, d = arr[:,0], arr[:,1]\n",
        "    for j in range(b.size):\n",
        "        bj, dj = b[j], d[j]\n",
        "        m  = 0.5 * (bj + dj)\n",
        "        h  = 0.5 * (dj - bj)\n",
        "        if not (np.isfinite(h) and h > 0):\n",
        "            continue\n",
        "        tri = np.zeros_like(grid, dtype=float)\n",
        "        left  = (grid >= bj) & (grid <= m)\n",
        "        right = (grid >= m)  & (grid <= dj)\n",
        "        if left.any():  tri[left]  = (grid[left]  - bj) * (h / max(m - bj, 1e-12))\n",
        "        if right.any(): tri[right] = (dj - grid[right]) * (h / max(dj - m, 1e-12))\n",
        "        lam1 = np.maximum(lam1, tri)\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "# --- Base H0 features (AUC-family) ---\n",
        "def h0_ratio_auc_l1_to_sum(diag_h):\n",
        "    \"\"\" max triangular envelope / sum lifetimes.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    L = _lifetimes(arr); S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    A = _auc_tri_max(arr)\n",
        "    return _safe_div(A, S)\n",
        "\n",
        "def h0_ratio_auc_to_max(diag_h):\n",
        "    \"\"\" max triangular envelope / max lifetime.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    A = _auc_tri_max(arr)\n",
        "    M = _bottleneck_amp(arr)\n",
        "    return _safe_div(A, M)\n",
        "\n",
        "def h0_ratio_auc_to_l2(diag_h):\n",
        "    \"\"\" max triangular envelope / L2(lifetimes).\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    A  = _auc_tri_max(arr)\n",
        "    L2 = h0_l2_norm(arr)\n",
        "    return _safe_div(A, L2)\n",
        "\n",
        "# --- Other base stats + streaming-friendly single-window stats ---\n",
        "def pete(diag_h, p=1.6, q=0.5):\n",
        "    \"\"\"Sum (L^p * |(b+d)/2|^q) / sum L.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0); S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    num = np.sum((L ** p) * (np.abs(radial) ** q))\n",
        "    return _safe_div(num, S)\n",
        "\n",
        "def _tail_share_q(diag_h, q=0.90):\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    if L.size == 0: return 0.0\n",
        "    S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    qv = float(np.quantile(L, q))\n",
        "    return _safe_div(float(L[L >= qv].sum()), S)\n",
        "\n",
        "def _tail_share_q90(diag_h): return _tail_share_q(diag_h, q=0.90)\n",
        "def _tail_share_q80(diag_h): return _tail_share_q(diag_h, q=0.80)\n",
        "\n",
        "def _sum_centroid(diag_h):\n",
        "    \"\"\"Lifetime-weighted |(b+d)/2| mean / sum lifetimes.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0); S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    radial = np.abs((b + d) / np.sqrt(2.0))\n",
        "    return _safe_div(float(np.sum(radial * L)), S)\n",
        "\n",
        "def _sumL(diag_h):\n",
        "    arr = _clean_diag_h(diag_h); L = _lifetimes(arr)\n",
        "    return float(np.sum(L)) if L.size > 0 else 0.0\n",
        "\n",
        "def H0_energy_concentration(diag_h):   # L2 / sumL\n",
        "    L2 = h0_l2_norm(diag_h); S = _sumL(diag_h)\n",
        "    return _safe_div(L2, S)\n",
        "\n",
        "def H0_dominance_share(diag_h):        # maxL / sumL\n",
        "    mx = _bottleneck_amp(diag_h); S = _sumL(diag_h)\n",
        "    return _safe_div(mx, S)\n",
        "\n",
        "def H0_tail_curvature_80_90(diag_h):   # tail90 - tail80\n",
        "    return float(_tail_share_q90(diag_h) - _tail_share_q80(diag_h))\n",
        "\n",
        "def H0_centroid_to_energy(diag_h):     # centroid / L2\n",
        "    cen = _sum_centroid(diag_h); L2 = h0_l2_norm(diag_h)\n",
        "    return _safe_div(cen, L2)\n",
        "\n",
        "def _gini_from_array(x):\n",
        "    \"\"\"Gini of nonnegative array x (0 if empty).\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    x = x[x >= 0.0]\n",
        "    if x.size == 0 or np.allclose(x.sum(), 0.0):\n",
        "        return 0.0\n",
        "    xs = np.sort(x); n = xs.size\n",
        "    cumx = np.cumsum(xs)\n",
        "    return float(1.0 + 1.0/n - 2.0 * (cumx.sum() / (n * xs.sum())))\n",
        "\n",
        "def H0_gini(diag_h):\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    return _gini_from_array(L)\n",
        "\n",
        "# ------------- Feature extraction per-window -------------\n",
        "def extract_h0_features_series(D0_list):\n",
        "    \"\"\"Return dict with requested H0 features (one value per window).\"\"\"\n",
        "    n = len(D0_list)\n",
        "    feats = {\n",
        "        # AUC-family\n",
        "        \"H0_ratio_auc_L1_to_sum\":  np.zeros(n, dtype=float),\n",
        "        \"H0_ratio_auc_to_max\":     np.zeros(n, dtype=float),\n",
        "        \"H0_ratio_auc_to_l2\":      np.zeros(n, dtype=float),\n",
        "        # base stats\n",
        "        \"H0_bottleneck\":           np.zeros(n, dtype=float),\n",
        "        \"tail_share_q90\":          np.zeros(n, dtype=float),\n",
        "        \"H0_sum_centroid\":         np.zeros(n, dtype=float),\n",
        "        \"H0_L2_norm\":              np.zeros(n, dtype=float),\n",
        "        \"PETE_p1.6_q0.5\":          np.zeros(n, dtype=float),\n",
        "        # streaming-friendly\n",
        "        \"H0_energy_concentration\": np.zeros(n, dtype=float),\n",
        "        \"H0_dominance_share\":      np.zeros(n, dtype=float),\n",
        "        \"H0_tail_curvature_80_90\": np.zeros(n, dtype=float),\n",
        "        \"H0_centroid_to_energy\":   np.zeros(n, dtype=float),\n",
        "        \"H0_gini\":                 np.zeros(n, dtype=float),\n",
        "    }\n",
        "    for i, D0 in enumerate(D0_list):\n",
        "        clean = _clean_diag_h(D0)\n",
        "        # AUC-family\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"][i] = h0_ratio_auc_l1_to_sum(clean)\n",
        "        feats[\"H0_ratio_auc_to_max\"][i]    = h0_ratio_auc_to_max(clean)\n",
        "        feats[\"H0_ratio_auc_to_l2\"][i]     = h0_ratio_auc_to_l2(clean)\n",
        "        # base stats\n",
        "        feats[\"H0_bottleneck\"][i]          = _bottleneck_amp(clean)\n",
        "        feats[\"tail_share_q90\"][i]         = _tail_share_q90(clean)\n",
        "        feats[\"H0_sum_centroid\"][i]        = _sum_centroid(clean)\n",
        "        feats[\"H0_L2_norm\"][i]             = h0_l2_norm(clean)\n",
        "        feats[\"PETE_p1.6_q0.5\"][i]         = pete(clean, p=1.6, q=0.5)\n",
        "        # streaming-friendly\n",
        "        feats[\"H0_energy_concentration\"][i] = H0_energy_concentration(clean)\n",
        "        feats[\"H0_dominance_share\"][i]      = H0_dominance_share(clean)\n",
        "        feats[\"H0_tail_curvature_80_90\"][i] = H0_tail_curvature_80_90(clean)\n",
        "        feats[\"H0_centroid_to_energy\"][i]   = H0_centroid_to_energy(clean)\n",
        "        feats[\"H0_gini\"][i]                 = H0_gini(clean)\n",
        "    return feats\n",
        "\n",
        "# ------------- Segmentation helpers -------------\n",
        "def _resolve_cp_column(df: pd.DataFrame) -> str:\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "    for cand in CP_CANDIDATES:\n",
        "        if cand in cols_lower: return cols_lower[cand]\n",
        "    raise ValueError(f\"No change-point column found. Looked for {CP_CANDIDATES}. Found: {list(df.columns)}\")\n",
        "\n",
        "def _segments_from_cp(df: pd.DataFrame, cp_col: str):\n",
        "    \"\"\"\n",
        "    Build half-open segments [start, end) using indices where cp==1 as breakpoints.\n",
        "    The cp index is treated as the *first* index of the new segment.\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    cp_idx = df.index[df[cp_col] == 1].astype(int).tolist()\n",
        "    bkps = sorted(set([i for i in cp_idx if 0 < i <= n]) | {n})\n",
        "    starts = [0] + bkps[:-1]\n",
        "    return list(zip(starts, bkps))  # [(s,e), ...], covering [0, n)\n",
        "\n",
        "# ------------- Per-dataset processing  DataFrame -------------\n",
        "def process_one(ts_id: int) -> pd.DataFrame | None:\n",
        "    path = DATA_PATTERN.format(ts_id=ts_id)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[WARN] Missing: {path}\")\n",
        "        return None\n",
        "\n",
        "    df_raw = pd.read_csv(path)\n",
        "    if VALUE_COL not in df_raw.columns or LABEL_COL not in df_raw.columns:\n",
        "        print(f\"[WARN] {path} missing required columns\")\n",
        "        return None\n",
        "    cp_col = _resolve_cp_column(df_raw)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"value\":   pd.to_numeric(df_raw[VALUE_COL], errors=\"coerce\"),\n",
        "        \"anomaly\": pd.to_numeric(df_raw[LABEL_COL], errors=\"coerce\").fillna(0).astype(int),\n",
        "        \"cp\":      pd.to_numeric(df_raw[cp_col], errors=\"coerce\").fillna(0).astype(int),\n",
        "    }).interpolate(limit_direction=\"both\").ffill().bfill().reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    segments = _segments_from_cp(df, \"cp\")\n",
        "\n",
        "    # Prepare output frame for ALL points (0..n-1)\n",
        "    out_cols = {\n",
        "        \"loadeddataset\": [f\"ts{ts_id}\"] * n,\n",
        "        \"segment\": 0,\n",
        "        \"point\":   np.arange(n, dtype=int),\n",
        "        \"is_anomaly\": df[\"anomaly\"].astype(int).values,\n",
        "        \"y\": df[\"value\"].astype(float).values,  # original time-series value\n",
        "    }\n",
        "    # Reserve NaNs for features at points without a complete window ending there\n",
        "    for name in FEATURE_NAMES:\n",
        "        out_cols[name] = np.nan\n",
        "    out = pd.DataFrame(out_cols)\n",
        "\n",
        "    # Tag each point with its segment id\n",
        "    seg_id_arr = np.zeros(n, dtype=int)\n",
        "    for sid, (s, e) in enumerate(segments, start=1):\n",
        "        seg_id_arr[s:e] = sid\n",
        "    out[\"segment\"] = seg_id_arr\n",
        "\n",
        "    # Compute features per window and assign to right-edge index (global coords)\n",
        "    x = df[\"value\"].to_numpy(dtype=float)\n",
        "    W = window_size\n",
        "    for sid, (s, e) in enumerate(segments, start=1):\n",
        "        seg_x = x[s:e]\n",
        "        if len(seg_x) < W:\n",
        "            continue\n",
        "\n",
        "        n_windows = (len(seg_x) - W) + 1  # stride=1\n",
        "        D0_list = []\n",
        "        for i in range(n_windows):\n",
        "            w = seg_x[i:i+W]\n",
        "            emb = takens_embed(w, time_delay, dimension)  # (m, dim)\n",
        "            dgms = ripser(emb, maxdim=0)[\"dgms\"]\n",
        "            D0_list.append(_clean_diag_h(dgms[0] if len(dgms) else None))\n",
        "\n",
        "        # Compute raw features per window\n",
        "        feat_dict = extract_h0_features_series(D0_list)\n",
        "        df_feat = pd.DataFrame({\"window\": np.arange(n_windows, dtype=int)})\n",
        "        for k, v in feat_dict.items(): df_feat[k] = v\n",
        "\n",
        "        # Map window to GLOBAL right-edge index\n",
        "        t_global = s + df_feat[\"window\"].astype(int) + (W - 1)\n",
        "\n",
        "        # Write RAW feature values directly (no VAAD, no normalization)\n",
        "        for name in FEATURE_NAMES:\n",
        "            if name in df_feat.columns:\n",
        "                out.loc[t_global.values, name] = df_feat[name].to_numpy(dtype=float)\n",
        "\n",
        "    return out\n",
        "\n",
        "# ------------- Batch over datasets  single CSV -------------\n",
        "all_rows = []\n",
        "for ts in range(1, 101):  # TS1..TS100 (adjust upper bound as needed)\n",
        "    try:\n",
        "        df_one = process_one(ts)\n",
        "        if df_one is not None:\n",
        "            all_rows.append(df_one)\n",
        "            print(f\"[OK] ts{ts}: rows={len(df_one)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ts{ts}: {e}\")\n",
        "\n",
        "if all_rows:\n",
        "    df_all = pd.concat(all_rows, ignore_index=True)\n",
        "    # Ensure dtypes are clean\n",
        "    df_all[\"y\"] = pd.to_numeric(df_all[\"y\"], errors=\"coerce\").astype(float)\n",
        "    df_all[\"is_anomaly\"] = pd.to_numeric(df_all[\"is_anomaly\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df_all.to_csv(OUT_CSV, index=False)\n",
        "    print(f\"Saved: {OUT_CSV} (rows={len(df_all)})\")\n",
        "else:\n",
        "    print(\"No data written.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "MspzVP2m2rim",
        "outputId": "e0b3d5fa-22b5-49e7-e450-aeafb1c0c09c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] ts1: rows=1680\n",
            "[OK] ts2: rows=1680\n",
            "[OK] ts3: rows=1680\n",
            "[OK] ts4: rows=1680\n",
            "[OK] ts5: rows=1680\n",
            "[OK] ts6: rows=1680\n",
            "[OK] ts7: rows=1680\n",
            "[OK] ts8: rows=1680\n",
            "[OK] ts9: rows=1680\n",
            "[OK] ts10: rows=1680\n",
            "[OK] ts11: rows=1680\n",
            "[OK] ts12: rows=1680\n",
            "[OK] ts13: rows=1680\n",
            "[OK] ts14: rows=1680\n",
            "[OK] ts15: rows=1680\n",
            "[OK] ts16: rows=1680\n",
            "[OK] ts17: rows=1680\n",
            "[OK] ts18: rows=1680\n",
            "[OK] ts19: rows=1680\n",
            "[OK] ts20: rows=1680\n",
            "[OK] ts21: rows=1680\n",
            "[OK] ts22: rows=1680\n",
            "[OK] ts23: rows=1680\n",
            "[OK] ts24: rows=1680\n",
            "[OK] ts25: rows=1680\n",
            "[OK] ts26: rows=1680\n",
            "[OK] ts27: rows=1680\n",
            "[OK] ts28: rows=1680\n",
            "[OK] ts29: rows=1680\n",
            "[OK] ts30: rows=1680\n",
            "[OK] ts31: rows=1680\n",
            "[OK] ts32: rows=1680\n",
            "[OK] ts33: rows=1680\n",
            "[OK] ts34: rows=1680\n",
            "[OK] ts35: rows=1680\n",
            "[OK] ts36: rows=1680\n",
            "[OK] ts37: rows=1680\n",
            "[OK] ts38: rows=1680\n",
            "[OK] ts39: rows=1680\n",
            "[OK] ts40: rows=1680\n",
            "[OK] ts41: rows=1680\n",
            "[OK] ts42: rows=1680\n",
            "[OK] ts43: rows=1680\n",
            "[OK] ts44: rows=1680\n",
            "[OK] ts45: rows=1680\n",
            "[OK] ts46: rows=1680\n",
            "[OK] ts47: rows=1680\n",
            "[OK] ts48: rows=1680\n",
            "[OK] ts49: rows=1680\n",
            "[OK] ts50: rows=1680\n",
            "[OK] ts51: rows=1680\n",
            "[OK] ts52: rows=1680\n",
            "[OK] ts53: rows=1680\n",
            "[OK] ts54: rows=1680\n",
            "[OK] ts55: rows=1680\n",
            "[OK] ts56: rows=1680\n",
            "[OK] ts57: rows=1680\n",
            "[OK] ts58: rows=1680\n",
            "[OK] ts59: rows=1680\n",
            "[OK] ts60: rows=1680\n",
            "[OK] ts61: rows=1680\n",
            "[OK] ts62: rows=1680\n",
            "[OK] ts63: rows=1680\n",
            "[OK] ts64: rows=1680\n",
            "[OK] ts65: rows=1680\n",
            "[OK] ts66: rows=1680\n",
            "[OK] ts67: rows=1680\n",
            "[OK] ts68: rows=1680\n",
            "[OK] ts69: rows=1680\n",
            "[OK] ts70: rows=1680\n",
            "[OK] ts71: rows=1680\n",
            "[OK] ts72: rows=1680\n",
            "[OK] ts73: rows=1680\n",
            "[OK] ts74: rows=1680\n",
            "[OK] ts75: rows=1680\n",
            "[OK] ts76: rows=1680\n",
            "[OK] ts77: rows=1680\n",
            "[OK] ts78: rows=1680\n",
            "[OK] ts79: rows=1680\n",
            "[OK] ts80: rows=1680\n",
            "[OK] ts81: rows=1680\n",
            "[OK] ts82: rows=1680\n",
            "[OK] ts83: rows=1680\n",
            "[OK] ts84: rows=1680\n",
            "[OK] ts85: rows=1680\n",
            "[OK] ts86: rows=1680\n",
            "[OK] ts87: rows=1680\n",
            "[OK] ts88: rows=1680\n",
            "[OK] ts89: rows=1680\n",
            "[OK] ts90: rows=1680\n",
            "[OK] ts91: rows=1680\n",
            "[OK] ts92: rows=1680\n",
            "[OK] ts93: rows=1680\n",
            "[OK] ts94: rows=1680\n",
            "[OK] ts95: rows=1680\n",
            "[OK] ts96: rows=1680\n",
            "[OK] ts97: rows=1680\n",
            "[OK] ts98: rows=1680\n",
            "[OK] ts99: rows=1680\n",
            "[OK] ts100: rows=1680\n",
            "Saved: /content/tda_features_a3.csv (rows=168000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TDA Feature Extractor and Anomaly Scoring Algorithm (VAAD)\n",
        "# Segmentation-aware TDA anomaly scores  single CSV (all datasets, all points)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ripser import ripser\n",
        "\n",
        "# ---------------- Tunables ----------------\n",
        "DATA_PATTERN = \"/content/A3Benchmark-TS{ts_id}.csv\"  # change if needed\n",
        "OUT_CSV      = \"/content/anomaly_scores_a3.csv\"     # final output CSV\n",
        "\n",
        "VALUE_COL = \"value\"\n",
        "LABEL_COL = \"anomaly\"\n",
        "CP_CANDIDATES = [\"new_cp\"]\n",
        "\n",
        "# Sliding window + Takens\n",
        "window_size = 14\n",
        "time_delay  = 1\n",
        "dimension   = 7\n",
        "assert window_size - (dimension - 1) * time_delay > 0, \\\n",
        "    \"Choose smaller time_delay*dimension or larger window_size.\"\n",
        "\n",
        "# VA scoring knobs\n",
        "KV = 3.5\n",
        "KA = 3.5\n",
        "MODE = \"strict\"  # \"strict\" | \"plateau\" | \"abs_plateau\" | \"none\"\n",
        "\n",
        "# Which TDA feature streams to export (-> anomaly score columns)\n",
        "FEATURES = [\n",
        "    (\"H0_ratio_auc_L1_to_sum\",  \"anomalyscore_h0_auc\"),\n",
        "    (\"H0_ratio_auc_to_max\",     \"anomalyscore_h0_auc_over_max\"),\n",
        "    (\"H0_ratio_auc_to_l2\",      \"anomalyscore_h0_auc_over_l2\"),\n",
        "\n",
        "    (\"H0_bottleneck\",           \"anomalyscore_bottleneck\"),\n",
        "    (\"tail_share_q90\",          \"anomalyscore_tail_q90\"),\n",
        "    (\"H0_sum_centroid\",         \"anomalyscore_sum_centroid\"),\n",
        "    (\"H0_L2_norm\",              \"anomalyscore_h0_l2norm\"),\n",
        "    (\"PETE_p1.6_q0.5\",          \"anomalyscore_pete\"),\n",
        "\n",
        "    (\"H0_energy_concentration\", \"anomalyscore_h0_energy_conc\"),   # L2 / sumL\n",
        "    (\"H0_dominance_share\",      \"anomalyscore_h0_dom_share\"),     # maxL / sumL\n",
        "    (\"H0_tail_curvature_80_90\", \"anomalyscore_h0_tail_curve\"),    # tail90 - tail80\n",
        "    (\"H0_centroid_to_energy\",   \"anomalyscore_h0_cen_to_energy\"), # centroid / L2\n",
        "    (\"H0_gini\",                 \"anomalyscore_h0_gini\"),          # inequality of lifetimes\n",
        "]\n",
        "\n",
        "# ------------- Utilities (TDA + features) -------------\n",
        "def takens_embed(window: np.ndarray, time_delay: int, dimension: int) -> np.ndarray:\n",
        "    m = len(window) - (dimension - 1) * time_delay\n",
        "    if m <= 0:\n",
        "        raise ValueError(\"Takens params too large for this window_size.\")\n",
        "    return np.stack([window[j : j + m * time_delay : time_delay] for j in range(dimension)], axis=1)\n",
        "\n",
        "def _clean_diag_h(diag_h):\n",
        "    if diag_h is None:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    arr = np.asarray(diag_h, dtype=float)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2 or arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    finite_mask = np.isfinite(arr).all(axis=1)\n",
        "    arr = arr[finite_mask]\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    ok = np.isfinite(d) & (d > b)\n",
        "    if not np.any(ok):\n",
        "        return np.empty((0, 2), dtype=float)\n",
        "    return np.stack([b[ok], d[ok]], axis=1)\n",
        "\n",
        "try:\n",
        "    _trapz = np.trapezoid\n",
        "except AttributeError:\n",
        "    _trapz = np.trapz\n",
        "\n",
        "_EPS = 1e-12\n",
        "def _safe_div(a, b, eps=_EPS): return float(a) / float(b + eps)\n",
        "\n",
        "def _build_grid_for_time_funcs(arr, n_grid=128):\n",
        "    if arr.size == 0: return np.linspace(0.0, 1.0, num=n_grid)\n",
        "    lo = float(np.min(arr[:, 0])); hi = float(np.max(arr[:, 1]))\n",
        "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo: lo, hi = 0.0, 1.0\n",
        "    return np.linspace(lo, hi, num=n_grid)\n",
        "\n",
        "def _lifetimes(arr):\n",
        "    if arr.size == 0:\n",
        "        return np.empty((0,), dtype=float)\n",
        "    return np.maximum(arr[:, 1] - arr[:, 0], 0.0)\n",
        "\n",
        "def _bottleneck_amp(diag_h):\n",
        "    \"\"\"Max lifetime.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    if L.size == 0: return 0.0\n",
        "    return float(np.max(L))\n",
        "\n",
        "def h0_l2_norm(diag_h):\n",
        "    \"\"\"sqrt(sum L_i^2).\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    if L.size == 0:\n",
        "        return 0.0\n",
        "    return float(np.sqrt(np.sum(L**2)))\n",
        "\n",
        "def _auc_tri_max(diag_h):\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    # Fast path: one bar  exact\n",
        "    if arr.shape[0] == 1:\n",
        "        L = float(arr[0,1] - arr[0,0])\n",
        "        return 0.25 * (L * L)\n",
        "\n",
        "    # Smaller grid is enough; adaptive to time span\n",
        "    grid = _build_grid_for_time_funcs(arr, n_grid=64)\n",
        "    lam1 = np.zeros_like(grid, dtype=float)\n",
        "    b, d = arr[:,0], arr[:,1]\n",
        "    for j in range(b.size):\n",
        "        bj, dj = b[j], d[j]\n",
        "        m  = 0.5 * (bj + dj)\n",
        "        h  = 0.5 * (dj - bj)\n",
        "        if not (np.isfinite(h) and h > 0):\n",
        "            continue\n",
        "        tri = np.zeros_like(grid, dtype=float)\n",
        "        left  = (grid >= bj) & (grid <= m)\n",
        "        right = (grid >= m)  & (grid <= dj)\n",
        "        if left.any():  tri[left]  = (grid[left]  - bj) * (h / max(m - bj, 1e-12))\n",
        "        if right.any(): tri[right] = (dj - grid[right]) * (h / max(dj - m, 1e-12))\n",
        "        lam1 = np.maximum(lam1, tri)\n",
        "    return float(_trapz(lam1, grid))\n",
        "\n",
        "\n",
        "# --- Base H0 features (AUC-family) ---\n",
        "def h0_ratio_auc_l1_to_sum(diag_h):\n",
        "    \"\"\" max triangular envelope / sum lifetimes.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    L = _lifetimes(arr); S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    A = _auc_tri_max(arr)\n",
        "    return _safe_div(A, S)\n",
        "\n",
        "def h0_ratio_auc_to_max(diag_h):\n",
        "    \"\"\" max triangular envelope / max lifetime.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    A = _auc_tri_max(arr)\n",
        "    M = _bottleneck_amp(arr)\n",
        "    return _safe_div(A, M)\n",
        "\n",
        "def h0_ratio_auc_to_l2(diag_h):\n",
        "    \"\"\" max triangular envelope / L2(lifetimes).\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    A  = _auc_tri_max(arr)\n",
        "    L2 = h0_l2_norm(arr)\n",
        "    return _safe_div(A, L2)\n",
        "\n",
        "# --- Other base stats + streaming-friendly single-window stats ---\n",
        "def pete(diag_h, p=1.6, q=0.5):\n",
        "    \"\"\"Sum (L^p * |(b+d)/2|^q) / sum L.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0); S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    radial = (b + d) / np.sqrt(2.0)\n",
        "    num = np.sum((L ** p) * (np.abs(radial) ** q))\n",
        "    return _safe_div(num, S)\n",
        "\n",
        "def _tail_share_q(diag_h, q=0.90):\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    if L.size == 0: return 0.0\n",
        "    S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    qv = float(np.quantile(L, q))\n",
        "    return _safe_div(float(L[L >= qv].sum()), S)\n",
        "\n",
        "def _tail_share_q90(diag_h): return _tail_share_q(diag_h, q=0.90)\n",
        "def _tail_share_q80(diag_h): return _tail_share_q(diag_h, q=0.80)\n",
        "\n",
        "def _sum_centroid(diag_h):\n",
        "    \"\"\"Lifetime-weighted |(b+d)/2| mean / sum lifetimes.\"\"\"\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    if arr.size == 0: return 0.0\n",
        "    b, d = arr[:, 0], arr[:, 1]\n",
        "    L = np.maximum(d - b, 0.0); S = float(L.sum())\n",
        "    if S <= _EPS: return 0.0\n",
        "    radial = np.abs((b + d) / np.sqrt(2.0))\n",
        "    return _safe_div(float(np.sum(radial * L)), S)\n",
        "\n",
        "def _sumL(diag_h):\n",
        "    arr = _clean_diag_h(diag_h); L = _lifetimes(arr)\n",
        "    return float(np.sum(L)) if L.size > 0 else 0.0\n",
        "\n",
        "def H0_energy_concentration(diag_h):   # L2 / sumL\n",
        "    L2 = h0_l2_norm(diag_h); S = _sumL(diag_h)\n",
        "    return _safe_div(L2, S)\n",
        "\n",
        "def H0_dominance_share(diag_h):        # maxL / sumL\n",
        "    mx = _bottleneck_amp(diag_h); S = _sumL(diag_h)\n",
        "    return _safe_div(mx, S)\n",
        "\n",
        "def H0_tail_curvature_80_90(diag_h):   # tail90 - tail80\n",
        "    return float(_tail_share_q90(diag_h) - _tail_share_q80(diag_h))\n",
        "\n",
        "def H0_centroid_to_energy(diag_h):     # centroid / L2\n",
        "    cen = _sum_centroid(diag_h); L2 = h0_l2_norm(diag_h)\n",
        "    return _safe_div(cen, L2)\n",
        "\n",
        "def _gini_from_array(x):\n",
        "    \"\"\"Gini of nonnegative array x (0 if empty).\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    x = x[x >= 0.0]\n",
        "    if x.size == 0 or np.allclose(x.sum(), 0.0):\n",
        "        return 0.0\n",
        "    xs = np.sort(x); n = xs.size\n",
        "    cumx = np.cumsum(xs)\n",
        "    return float(1.0 + 1.0/n - 2.0 * (cumx.sum() / (n * xs.sum())))\n",
        "\n",
        "def H0_gini(diag_h):\n",
        "    arr = _clean_diag_h(diag_h)\n",
        "    L = _lifetimes(arr)\n",
        "    return _gini_from_array(L)\n",
        "\n",
        "# ------------- Feature extraction per-window -------------\n",
        "def extract_h0_features_series(D0_list):\n",
        "    \"\"\"Return dict with requested H0 features (one value per window).\"\"\"\n",
        "    n = len(D0_list)\n",
        "    feats = {\n",
        "        # AUC-family\n",
        "        \"H0_ratio_auc_L1_to_sum\":  np.zeros(n, dtype=float),\n",
        "        \"H0_ratio_auc_to_max\":     np.zeros(n, dtype=float),\n",
        "        \"H0_ratio_auc_to_l2\":      np.zeros(n, dtype=float),\n",
        "        # base stats\n",
        "        \"H0_bottleneck\":           np.zeros(n, dtype=float),\n",
        "        \"tail_share_q90\":          np.zeros(n, dtype=float),\n",
        "        \"H0_sum_centroid\":         np.zeros(n, dtype=float),\n",
        "        \"H0_L2_norm\":              np.zeros(n, dtype=float),\n",
        "        \"PETE_p1.6_q0.5\":          np.zeros(n, dtype=float),\n",
        "        # streaming-friendly\n",
        "        \"H0_energy_concentration\": np.zeros(n, dtype=float),\n",
        "        \"H0_dominance_share\":      np.zeros(n, dtype=float),\n",
        "        \"H0_tail_curvature_80_90\": np.zeros(n, dtype=float),\n",
        "        \"H0_centroid_to_energy\":   np.zeros(n, dtype=float),\n",
        "        \"H0_gini\":                 np.zeros(n, dtype=float),\n",
        "    }\n",
        "    for i, D0 in enumerate(D0_list):\n",
        "        clean = _clean_diag_h(D0)\n",
        "        # AUC-family\n",
        "        feats[\"H0_ratio_auc_L1_to_sum\"][i] = h0_ratio_auc_l1_to_sum(clean)\n",
        "        feats[\"H0_ratio_auc_to_max\"][i]    = h0_ratio_auc_to_max(clean)\n",
        "        feats[\"H0_ratio_auc_to_l2\"][i]     = h0_ratio_auc_to_l2(clean)\n",
        "        # base stats\n",
        "        feats[\"H0_bottleneck\"][i]          = _bottleneck_amp(clean)\n",
        "        feats[\"tail_share_q90\"][i]         = _tail_share_q90(clean)\n",
        "        feats[\"H0_sum_centroid\"][i]        = _sum_centroid(clean)\n",
        "        feats[\"H0_L2_norm\"][i]             = h0_l2_norm(clean)\n",
        "        feats[\"PETE_p1.6_q0.5\"][i]         = pete(clean, p=1.6, q=0.5)\n",
        "        # streaming-friendly\n",
        "        feats[\"H0_energy_concentration\"][i] = H0_energy_concentration(clean)\n",
        "        feats[\"H0_dominance_share\"][i]      = H0_dominance_share(clean)\n",
        "        feats[\"H0_tail_curvature_80_90\"][i] = H0_tail_curvature_80_90(clean)\n",
        "        feats[\"H0_centroid_to_energy\"][i]   = H0_centroid_to_energy(clean)\n",
        "        feats[\"H0_gini\"][i]                 = H0_gini(clean)\n",
        "    return feats\n",
        "\n",
        "# ------------- Scoring helpers -------------\n",
        "def _mad_with_median(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    med = np.nanmedian(x); mad = np.nanmedian(np.abs(x - med))\n",
        "    return med, (mad + 1e-12)\n",
        "\n",
        "\n",
        "# old version of code\n",
        "#def velocity_accel_score(series, kv=KV, ka=KA, mode=MODE):\n",
        "#    s = pd.to_numeric(pd.Series(series, dtype=float), errors=\"coerce\").interpolate(limit_direction=\"both\")\n",
        "#    v = s.diff(1); a = v.diff(1)\n",
        "#    v_med, v_mad = _mad_with_median(v.values)\n",
        "#    a_med, a_mad = _mad_with_median(a.values)\n",
        "#    zv = np.maximum(0.0, (v.values - v_med) / v_mad)\n",
        "#    za = np.maximum(0.0, (a.values - a_med) / a_mad)\n",
        "#    return np.nan_to_num(zv * za, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "def velocity_accel_score(series, kv=KV, ka=KA, mode=MODE):\n",
        "    s = pd.to_numeric(pd.Series(series, dtype=float), errors=\"coerce\").interpolate(limit_direction=\"both\")\n",
        "    v = s.diff(1); a = v.diff(1)\n",
        "\n",
        "    def _zm(x):\n",
        "        med = np.nanmedian(x); mad = np.nanmedian(np.abs(x - med)) + 1e-12\n",
        "        return (x - med) / mad\n",
        "\n",
        "    zv = _zm(v.values); za = _zm(a.values)\n",
        "\n",
        "    mode = (mode or \"strict\").lower()\n",
        "    if mode == \"strict\":\n",
        "        zv = np.maximum(0.0, zv); za = np.maximum(0.0, za)\n",
        "    elif mode == \"plateau\":\n",
        "        # allow small negatives to survive (flattened spikes)\n",
        "        zv = np.where(zv > -0.25, zv, 0.0)\n",
        "        za = np.where(za > -0.25, za, 0.0)\n",
        "    elif mode == \"abs_plateau\":\n",
        "        zv = np.abs(zv); za = np.abs(za)\n",
        "    elif mode == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        # fallback\n",
        "        zv = np.maximum(0.0, zv); za = np.maximum(0.0, za)\n",
        "\n",
        "    score = (kv * zv) * (ka * za)\n",
        "    return np.nan_to_num(score, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "\n",
        "def minmax01(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if x.size == 0: return x\n",
        "    lo, hi = float(np.nanmin(x)), float(np.nanmax(x))\n",
        "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
        "        return np.zeros_like(x, dtype=float)\n",
        "    return (x - lo) / (hi - lo)\n",
        "\n",
        "# ------------- Segmentation helpers -------------\n",
        "def _resolve_cp_column(df: pd.DataFrame) -> str:\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "    for cand in CP_CANDIDATES:\n",
        "        if cand in cols_lower: return cols_lower[cand]\n",
        "    raise ValueError(f\"No change-point column found. Looked for {CP_CANDIDATES}. Found: {list(df.columns)}\")\n",
        "\n",
        "def _segments_from_cp(df: pd.DataFrame, cp_col: str):\n",
        "    n = len(df)\n",
        "    cp_idx = df.index[df[cp_col] == 1].astype(int).tolist()\n",
        "    bkps = sorted(set([i for i in cp_idx if 0 < i <= n]) | {n})\n",
        "    starts = [0] + bkps[:-1]\n",
        "    return list(zip(starts, bkps))  # [(s,e), ...]\n",
        "\n",
        "# ------------- Per-dataset processing  DataFrame -------------\n",
        "def process_one(ts_id: int) -> pd.DataFrame | None:\n",
        "    path = DATA_PATTERN.format(ts_id=ts_id)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[WARN] Missing: {path}\")\n",
        "        return None\n",
        "\n",
        "    df_raw = pd.read_csv(path)\n",
        "    if VALUE_COL not in df_raw.columns or LABEL_COL not in df_raw.columns:\n",
        "        print(f\"[WARN] {path} missing required columns\")\n",
        "        return None\n",
        "    cp_col = _resolve_cp_column(df_raw)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"value\":   pd.to_numeric(df_raw[VALUE_COL], errors=\"coerce\"),\n",
        "        \"anomaly\": pd.to_numeric(df_raw[LABEL_COL], errors=\"coerce\").fillna(0).astype(int),\n",
        "        \"cp\":      pd.to_numeric(df_raw[cp_col], errors=\"coerce\").fillna(0).astype(int),\n",
        "    }).ffill().bfill().reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    segments = _segments_from_cp(df, \"cp\")\n",
        "\n",
        "    # Prepare output frame for ALL points (0..n-1)\n",
        "    out_cols = {\n",
        "        \"loadeddataset\": [f\"ts{ts_id}\"] * n,\n",
        "        \"segment\": 0,\n",
        "        \"point\":   np.arange(n, dtype=int),\n",
        "        \"is_anomaly\": df[\"anomaly\"].astype(int).values,\n",
        "        \"y\": df[\"value\"].astype(float).values,  # original time-series value\n",
        "    }\n",
        "    for _, col_name in FEATURES:\n",
        "        out_cols[col_name] = np.nan\n",
        "    out = pd.DataFrame(out_cols)\n",
        "\n",
        "    # Tag each point with its segment id\n",
        "    seg_id_arr = np.zeros(n, dtype=int)\n",
        "    for sid, (s, e) in enumerate(segments, start=1):\n",
        "        seg_id_arr[s:e] = sid\n",
        "    out[\"segment\"] = seg_id_arr\n",
        "\n",
        "    # Fill scores only at window right-edges inside each segment\n",
        "    x = df[\"value\"].to_numpy(float)\n",
        "    for sid, (s, e) in enumerate(segments, start=1):\n",
        "        seg_x = x[s:e]\n",
        "        if len(seg_x) < window_size:\n",
        "            continue\n",
        "\n",
        "        n_windows = (len(seg_x) - window_size) + 1  # stride=1\n",
        "        D0_list = []\n",
        "        for i in range(n_windows):\n",
        "            w = seg_x[i:i+window_size]\n",
        "            emb = takens_embed(w, time_delay, dimension)\n",
        "            dgms = ripser(emb, maxdim=0)[\"dgms\"]\n",
        "            D0_list.append(_clean_diag_h(dgms[0] if len(dgms) else None))\n",
        "\n",
        "        # Compute raw features per window\n",
        "        feat_dict = extract_h0_features_series(D0_list)\n",
        "        df_feat = pd.DataFrame({\"window\": np.arange(n_windows, dtype=int)})\n",
        "        for k, v in feat_dict.items(): df_feat[k] = v\n",
        "\n",
        "        # Map window to GLOBAL right-edge index\n",
        "        t_global = s + df_feat[\"window\"].astype(int) + (window_size - 1)\n",
        "\n",
        "        # Turn each feature into an anomaly-score series (minmax 0..1 within segment)\n",
        "        for feat_name, col_name in FEATURES:\n",
        "            if feat_name not in df_feat.columns:\n",
        "                continue\n",
        "            fvals  = df_feat[feat_name].to_numpy(float)\n",
        "            score  = velocity_accel_score(fvals, kv=KV, ka=KA, mode=MODE)\n",
        "            scoreN = minmax01(score)\n",
        "            out.loc[t_global.values, col_name] = scoreN\n",
        "\n",
        "    return out\n",
        "\n",
        "# ------------- Batch over datasets  single CSV -------------\n",
        "all_rows = []\n",
        "for ts in range(1, 101):  # TS1..TS101\n",
        "    try:\n",
        "        df_one = process_one(ts)\n",
        "        if df_one is not None:\n",
        "            all_rows.append(df_one)\n",
        "            print(f\"[OK] ts{ts}: rows={len(df_one)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ts{ts}: {e}\")\n",
        "\n",
        "if all_rows:\n",
        "    df_all = pd.concat(all_rows, ignore_index=True)\n",
        "    df_all.to_csv(OUT_CSV, index=False)\n",
        "    print(f\"Saved: {OUT_CSV} (rows={len(df_all)})\")\n",
        "else:\n",
        "    print(\"No data written.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOcwQDD_SwRy",
        "outputId": "e91524ff-dbfd-4ced-fd56-a8acec84e3db",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] ts1: rows=1680\n",
            "[OK] ts2: rows=1680\n",
            "[OK] ts3: rows=1680\n",
            "[OK] ts4: rows=1680\n",
            "[OK] ts5: rows=1680\n",
            "[OK] ts6: rows=1680\n",
            "[OK] ts7: rows=1680\n",
            "[OK] ts8: rows=1680\n",
            "[OK] ts9: rows=1680\n",
            "[OK] ts10: rows=1680\n",
            "[OK] ts11: rows=1680\n",
            "[OK] ts12: rows=1680\n",
            "[OK] ts13: rows=1680\n",
            "[OK] ts14: rows=1680\n",
            "[OK] ts15: rows=1680\n",
            "[OK] ts16: rows=1680\n",
            "[OK] ts17: rows=1680\n",
            "[OK] ts18: rows=1680\n",
            "[OK] ts19: rows=1680\n",
            "[OK] ts20: rows=1680\n",
            "[OK] ts21: rows=1680\n",
            "[OK] ts22: rows=1680\n",
            "[OK] ts23: rows=1680\n",
            "[OK] ts24: rows=1680\n",
            "[OK] ts25: rows=1680\n",
            "[OK] ts26: rows=1680\n",
            "[OK] ts27: rows=1680\n",
            "[OK] ts28: rows=1680\n",
            "[OK] ts29: rows=1680\n",
            "[OK] ts30: rows=1680\n",
            "[OK] ts31: rows=1680\n",
            "[OK] ts32: rows=1680\n",
            "[OK] ts33: rows=1680\n",
            "[OK] ts34: rows=1680\n",
            "[OK] ts35: rows=1680\n",
            "[OK] ts36: rows=1680\n",
            "[OK] ts37: rows=1680\n",
            "[OK] ts38: rows=1680\n",
            "[OK] ts39: rows=1680\n",
            "[OK] ts40: rows=1680\n",
            "[OK] ts41: rows=1680\n",
            "[OK] ts42: rows=1680\n",
            "[OK] ts43: rows=1680\n",
            "[OK] ts44: rows=1680\n",
            "[OK] ts45: rows=1680\n",
            "[OK] ts46: rows=1680\n",
            "[OK] ts47: rows=1680\n",
            "[OK] ts48: rows=1680\n",
            "[OK] ts49: rows=1680\n",
            "[OK] ts50: rows=1680\n",
            "[OK] ts51: rows=1680\n",
            "[OK] ts52: rows=1680\n",
            "[OK] ts53: rows=1680\n",
            "[OK] ts54: rows=1680\n",
            "[OK] ts55: rows=1680\n",
            "[OK] ts56: rows=1680\n",
            "[OK] ts57: rows=1680\n",
            "[OK] ts58: rows=1680\n",
            "[OK] ts59: rows=1680\n",
            "[OK] ts60: rows=1680\n",
            "[OK] ts61: rows=1680\n",
            "[OK] ts62: rows=1680\n",
            "[OK] ts63: rows=1680\n",
            "[OK] ts64: rows=1680\n",
            "[OK] ts65: rows=1680\n",
            "[OK] ts66: rows=1680\n",
            "[OK] ts67: rows=1680\n",
            "[OK] ts68: rows=1680\n",
            "[OK] ts69: rows=1680\n",
            "[OK] ts70: rows=1680\n",
            "[OK] ts71: rows=1680\n",
            "[OK] ts72: rows=1680\n",
            "[OK] ts73: rows=1680\n",
            "[OK] ts74: rows=1680\n",
            "[OK] ts75: rows=1680\n",
            "[OK] ts76: rows=1680\n",
            "[OK] ts77: rows=1680\n",
            "[OK] ts78: rows=1680\n",
            "[OK] ts79: rows=1680\n",
            "[OK] ts80: rows=1680\n",
            "[OK] ts81: rows=1680\n",
            "[OK] ts82: rows=1680\n",
            "[OK] ts83: rows=1680\n",
            "[OK] ts84: rows=1680\n",
            "[OK] ts85: rows=1680\n",
            "[OK] ts86: rows=1680\n",
            "[OK] ts87: rows=1680\n",
            "[OK] ts88: rows=1680\n",
            "[OK] ts89: rows=1680\n",
            "[OK] ts90: rows=1680\n",
            "[OK] ts91: rows=1680\n",
            "[OK] ts92: rows=1680\n",
            "[OK] ts93: rows=1680\n",
            "[OK] ts94: rows=1680\n",
            "[OK] ts95: rows=1680\n",
            "[OK] ts96: rows=1680\n",
            "[OK] ts97: rows=1680\n",
            "[OK] ts98: rows=1680\n",
            "[OK] ts99: rows=1680\n",
            "[OK] ts100: rows=1680\n",
            "Saved: /content/anomaly_scores_a3.csv (rows=168000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANOMALY SCORES OF THE TIME SERIES"
      ],
      "metadata": {
        "id": "8O12D8IO8HQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV (adjust path if needed)\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/anomaly_scores_a3.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "ZydbEw_nLlXk",
        "outputId": "9a894819-cdb0-4a5e-960d-79cda3883db2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       loadeddataset  segment  point  is_anomaly            y  \\\n",
              "0                ts1        1      0           0  -363.278909   \n",
              "1                ts1        1      1           0   320.888590   \n",
              "2                ts1        1      2           0   891.727422   \n",
              "3                ts1        1      3           0  1174.652287   \n",
              "4                ts1        1      4           0  1712.290261   \n",
              "...              ...      ...    ...         ...          ...   \n",
              "167995         ts100        1   1675           0  -143.374166   \n",
              "167996         ts100        1   1676           0  -213.830741   \n",
              "167997         ts100        1   1677           0  -191.906215   \n",
              "167998         ts100        1   1678           0  -158.322475   \n",
              "167999         ts100        1   1679           0   -89.296180   \n",
              "\n",
              "        anomalyscore_h0_auc  anomalyscore_h0_auc_over_max  \\\n",
              "0                       NaN                           NaN   \n",
              "1                       NaN                           NaN   \n",
              "2                       NaN                           NaN   \n",
              "3                       NaN                           NaN   \n",
              "4                       NaN                           NaN   \n",
              "...                     ...                           ...   \n",
              "167995             0.000000                      0.000000   \n",
              "167996             0.000010                      0.000097   \n",
              "167997             0.000000                      0.000000   \n",
              "167998             0.000000                      0.000027   \n",
              "167999             0.000946                      0.003166   \n",
              "\n",
              "        anomalyscore_h0_auc_over_l2  anomalyscore_bottleneck  \\\n",
              "0                               NaN                      NaN   \n",
              "1                               NaN                      NaN   \n",
              "2                               NaN                      NaN   \n",
              "3                               NaN                      NaN   \n",
              "4                               NaN                      NaN   \n",
              "...                             ...                      ...   \n",
              "167995                     0.000000                 0.000000   \n",
              "167996                     0.000009                 0.000097   \n",
              "167997                     0.000000                 0.000000   \n",
              "167998                     0.000000                 0.000027   \n",
              "167999                     0.001331                 0.003166   \n",
              "\n",
              "        anomalyscore_tail_q90  anomalyscore_sum_centroid  \\\n",
              "0                         NaN                        NaN   \n",
              "1                         NaN                        NaN   \n",
              "2                         NaN                        NaN   \n",
              "3                         NaN                        NaN   \n",
              "4                         NaN                        NaN   \n",
              "...                       ...                        ...   \n",
              "167995               0.000000                   0.000000   \n",
              "167996               0.000000                   0.000704   \n",
              "167997               0.000000                   0.001147   \n",
              "167998               0.000000                   0.000706   \n",
              "167999               0.002181                   0.002004   \n",
              "\n",
              "        anomalyscore_h0_l2norm  anomalyscore_pete  \\\n",
              "0                          NaN                NaN   \n",
              "1                          NaN                NaN   \n",
              "2                          NaN                NaN   \n",
              "3                          NaN                NaN   \n",
              "4                          NaN                NaN   \n",
              "...                        ...                ...   \n",
              "167995                0.000000           0.000000   \n",
              "167996                0.001063           0.000603   \n",
              "167997                0.002574           0.000953   \n",
              "167998                0.002008           0.000584   \n",
              "167999                0.004650           0.001734   \n",
              "\n",
              "        anomalyscore_h0_energy_conc  anomalyscore_h0_dom_share  \\\n",
              "0                               NaN                        NaN   \n",
              "1                               NaN                        NaN   \n",
              "2                               NaN                        NaN   \n",
              "3                               NaN                        NaN   \n",
              "4                               NaN                        NaN   \n",
              "...                             ...                        ...   \n",
              "167995                      0.00000                   0.000000   \n",
              "167996                      0.00019                   0.000000   \n",
              "167997                      0.00000                   0.000000   \n",
              "167998                      0.00000                   0.000000   \n",
              "167999                      0.00000                   0.002181   \n",
              "\n",
              "        anomalyscore_h0_tail_curve  anomalyscore_h0_cen_to_energy  \\\n",
              "0                              NaN                            NaN   \n",
              "1                              NaN                            NaN   \n",
              "2                              NaN                            NaN   \n",
              "3                              NaN                            NaN   \n",
              "4                              NaN                            NaN   \n",
              "...                            ...                            ...   \n",
              "167995                    0.000000                        0.00000   \n",
              "167996                    0.000000                        0.00019   \n",
              "167997                    0.002241                        0.00000   \n",
              "167998                    0.000000                        0.00000   \n",
              "167999                    0.001243                        0.00000   \n",
              "\n",
              "        anomalyscore_h0_gini  \n",
              "0                        NaN  \n",
              "1                        NaN  \n",
              "2                        NaN  \n",
              "3                        NaN  \n",
              "4                        NaN  \n",
              "...                      ...  \n",
              "167995              0.000000  \n",
              "167996              0.001116  \n",
              "167997              0.000000  \n",
              "167998              0.000000  \n",
              "167999              0.000000  \n",
              "\n",
              "[168000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62258b08-d5d0-4762-a1a6-dc3bfe7c22f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loadeddataset</th>\n",
              "      <th>segment</th>\n",
              "      <th>point</th>\n",
              "      <th>is_anomaly</th>\n",
              "      <th>y</th>\n",
              "      <th>anomalyscore_h0_auc</th>\n",
              "      <th>anomalyscore_h0_auc_over_max</th>\n",
              "      <th>anomalyscore_h0_auc_over_l2</th>\n",
              "      <th>anomalyscore_bottleneck</th>\n",
              "      <th>anomalyscore_tail_q90</th>\n",
              "      <th>anomalyscore_sum_centroid</th>\n",
              "      <th>anomalyscore_h0_l2norm</th>\n",
              "      <th>anomalyscore_pete</th>\n",
              "      <th>anomalyscore_h0_energy_conc</th>\n",
              "      <th>anomalyscore_h0_dom_share</th>\n",
              "      <th>anomalyscore_h0_tail_curve</th>\n",
              "      <th>anomalyscore_h0_cen_to_energy</th>\n",
              "      <th>anomalyscore_h0_gini</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ts1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-363.278909</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ts1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>320.888590</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ts1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>891.727422</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ts1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1174.652287</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ts1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1712.290261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167995</th>\n",
              "      <td>ts100</td>\n",
              "      <td>1</td>\n",
              "      <td>1675</td>\n",
              "      <td>0</td>\n",
              "      <td>-143.374166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167996</th>\n",
              "      <td>ts100</td>\n",
              "      <td>1</td>\n",
              "      <td>1676</td>\n",
              "      <td>0</td>\n",
              "      <td>-213.830741</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>0.00019</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00019</td>\n",
              "      <td>0.001116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167997</th>\n",
              "      <td>ts100</td>\n",
              "      <td>1</td>\n",
              "      <td>1677</td>\n",
              "      <td>0</td>\n",
              "      <td>-191.906215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.002574</td>\n",
              "      <td>0.000953</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002241</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167998</th>\n",
              "      <td>ts100</td>\n",
              "      <td>1</td>\n",
              "      <td>1678</td>\n",
              "      <td>0</td>\n",
              "      <td>-158.322475</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000706</td>\n",
              "      <td>0.002008</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167999</th>\n",
              "      <td>ts100</td>\n",
              "      <td>1</td>\n",
              "      <td>1679</td>\n",
              "      <td>0</td>\n",
              "      <td>-89.296180</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.004650</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>168000 rows  18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62258b08-d5d0-4762-a1a6-dc3bfe7c22f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62258b08-d5d0-4762-a1a6-dc3bfe7c22f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62258b08-d5d0-4762-a1a6-dc3bfe7c22f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c2632a98-a94e-4c4f-a952-4c20b9c75646\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c2632a98-a94e-4c4f-a952-4c20b9c75646')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c2632a98-a94e-4c4f-a952-4c20b9c75646 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_0661c9f1-4692-4a6e-a385-1598ade32ed7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0661c9f1-4692-4a6e-a385-1598ade32ed7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all NaN with 0\n",
        "df = df.fillna(0)\n",
        "print(\"\\nAny NaNs left?:\", df.isna().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPeuSzSkL2in",
        "outputId": "053c8afa-0b72-4368-db0e-e360a506b171",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Any NaNs left?: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure is_anomaly is integer\n",
        "df[\"is_anomaly\"] = pd.to_numeric(df[\"is_anomaly\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "# Print total anomalies\n",
        "print(\"Total anomalies (is_anomaly=1):\", df[\"is_anomaly\"].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqIa8G12L7Xj",
        "outputId": "f141cd8a-927f-4812-e4b2-e30d35197c0b",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total anomalies (is_anomaly=1): 943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMo9oeUSL_CI",
        "outputId": "330f14aa-2c69-4293-bce0-603c5ab24327",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['loadeddataset', 'segment', 'point', 'is_anomaly', 'y',\n",
              "       'anomalyscore_h0_auc', 'anomalyscore_h0_auc_over_max',\n",
              "       'anomalyscore_h0_auc_over_l2', 'anomalyscore_bottleneck',\n",
              "       'anomalyscore_tail_q90', 'anomalyscore_sum_centroid',\n",
              "       'anomalyscore_h0_l2norm', 'anomalyscore_pete',\n",
              "       'anomalyscore_h0_energy_conc', 'anomalyscore_h0_dom_share',\n",
              "       'anomalyscore_h0_tail_curve', 'anomalyscore_h0_cen_to_energy',\n",
              "       'anomalyscore_h0_gini'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title descriptive statistics of anomaly scores\n",
        "# assume df is already loaded\n",
        "# detect anomaly score columns automatically\n",
        "anomaly_cols = [c for c in df.columns if c.startswith(\"anomalyscore\")]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for col in anomaly_cols:\n",
        "    print(f\"\\n=== {col} ===\")\n",
        "    # 1. Descriptive statistics\n",
        "    stats = df[col].describe()\n",
        "    print(stats)\n",
        "\n",
        "    # 2. Count of GT anomaly points (is_anomaly==1) where value < 0.001\n",
        "    mask = (df[\"is_anomaly\"] == 1) & (df[col] < 0.001)\n",
        "    count_below = mask.sum()\n",
        "    print(f\"Ground truth anomalies with {col} < 0.001: {count_below}\")\n",
        "\n",
        "    results[col] = {\n",
        "        \"stats\": stats.to_dict(),\n",
        "        \"count_below_0.001\": count_below\n",
        "    }\n",
        "\n",
        "# If you want a compact summary table instead:\n",
        "summary = pd.DataFrame({\n",
        "    col: {\n",
        "        \"mean\": df[col].mean(),\n",
        "        \"std\": df[col].std(),\n",
        "        \"min\": df[col].min(),\n",
        "        \"25%\": df[col].quantile(0.25),\n",
        "        \"50%\": df[col].median(),\n",
        "        \"75%\": df[col].quantile(0.75),\n",
        "        \"max\": df[col].max(),\n",
        "        \"count_below_0.001 (GT=1)\": ((df[\"is_anomaly\"]==1) & (df[col] < 0.001)).sum()\n",
        "    }\n",
        "    for col in anomaly_cols\n",
        "}).T\n",
        "\n",
        "print(\"\\n\\n=== Summary Table ===\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTa96gmd2_61",
        "outputId": "fe1b2f28-97fc-4642-d840-81d4ad7f8977",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== anomalyscore_h0_auc ===\n",
            "count    168000.000000\n",
            "mean          0.002958\n",
            "std           0.038789\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000030\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_auc, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_auc < 0.001: 67\n",
            "\n",
            "=== anomalyscore_h0_auc_over_max ===\n",
            "count    168000.000000\n",
            "mean          0.003667\n",
            "std           0.042684\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000000\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_auc_over_max, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_auc_over_max < 0.001: 65\n",
            "\n",
            "=== anomalyscore_h0_auc_over_l2 ===\n",
            "count    168000.000000\n",
            "mean          0.003261\n",
            "std           0.040650\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000039\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_auc_over_l2, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_auc_over_l2 < 0.001: 65\n",
            "\n",
            "=== anomalyscore_bottleneck ===\n",
            "count    168000.000000\n",
            "mean          0.003667\n",
            "std           0.042684\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000000\n",
            "max           1.000000\n",
            "Name: anomalyscore_bottleneck, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_bottleneck < 0.001: 65\n",
            "\n",
            "=== anomalyscore_tail_q90 ===\n",
            "count    168000.000000\n",
            "mean          0.004023\n",
            "std           0.040969\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000239\n",
            "max           1.000000\n",
            "Name: anomalyscore_tail_q90, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_tail_q90 < 0.001: 74\n",
            "\n",
            "=== anomalyscore_sum_centroid ===\n",
            "count    168000.000000\n",
            "mean          0.003977\n",
            "std           0.040318\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000121\n",
            "max           1.000000\n",
            "Name: anomalyscore_sum_centroid, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_sum_centroid < 0.001: 58\n",
            "\n",
            "=== anomalyscore_h0_l2norm ===\n",
            "count    168000.000000\n",
            "mean          0.005511\n",
            "std           0.044860\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000245\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_l2norm, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_l2norm < 0.001: 59\n",
            "\n",
            "=== anomalyscore_pete ===\n",
            "count    168000.000000\n",
            "mean          0.003937\n",
            "std           0.040502\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000108\n",
            "max           1.000000\n",
            "Name: anomalyscore_pete, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_pete < 0.001: 59\n",
            "\n",
            "=== anomalyscore_h0_energy_conc ===\n",
            "count    168000.000000\n",
            "mean          0.002730\n",
            "std           0.035756\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000033\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_energy_conc, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_energy_conc < 0.001: 89\n",
            "\n",
            "=== anomalyscore_h0_dom_share ===\n",
            "count    168000.000000\n",
            "mean          0.004023\n",
            "std           0.040969\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000239\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_dom_share, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_dom_share < 0.001: 74\n",
            "\n",
            "=== anomalyscore_h0_tail_curve ===\n",
            "count    168000.000000\n",
            "mean          0.004979\n",
            "std           0.043575\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000387\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_tail_curve, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_tail_curve < 0.001: 151\n",
            "\n",
            "=== anomalyscore_h0_cen_to_energy ===\n",
            "count    168000.000000\n",
            "mean          0.002730\n",
            "std           0.035756\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000033\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_cen_to_energy, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_cen_to_energy < 0.001: 89\n",
            "\n",
            "=== anomalyscore_h0_gini ===\n",
            "count    168000.000000\n",
            "mean          0.008052\n",
            "std           0.050527\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.001039\n",
            "max           1.000000\n",
            "Name: anomalyscore_h0_gini, dtype: float64\n",
            "Ground truth anomalies with anomalyscore_h0_gini < 0.001: 79\n",
            "\n",
            "\n",
            "=== Summary Table ===\n",
            "                                   mean       std  min  25%  50%       75%  \\\n",
            "anomalyscore_h0_auc            0.002958  0.038789  0.0  0.0  0.0  0.000030   \n",
            "anomalyscore_h0_auc_over_max   0.003667  0.042684  0.0  0.0  0.0  0.000000   \n",
            "anomalyscore_h0_auc_over_l2    0.003261  0.040650  0.0  0.0  0.0  0.000039   \n",
            "anomalyscore_bottleneck        0.003667  0.042684  0.0  0.0  0.0  0.000000   \n",
            "anomalyscore_tail_q90          0.004023  0.040969  0.0  0.0  0.0  0.000239   \n",
            "anomalyscore_sum_centroid      0.003977  0.040318  0.0  0.0  0.0  0.000121   \n",
            "anomalyscore_h0_l2norm         0.005511  0.044860  0.0  0.0  0.0  0.000245   \n",
            "anomalyscore_pete              0.003937  0.040502  0.0  0.0  0.0  0.000108   \n",
            "anomalyscore_h0_energy_conc    0.002730  0.035756  0.0  0.0  0.0  0.000033   \n",
            "anomalyscore_h0_dom_share      0.004023  0.040969  0.0  0.0  0.0  0.000239   \n",
            "anomalyscore_h0_tail_curve     0.004979  0.043575  0.0  0.0  0.0  0.000387   \n",
            "anomalyscore_h0_cen_to_energy  0.002730  0.035756  0.0  0.0  0.0  0.000033   \n",
            "anomalyscore_h0_gini           0.008052  0.050527  0.0  0.0  0.0  0.001039   \n",
            "\n",
            "                               max  count_below_0.001 (GT=1)  \n",
            "anomalyscore_h0_auc            1.0                      67.0  \n",
            "anomalyscore_h0_auc_over_max   1.0                      65.0  \n",
            "anomalyscore_h0_auc_over_l2    1.0                      65.0  \n",
            "anomalyscore_bottleneck        1.0                      65.0  \n",
            "anomalyscore_tail_q90          1.0                      74.0  \n",
            "anomalyscore_sum_centroid      1.0                      58.0  \n",
            "anomalyscore_h0_l2norm         1.0                      59.0  \n",
            "anomalyscore_pete              1.0                      59.0  \n",
            "anomalyscore_h0_energy_conc    1.0                      89.0  \n",
            "anomalyscore_h0_dom_share      1.0                      74.0  \n",
            "anomalyscore_h0_tail_curve     1.0                     151.0  \n",
            "anomalyscore_h0_cen_to_energy  1.0                      89.0  \n",
            "anomalyscore_h0_gini           1.0                      79.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VISUALIZATION OF TIME SERIES, ANOMALY SCORES AND ANOMALY POINTS"
      ],
      "metadata": {
        "id": "aqeg42Xz77jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plotting (with \"all\" option for dataset and feature)\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ========= CONFIG =========\n",
        "CSV_PATH = \"/content/anomaly_scores_a4.csv\"\n",
        "\n",
        "# Fixed schema per your description\n",
        "DATASET_COL = \"loadeddataset\"\n",
        "VALUE_COL   = \"y\"\n",
        "ANOM_COL    = \"is_anomaly\"\n",
        "SEG_COL     = \"segment\"\n",
        "POINT_COL   = \"point\"  # x-axis if present, else fallback to index\n",
        "\n",
        "# Allowed feature columns\n",
        "FEATURE_COLUMNS = [\n",
        "    \"anomalyscore_h0_auc\",\n",
        "    \"anomalyscore_h0_auc_over_max\",\n",
        "    \"anomalyscore_h0_auc_over_l2\",\n",
        "    \"anomalyscore_bottleneck\",\n",
        "    \"anomalyscore_tail_q90\",\n",
        "    \"anomalyscore_sum_centroid\",\n",
        "    \"anomalyscore_h0_l2norm\",\n",
        "    \"anomalyscore_pete\",\n",
        "    \"anomalyscore_h0_energy_conc\",\n",
        "    \"anomalyscore_h0_dom_share\",\n",
        "    \"anomalyscore_h0_tail_curve\",\n",
        "    \"anomalyscore_h0_cen_to_energy\",\n",
        "    \"anomalyscore_h0_gini\",\n",
        "]\n",
        "\n",
        "ALL_TOKEN = \"__ALL__\"\n",
        "\n",
        "# ========= HELPERS =========\n",
        "def read_csv_safely(path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, engine=\"python\")\n",
        "\n",
        "def normalize_boolish(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Convert anomaly flags to 0/1 integers robustly.\"\"\"\n",
        "    s = series.copy()\n",
        "    if s.dtype == bool:\n",
        "        return s.astype(int)\n",
        "    if s.dtype == object:\n",
        "        return s.astype(str).str.lower().isin([\"1\",\"true\",\"t\",\"yes\",\"y\"]).astype(int)\n",
        "    return (pd.to_numeric(s, errors=\"coerce\").fillna(0) != 0).astype(int)\n",
        "\n",
        "def derive_segment_starts_from_segment_col(seg_series: pd.Series) -> np.ndarray:\n",
        "    \"\"\"Return row indices where a new segment starts (0 included).\"\"\"\n",
        "    seg = seg_series.values\n",
        "    starts = [0]\n",
        "    for i in range(1, len(seg)):\n",
        "        if seg[i] != seg[i-1]:\n",
        "            starts.append(i)\n",
        "    return np.array(starts, dtype=int)\n",
        "\n",
        "def list_datasets(df: pd.DataFrame) -> list:\n",
        "    return df[DATASET_COL].astype(str).dropna().unique().tolist()\n",
        "\n",
        "def pick_dataset(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Ask user for dataset name (or 'all').\"\"\"\n",
        "    if DATASET_COL not in df.columns:\n",
        "        raise KeyError(f\"Required column '{DATASET_COL}' not found. Columns: {list(df.columns)}\")\n",
        "    sample = list_datasets(df)\n",
        "    preview = sample[:20]\n",
        "    print(f\"\\nDetected {len(sample)} dataset ids in '{DATASET_COL}'. Examples:\\n  {preview}\")\n",
        "    ds = input(\"Enter dataset name (exact/basename/substring) or type 'all': \").strip()\n",
        "    if not ds:\n",
        "        raise ValueError(\"Dataset name cannot be empty.\")\n",
        "    if ds.lower() == \"all\":\n",
        "        return ALL_TOKEN\n",
        "    return ds\n",
        "\n",
        "def filter_rows_by_dataset(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
        "    \"\"\"Exact match, else basename match, else substring (case-insensitive).\"\"\"\n",
        "    col = df[DATASET_COL].astype(str)\n",
        "    sub = df[col == dataset_name]\n",
        "    if len(sub) > 0:\n",
        "        return sub\n",
        "    # basename\n",
        "    bn = os.path.basename(dataset_name)\n",
        "    sub = df[col.map(os.path.basename) == bn]\n",
        "    if len(sub) > 0:\n",
        "        return sub\n",
        "    # substring (case-insensitive)\n",
        "    mask = col.str.contains(re.escape(bn), case=False, na=False)\n",
        "    sub = df[mask]\n",
        "    if len(sub) > 0:\n",
        "        return sub\n",
        "    raise ValueError(\n",
        "        f\"No rows match dataset name '{dataset_name}' in column '{DATASET_COL}'. \"\n",
        "        f\"Try one of: {df[DATASET_COL].astype(str).head(10).tolist()}\"\n",
        "    )\n",
        "\n",
        "def pick_feature() -> str:\n",
        "    \"\"\"Ask user for feature name (or 'all').\"\"\"\n",
        "    print(\"\\nAvailable feature columns:\")\n",
        "    for c in FEATURE_COLUMNS:\n",
        "        print(\"  -\", c)\n",
        "    feat = input(\"Enter FEATURE (anomaly score) column, or type 'all': \").strip()\n",
        "    if feat.lower() == \"all\":\n",
        "        return ALL_TOKEN\n",
        "    if feat not in FEATURE_COLUMNS:\n",
        "        raise KeyError(f\"Feature '{feat}' is not in the allowed list above.\")\n",
        "    return feat\n",
        "\n",
        "def plot_one(sub: pd.DataFrame, dataset_name: str, feature_col: str):\n",
        "    # x-axis\n",
        "    if POINT_COL in sub.columns:\n",
        "        x = pd.to_numeric(sub[POINT_COL], errors=\"coerce\").values\n",
        "        x_is_index = False\n",
        "    else:\n",
        "        sub = sub.copy()\n",
        "        sub[\"__idx\"] = np.arange(len(sub))\n",
        "        x = sub[\"__idx\"].values\n",
        "        x_is_index = True\n",
        "\n",
        "    # y and feature\n",
        "    y = pd.to_numeric(sub[VALUE_COL], errors=\"coerce\").astype(float).values\n",
        "    f = pd.to_numeric(sub[feature_col], errors=\"coerce\").astype(float).values\n",
        "\n",
        "    # anomalies (GT)\n",
        "    anomalies = normalize_boolish(sub[ANOM_COL])\n",
        "    anom_x_positions = list(np.where(anomalies.values == 1)[0])\n",
        "\n",
        "    # segment starts from segment changes\n",
        "    seg_starts = derive_segment_starts_from_segment_col(sub[SEG_COL])\n",
        "\n",
        "    # ---- Plot ----\n",
        "    fig, ax_left = plt.subplots(figsize=(14, 6))\n",
        "    ax_right = ax_left.twinx()\n",
        "\n",
        "    # Left axis: y\n",
        "    line_y, = ax_left.plot(x, y, linewidth=1.2, alpha=0.9, label=VALUE_COL)\n",
        "\n",
        "    # Right axis: feature\n",
        "    line_f, = ax_right.plot(x, f, linewidth=1.2, alpha=0.9, label=feature_col, color=\"red\")\n",
        "\n",
        "    # Anomaly dashed lines\n",
        "    for i in anom_x_positions:\n",
        "        ax_left.axvline(x=x[i], linestyle=\"--\", linewidth=0.85, alpha=0.75, color=\"black\")\n",
        "\n",
        "    # Segment start solid red lines\n",
        "    for i in seg_starts:\n",
        "        ax_left.axvline(x=x[i], linestyle=\"-\", linewidth=1.0, alpha=0.9, color=\"yellow\")\n",
        "\n",
        "    # Labels / legend\n",
        "    ttl_ds = os.path.basename(str(dataset_name))\n",
        "    x_name = POINT_COL if not x_is_index else \"row_index\"\n",
        "    ax_left.set_title(f\"{ttl_ds}: y (left) vs {feature_col} (right)  anomalies (dashed), segments (red)\")\n",
        "    ax_left.set_xlabel(x_name)\n",
        "    ax_left.set_ylabel(VALUE_COL)\n",
        "    ax_right.set_ylabel(feature_col)\n",
        "\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elems = [line_y, line_f]\n",
        "    if len(anom_x_positions) > 0:\n",
        "        legend_elems.append(Line2D([0], [0], color=\"black\", lw=1.0, ls=\"--\", label=\"GT anomaly\"))\n",
        "    if len(seg_starts) > 0:\n",
        "        legend_elems.append(Line2D([0], [0], color=\"red\", lw=1.0, ls=\"-\", label=\"Segment start\"))\n",
        "    ax_left.legend(handles=legend_elems, loc=\"upper left\")\n",
        "\n",
        "    ax_left.grid(True, which=\"both\", linestyle=\":\", linewidth=0.6, alpha=0.6)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    print(f\"Plotted dataset='{dataset_name}', feature='{feature_col}', rows={len(sub)}, \"\n",
        "          f\"anomalies={int(anomalies.sum())}, segments{len(seg_starts)}\")\n",
        "    plt.show()\n",
        "\n",
        "# ========= MAIN =========\n",
        "def main():\n",
        "    # Load\n",
        "    df = read_csv_safely(CSV_PATH)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"The CSV is empty.\")\n",
        "    for req in [DATASET_COL, VALUE_COL, ANOM_COL, SEG_COL]:\n",
        "        if req not in df.columns:\n",
        "            raise KeyError(f\"Required column '{req}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Print all options up front\n",
        "    all_ds = list_datasets(df)\n",
        "    print(f\"\\n=== OPTIONS ===\")\n",
        "    print(f\"- Datasets ({len(all_ds)}): {all_ds[:30]}{' ...' if len(all_ds) > 30 else ''}\")\n",
        "    print(f\"- Features ({len(FEATURE_COLUMNS)}): {FEATURE_COLUMNS}\")\n",
        "    print(\"Type 'all' at prompts to select all datasets or all features.\\n\")\n",
        "\n",
        "    # Inputs\n",
        "    dataset_name = pick_dataset(df)   # string or ALL_TOKEN\n",
        "    feature_sel  = pick_feature()     # string or ALL_TOKEN\n",
        "\n",
        "    if dataset_name == ALL_TOKEN and feature_sel == ALL_TOKEN:\n",
        "        print(\"\\nYou selected ALL datasets and ALL features.\")\n",
        "        print(\"Aborting plotting to avoid generating a huge number of figures.\")\n",
        "        print(\"Pick either a single dataset with 'all' features, or 'all' datasets with a single feature.\")\n",
        "        return\n",
        "\n",
        "    if dataset_name == ALL_TOKEN:\n",
        "        # Plot single feature for every dataset\n",
        "        if feature_sel == ALL_TOKEN:\n",
        "            # handled above\n",
        "            return\n",
        "        print(f\"\\nPlotting feature '{feature_sel}' for ALL datasets...\")\n",
        "        for ds in all_ds:\n",
        "            try:\n",
        "                sub = filter_rows_by_dataset(df, ds).copy().reset_index(drop=True)\n",
        "                plot_one(sub, ds, feature_sel)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Skipped '{ds}': {e}\")\n",
        "        return\n",
        "\n",
        "    # Single dataset path\n",
        "    sub_all = filter_rows_by_dataset(df, dataset_name).copy().reset_index(drop=True)\n",
        "\n",
        "    if feature_sel == ALL_TOKEN:\n",
        "        print(f\"\\nPlotting ALL features for dataset '{dataset_name}' ...\")\n",
        "        for feat in FEATURE_COLUMNS:\n",
        "            try:\n",
        "                plot_one(sub_all, dataset_name, feat)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Skipped feature '{feat}': {e}\")\n",
        "        return\n",
        "\n",
        "    # Single dataset + single feature\n",
        "    plot_one(sub_all, dataset_name, feature_sel)\n",
        "\n",
        "# Run\n",
        "main()\n"
      ],
      "metadata": {
        "id": "jXtfajBULDgu",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plotting the anomaly points and neighbourhoods\n",
        "# ========= NEW: window plot around a chosen anomaly =========\n",
        "WINDOW = 10  # +/- points to show\n",
        "DEFAULT_FEATURE = \"anomalyscore_h0_auc\"  # used if you choose 'all' features here\n",
        "\n",
        "def _ensure_point_index(sub: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Guarantee we have a positional row index and a numeric POINT_COL.\"\"\"\n",
        "    sub = sub.copy().reset_index(drop=True)\n",
        "    if POINT_COL in sub.columns:\n",
        "        sub[POINT_COL] = pd.to_numeric(sub[POINT_COL], errors=\"coerce\")\n",
        "    else:\n",
        "        sub[POINT_COL] = np.arange(len(sub), dtype=float)\n",
        "    return sub\n",
        "\n",
        "def _segment_of_row(sub: pd.DataFrame, row_i: int) -> int:\n",
        "    \"\"\"Return the segment value at row_i.\"\"\"\n",
        "    return int(pd.to_numeric(sub.loc[row_i, SEG_COL], errors=\"coerce\"))\n",
        "\n",
        "def _rows_for_window(center_i: int, n_rows: int, w: int = WINDOW) -> slice:\n",
        "    lo = max(0, center_i - w)\n",
        "    hi = min(n_rows, center_i + w + 1)\n",
        "    return slice(lo, hi)\n",
        "\n",
        "def _pick_point_via_segment(sub: pd.DataFrame) -> int:\n",
        "    \"\"\"Interactive: list segments and anomalies; return a chosen row index.\"\"\"\n",
        "    sub = sub.copy().reset_index(drop=True)\n",
        "    seg_vals = sub[SEG_COL].astype(int).values\n",
        "    anom = normalize_boolish(sub[ANOM_COL]).values\n",
        "\n",
        "    # Group anomaly row indices by segment\n",
        "    seg_to_rows = {}\n",
        "    for i, (s, a) in enumerate(zip(seg_vals, anom)):\n",
        "        if a == 1:\n",
        "            seg_to_rows.setdefault(s, []).append(i)\n",
        "\n",
        "    # Show a summary\n",
        "    print(\"\\nSegments with anomalies:\")\n",
        "    if not seg_to_rows:\n",
        "        raise RuntimeError(\"No ground-truth anomalies in the selected dataset.\")\n",
        "    for s, rows in sorted(seg_to_rows.items()):\n",
        "        pts = [int(sub.loc[i, POINT_COL]) if not pd.isna(sub.loc[i, POINT_COL]) else i for i in rows]\n",
        "        print(f\"  Segment {s}: {len(rows)} anomalies  points {pts[:20]}{' ...' if len(pts) > 20 else ''}\")\n",
        "\n",
        "    # Choose a segment\n",
        "    while True:\n",
        "        try:\n",
        "            seg_sel = int(input(\"Enter a SEGMENT number to inspect: \").strip())\n",
        "            if seg_sel in seg_to_rows:\n",
        "                break\n",
        "            print(\"Segment not in list; try again.\")\n",
        "        except Exception:\n",
        "            print(\"Please enter an integer segment id.\")\n",
        "\n",
        "    # Choose an anomaly point inside that segment\n",
        "    rows = seg_to_rows[seg_sel]\n",
        "    # Display full list to choose by ordinal\n",
        "    print(f\"\\nSegment {seg_sel} anomaly choices (index in this list  row_i  point):\")\n",
        "    for k, i in enumerate(rows):\n",
        "        pval = sub.loc[i, POINT_COL]\n",
        "        print(f\"  [{k}] row={i}  point={int(pval) if pd.notna(pval) else i}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            k = int(input(\"Pick anomaly by its [index] above: \").strip())\n",
        "            if 0 <= k < len(rows):\n",
        "                return rows[k]\n",
        "            print(\"Out of range; try again.\")\n",
        "        except Exception:\n",
        "            print(\"Please enter a valid integer index.\")\n",
        "\n",
        "def _find_row_by_point_value(sub: pd.DataFrame, point_value: int) -> int:\n",
        "    \"\"\"Find the FIRST row whose POINT_COL equals point_value; fallback: nearest by absolute diff.\"\"\"\n",
        "    sub = sub.copy().reset_index(drop=True)\n",
        "    if POINT_COL in sub.columns:\n",
        "        hits = np.where(sub[POINT_COL].astype(float).values == float(point_value))[0]\n",
        "        if len(hits) > 0:\n",
        "            return int(hits[0])\n",
        "        # nearest fallback\n",
        "        diffs = np.abs(sub[POINT_COL].astype(float).values - float(point_value))\n",
        "        return int(np.nanargmin(diffs))\n",
        "    return int(point_value)  # if POINT_COL missing, treat as row index\n",
        "\n",
        "def plot_window_around_anomaly(sub: pd.DataFrame, row_i: int, feature_col: str = DEFAULT_FEATURE, w: int = WINDOW):\n",
        "    \"\"\"Plot y + feature around a selected row_i with w window; mark anomalies and the chosen one.\"\"\"\n",
        "    sub = _ensure_point_index(sub)\n",
        "    if feature_col not in sub.columns:\n",
        "        raise KeyError(f\"Feature '{feature_col}' not in dataframe columns.\")\n",
        "\n",
        "    # Determine window slice\n",
        "    n = len(sub)\n",
        "    win = _rows_for_window(row_i, n, w=w)\n",
        "    subw = sub.iloc[win].copy()\n",
        "\n",
        "    # Axis vectors\n",
        "    x = pd.to_numeric(subw[POINT_COL], errors=\"coerce\").astype(float).values\n",
        "    y = pd.to_numeric(subw[VALUE_COL], errors=\"coerce\").astype(float).values\n",
        "    f = pd.to_numeric(subw[feature_col], errors=\"coerce\").astype(float).values\n",
        "\n",
        "    # Flags\n",
        "    anom = normalize_boolish(subw[ANOM_COL]).values\n",
        "    chosen_x = float(sub.loc[row_i, POINT_COL])\n",
        "\n",
        "    # Get full-series segment starts (yellow) within window for context\n",
        "    seg_change_rows = derive_segment_starts_from_segment_col(sub[SEG_COL])\n",
        "    seg_change_points = [float(sub.loc[i, POINT_COL]) for i in seg_change_rows if win.start <= i < win.stop]\n",
        "\n",
        "    # ---- Plot ----\n",
        "    fig, ax_left = plt.subplots(figsize=(12, 5))\n",
        "    ax_right = ax_left.twinx()\n",
        "\n",
        "    ax_left.plot(x, y, linewidth=1.2, alpha=0.9, label=VALUE_COL)\n",
        "    ax_right.plot(x, f, linewidth=1.0, alpha=0.9, label=feature_col, color=\"red\")\n",
        "\n",
        "    # All GT anomalies in window (dashed)\n",
        "    for xi, ai in zip(x, anom):\n",
        "        if ai == 1:\n",
        "            ax_left.axvline(x=xi, linestyle=\"--\", linewidth=0.9, alpha=0.7, color=\"black\")\n",
        "\n",
        "    # Segment starts (yellow)\n",
        "    for xp in seg_change_points:\n",
        "        ax_left.axvline(x=xp, linestyle=\"-\", linewidth=1.0, alpha=0.9, color=\"yellow\")\n",
        "\n",
        "    # Chosen anomaly (big X)\n",
        "    ax_left.scatter([chosen_x], [np.interp(chosen_x, x, y)], marker=\"x\", s=120, linewidths=2.0, zorder=5)\n",
        "\n",
        "    # Titles / labels\n",
        "    seg_id = _segment_of_row(sub, row_i)\n",
        "    ax_left.set_title(f\"Segment {seg_id}  window {w} around point={int(chosen_x)} (row {row_i})\\n\"\n",
        "                      f\"y (left) vs {feature_col} (right)\")\n",
        "    ax_left.set_xlabel(POINT_COL)\n",
        "    ax_left.set_ylabel(VALUE_COL)\n",
        "    ax_right.set_ylabel(feature_col)\n",
        "    ax_left.grid(True, which=\"both\", linestyle=\":\", linewidth=0.6, alpha=0.6)\n",
        "\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elems = [\n",
        "        Line2D([0], [0], color=\"C0\", lw=1.2, label=VALUE_COL),\n",
        "        Line2D([0], [0], color=\"red\", lw=1.0, label=feature_col),\n",
        "        Line2D([0], [0], color=\"black\", lw=1.0, ls=\"--\", label=\"GT anomaly\"),\n",
        "        Line2D([0], [0], marker=\"x\", color=\"black\", lw=0, markersize=8, label=\"Chosen anomaly\"),\n",
        "        Line2D([0], [0], color=\"yellow\", lw=1.0, label=\"Segment start\"),\n",
        "    ]\n",
        "    ax_left.legend(handles=legend_elems, loc=\"upper left\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def interactive_window_plot():\n",
        "    # Load & sanity\n",
        "    df = read_csv_safely(CSV_PATH)\n",
        "    for req in [DATASET_COL, VALUE_COL, ANOM_COL, SEG_COL]:\n",
        "        if req not in df.columns:\n",
        "            raise KeyError(f\"Required column '{req}' not found. Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Pick dataset\n",
        "    all_ds = list_datasets(df)\n",
        "    print(f\"\\nDatasets found ({len(all_ds)}): {all_ds[:30]}{' ...' if len(all_ds) > 30 else ''}\")\n",
        "    ds = pick_dataset(df)\n",
        "    if ds == ALL_TOKEN:\n",
        "        raise RuntimeError(\"Please pick a single dataset for window plotting.\")\n",
        "\n",
        "    sub = filter_rows_by_dataset(df, ds).copy().reset_index(drop=True)\n",
        "    sub = _ensure_point_index(sub)\n",
        "\n",
        "    # Offer feature choice (or keep default)\n",
        "    print(\"\\nAvailable feature columns:\")\n",
        "    for c in FEATURE_COLUMNS:\n",
        "        print(\"  -\", c)\n",
        "    feat = input(f\"Feature to plot (Enter for default '{DEFAULT_FEATURE}'): \").strip()\n",
        "    if not feat:\n",
        "        feat = DEFAULT_FEATURE\n",
        "    if feat not in sub.columns:\n",
        "        raise KeyError(f\"Feature '{feat}' not in dataframe for this dataset.\")\n",
        "\n",
        "    # Option A: user already knows a point number\n",
        "    raw = input(\"\\nEnter an anomaly POINT value to jump to (or press Enter to browse by segment): \").strip()\n",
        "    if raw:\n",
        "        try:\n",
        "            point_value = int(float(raw))\n",
        "        except Exception:\n",
        "            raise ValueError(\"Please enter a numeric point value.\")\n",
        "        row_i = _find_row_by_point_value(sub, point_value)\n",
        "        seg_id = _segment_of_row(sub, row_i)\n",
        "\n",
        "        # Show segment and anomalies in that segment\n",
        "        seg_mask = (sub[SEG_COL].astype(int) == seg_id)\n",
        "        seg_rows = sub[seg_mask].reset_index(drop=True)\n",
        "        seg_anom_rows = np.where(normalize_boolish(seg_rows[ANOM_COL]).values == 1)[0]\n",
        "        seg_anom_points = [int(seg_rows.loc[i, POINT_COL]) for i in seg_anom_rows]\n",
        "        print(f\"\\nPoint {point_value} is in SEGMENT {seg_id}.\")\n",
        "        print(f\"Segment {seg_id} anomaly points: {seg_anom_points[:50]}{' ...' if len(seg_anom_points) > 50 else ''}\")\n",
        "\n",
        "        # Confirm or pick one of the listed anomalies to plot\n",
        "        confirm = input(\"Plot the window around this point? (y to confirm, anything else to pick from segment): \").strip().lower()\n",
        "        if confirm == \"y\":\n",
        "            plot_window_around_anomaly(sub, row_i=row_i, feature_col=feat, w=WINDOW)\n",
        "            return\n",
        "        else:\n",
        "            # Let user pick from anomalies of this segment\n",
        "            if not len(seg_anom_rows):\n",
        "                raise RuntimeError(\"Selected segment has no GT anomalies.\")\n",
        "            print(\"Pick an index among the listed segment anomalies.\")\n",
        "            while True:\n",
        "                try:\n",
        "                    p = int(input(\"Enter a POINT value from the list: \").strip())\n",
        "                    row_i = _find_row_by_point_value(sub, p)\n",
        "                    break\n",
        "                except Exception:\n",
        "                    print(\"Please enter a valid integer POINT.\")\n",
        "            plot_window_around_anomaly(sub, row_i=row_i, feature_col=feat, w=WINDOW)\n",
        "            return\n",
        "\n",
        "    # Option B: browse segments  anomalies\n",
        "    print(\"\\nNo point entered; browsing by segment\")\n",
        "    row_i = _pick_point_via_segment(sub)\n",
        "    plot_window_around_anomaly(sub, row_i=row_i, feature_col=feat, w=WINDOW)\n",
        "\n",
        "# Run the interactive flow\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_window_plot()\n"
      ],
      "metadata": {
        "id": "7XXNzsoQVf74",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WHAT IS OUR MAXIMUM ANOMALY DETECTION CAPABILITY BY K TOP ANALYSIS"
      ],
      "metadata": {
        "id": "tjxUVAZh7qrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title by ground truth K value\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Settings ---\n",
        "CSV_PATH = \"/content/anomaly_scores_a4.csv\"   # standard CSV\n",
        "NTOL = 3                                # tolerance window for SCORING ONLY\n",
        "FEATURES = [\n",
        "    \"anomalyscore_h0_auc\",            # max-tri / sumL\n",
        "    \"anomalyscore_h0_auc_over_max\",   # max-tri / maxL\n",
        "    \"anomalyscore_h0_auc_over_l2\",    # max-tri / L2(L)\n",
        " 'anomalyscore_bottleneck',\n",
        "       'anomalyscore_tail_q90', 'anomalyscore_sum_centroid',\n",
        "       'anomalyscore_h0_l2norm', 'anomalyscore_pete',\n",
        "       'anomalyscore_h0_energy_conc', 'anomalyscore_h0_dom_share',\n",
        "       'anomalyscore_h0_tail_curve', 'anomalyscore_h0_cen_to_energy',    'anomalyscore_h0_gini'\n",
        "]\n",
        "\n",
        "GROUP_COLS = [\"loadeddataset\", \"segment\"]\n",
        "\n",
        "# --- Load (simple) ---\n",
        "df = pd.read_csv(CSV_PATH).fillna(0)\n",
        "df[\"is_anomaly\"] = pd.to_numeric(df[\"is_anomaly\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "for c in FEATURES:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "y_true = df[\"is_anomaly\"].values\n",
        "N = len(df)\n",
        "total_gt = int(y_true.sum())\n",
        "print(\"Total ground-truth anomalies:\", total_gt)\n",
        "\n",
        "# Precompute groups once\n",
        "groups = df.groupby(GROUP_COLS).indices  # dict: (ds, seg) -> ndarray of row indices (global)\n",
        "\n",
        "def topk_pred_indices_for_feature(feat: str) -> np.ndarray:\n",
        "    \"\"\"Return global indices predicted as 1 by taking K=GT per segment, top-K by 'feat'.\"\"\"\n",
        "    y_pred = np.zeros(N, dtype=int)\n",
        "    for ix in groups.values():\n",
        "        ix = np.asarray(ix, dtype=int)\n",
        "        K = int(df.loc[ix, \"is_anomaly\"].sum())\n",
        "        if K <= 0:\n",
        "            continue\n",
        "        scores = df.loc[ix, feat].values\n",
        "        if K >= len(ix):\n",
        "            chosen_local = np.arange(len(ix))\n",
        "        else:\n",
        "            chosen_local = np.argpartition(-scores, K-1)[:K]  # top-K unsorted\n",
        "        y_pred[ix[chosen_local]] = 1\n",
        "    return np.where(y_pred == 1)[0]\n",
        "\n",
        "def score_with_tolerance(pred_idx: np.ndarray, true_idx: np.ndarray, ntol: int):\n",
        "    \"\"\"\n",
        "    Greedy matching for tolerance:\n",
        "    - A prediction i is TP if there exists an *unmatched* ground-truth t with |i - t| <= ntol.\n",
        "    - Each ground-truth can be matched at most once.\n",
        "    \"\"\"\n",
        "    if ntol < 0:\n",
        "        ntol = 0\n",
        "    pred_idx = np.sort(pred_idx)\n",
        "    true_idx = np.sort(true_idx)\n",
        "\n",
        "    tp = 0\n",
        "    used_true = np.zeros(true_idx.size, dtype=bool)\n",
        "    t_ptr = 0  # pointer over true_idx\n",
        "\n",
        "    for i in pred_idx:\n",
        "        # advance pointer to first true >= i-ntol\n",
        "        while t_ptr < true_idx.size and true_idx[t_ptr] < i - ntol:\n",
        "            t_ptr += 1\n",
        "        # check current and maybe previous one (nearest within window)\n",
        "        matched = False\n",
        "        # try current t_ptr\n",
        "        if t_ptr < true_idx.size and (not used_true[t_ptr]) and abs(true_idx[t_ptr] - i) <= ntol:\n",
        "            used_true[t_ptr] = True\n",
        "            tp += 1\n",
        "            matched = True\n",
        "        # also try left neighbor if exists and closer\n",
        "        if not matched and t_ptr > 0:\n",
        "            left = t_ptr - 1\n",
        "            if (not used_true[left]) and abs(true_idx[left] - i) <= ntol:\n",
        "                used_true[left] = True\n",
        "                tp += 1\n",
        "                matched = True\n",
        "        # if not matched, it's FP; handled later by counts\n",
        "\n",
        "    fp = int(pred_idx.size - tp)\n",
        "    fn = int(true_idx.size - tp)\n",
        "    tn = int(N - tp - fp - fn)\n",
        "    return tp, fp, fn, tn\n",
        "\n",
        "rows = []\n",
        "true_idx = np.where(y_true == 1)[0]\n",
        "sum_K_over_segments = sum(int(df.loc[ix, \"is_anomaly\"].sum()) for ix in groups.values())\n",
        "print(\"Sum of K over segments:\", sum_K_over_segments)\n",
        "\n",
        "for feat in FEATURES:\n",
        "    pred_idx = topk_pred_indices_for_feature(feat)    # predictions (indices)  EXACTLY K per segment\n",
        "    print(f\"{feat}: predicted ones (no tolerance applied to predictions) = {pred_idx.size}\")\n",
        "\n",
        "    tp, fp, fn, tn = score_with_tolerance(pred_idx, true_idx, NTOL)\n",
        "\n",
        "    # Metrics\n",
        "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
        "    recall    = tp / (tp + fn) if (tp + fn) else 0.0\n",
        "    f1        = (2*precision*recall)/(precision+recall) if (precision+recall) else 0.0\n",
        "\n",
        "    rows.append({\n",
        "        \"feature\": feat,\n",
        "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn,\n",
        "        \"Precision\": precision, \"Recall\": recall, \"F1\": f1,\n",
        "        \"pred_ones\": int(pred_idx.size)\n",
        "    })\n",
        "\n",
        "out = pd.DataFrame(rows).sort_values(\"F1\", ascending=False).reset_index(drop=True)\n",
        "print(f\"\\n=== Top-K per segment (K from GT)  Scored with {NTOL} tolerance (predictions unchanged) ===\")\n",
        "print(out.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "EJozrk9U0EZX",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title individual results of anomaly points\n",
        "# ==== TP/FP for predictions AND TP/FN for ground truth (NO TNs, print ALL rows) ====\n",
        "\n",
        "DATASET_COL = \"loadeddataset\"\n",
        "SEG_COL     = \"segment\"\n",
        "POINT_COL   = \"point\"\n",
        "ANOM_COL    = \"is_anomaly\"\n",
        "\n",
        "def _greedy_match_with_tolerance(pred_idx: np.ndarray,\n",
        "                                 true_idx: np.ndarray,\n",
        "                                 ntol: int):\n",
        "    \"\"\"\n",
        "    Greedy one-to-one matching between predictions and ground-truth positives (anomalies),\n",
        "    using ntol tolerance on indices.\n",
        "\n",
        "    Returns:\n",
        "      pred_to_label: dict[pred_i] = (\"TP\", matched_true_i) or (\"FP\", None)\n",
        "      true_to_hit:   dict[true_i] = matched_pred_i or None  (None => FN)\n",
        "    \"\"\"\n",
        "    pred_idx = np.sort(np.asarray(pred_idx, dtype=int))\n",
        "    true_idx = np.sort(np.asarray(true_idx, dtype=int))\n",
        "\n",
        "    pred_to_label = {}\n",
        "    true_to_hit = {int(t): None for t in true_idx}\n",
        "\n",
        "    used_true = np.zeros(true_idx.size, dtype=bool)\n",
        "    t_ptr = 0\n",
        "\n",
        "    for i in pred_idx:\n",
        "        while t_ptr < true_idx.size and true_idx[t_ptr] < i - ntol:\n",
        "            t_ptr += 1\n",
        "\n",
        "        matched = False\n",
        "        # try current\n",
        "        if t_ptr < true_idx.size and (not used_true[t_ptr]) and abs(true_idx[t_ptr] - i) <= ntol:\n",
        "            used_true[t_ptr] = True\n",
        "            pred_to_label[i] = (\"TP\", int(true_idx[t_ptr]))\n",
        "            true_to_hit[int(true_idx[t_ptr])] = int(i)\n",
        "            matched = True\n",
        "\n",
        "        # try left neighbor\n",
        "        if not matched and t_ptr > 0:\n",
        "            left = t_ptr - 1\n",
        "            if (not used_true[left]) and abs(true_idx[left] - i) <= ntol:\n",
        "                used_true[left] = True\n",
        "                pred_to_label[i] = (\"TP\", int(true_idx[left]))\n",
        "                true_to_hit[int(true_idx[left])] = int(i)\n",
        "                matched = True\n",
        "\n",
        "        if not matched:\n",
        "            pred_to_label[i] = (\"FP\", None)\n",
        "\n",
        "    return pred_to_label, true_to_hit\n",
        "\n",
        "def _mk_addr_table(idxs: np.ndarray, extra_cols: dict = None) -> pd.DataFrame:\n",
        "    \"\"\"Build (dataset, segment, point, global_index) table; optionally add extra cols.\"\"\"\n",
        "    if idxs.size == 0:\n",
        "        base = pd.DataFrame(columns=[DATASET_COL, SEG_COL, POINT_COL, \"global_index\"])\n",
        "    else:\n",
        "        base = df.loc[idxs, [DATASET_COL, SEG_COL, POINT_COL]].copy()\n",
        "        base[\"global_index\"] = idxs\n",
        "    if extra_cols:\n",
        "        for k, v in extra_cols.items():\n",
        "            base[k] = v\n",
        "    return base.sort_values([DATASET_COL, SEG_COL, POINT_COL, \"global_index\"]).reset_index(drop=True)\n",
        "\n",
        "def list_tp_fp_and_gt_status_for_feature(feature_name: str):\n",
        "    if feature_name not in FEATURES:\n",
        "        raise ValueError(f\"Feature '{feature_name}' not in FEATURES.\\nAvailable: {FEATURES}\")\n",
        "\n",
        "    # 1) Predictions for this feature (Top-K per segment)\n",
        "    pred_idx = topk_pred_indices_for_feature(feature_name)\n",
        "\n",
        "    # 2) Ground truth indices (positives only)\n",
        "    true_idx = np.where(y_true == 1)[0]\n",
        "\n",
        "    # 3) Greedy matching with NTOL\n",
        "    pred_labels, true_hits = _greedy_match_with_tolerance(pred_idx, true_idx, NTOL)\n",
        "\n",
        "    # ---- PREDICTIONS table (TP/FP) ----\n",
        "    statuses = []\n",
        "    matched_true_idx = []\n",
        "    for i in pred_idx:\n",
        "        lab, t_i = pred_labels[i]\n",
        "        statuses.append(lab)               # \"TP\" or \"FP\"\n",
        "        matched_true_idx.append(t_i)       # matched GT index if TP else None\n",
        "\n",
        "    pred_tbl = _mk_addr_table(\n",
        "        pred_idx,\n",
        "        extra_cols={\n",
        "            \"status\": statuses,\n",
        "            \"matched_true_index\": matched_true_idx,\n",
        "            \"matched_true_point\": [\n",
        "                (None if t is None else df.loc[int(t), POINT_COL]) for t in matched_true_idx\n",
        "            ],\n",
        "            \"score\": df.loc[pred_idx, feature_name].values\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # ---- GROUND TRUTH table (TP/FN only) ----\n",
        "    gt_status = []\n",
        "    matched_pred_idx = []\n",
        "    for t in true_idx:\n",
        "        p = true_hits[int(t)]\n",
        "        if p is None:\n",
        "            gt_status.append(\"FN\")\n",
        "            matched_pred_idx.append(None)\n",
        "        else:\n",
        "            gt_status.append(\"TP\")\n",
        "            matched_pred_idx.append(int(p))\n",
        "\n",
        "    gt_tbl = _mk_addr_table(\n",
        "        true_idx,\n",
        "        extra_cols={\n",
        "            \"gt_status\": gt_status,                 # \"TP\" or \"FN\"\n",
        "            \"matched_pred_index\": matched_pred_idx,\n",
        "            \"matched_pred_point\": [\n",
        "                (None if p is None else df.loc[int(p), POINT_COL]) for p in matched_pred_idx\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # ---- Print ALL rows (no head/limit) ----\n",
        "    print(f\"\\n=== ALL Predictions for '{feature_name}' labeled ({NTOL}) ===\")\n",
        "    if len(pred_tbl):\n",
        "        print(pred_tbl.to_string(index=False))\n",
        "    else:\n",
        "        print(\"(no predictions)\")\n",
        "\n",
        "    print(f\"\\n=== ALL Ground-truth anomalies labeled TP/FN ({NTOL}) ===\")\n",
        "    if len(gt_tbl):\n",
        "        print(gt_tbl.to_string(index=False))\n",
        "    else:\n",
        "        print(\"(no ground-truth anomalies)\")\n",
        "\n",
        "    # ---- Summary counts (no TN reported) ----\n",
        "    tp_pred = int((pred_tbl[\"status\"] == \"TP\").sum())\n",
        "    fp_pred = int((pred_tbl[\"status\"] == \"FP\").sum())\n",
        "    tp_gt   = int((gt_tbl[\"gt_status\"] == \"TP\").sum())\n",
        "    fn_gt   = int((gt_tbl[\"gt_status\"] == \"FN\").sum())\n",
        "    print(f\"\\nCounts  TP: {tp_gt} | FP: {fp_pred} | FN: {fn_gt}\")\n",
        "\n",
        "    return pred_tbl, gt_tbl\n",
        "\n",
        "# ---- Run interactively ----\n",
        "SELECTED_FEATURE = input(f\"\\nEnter a feature to list ALL PRED (TP/FP) and ALL GT (TP/FN):\\n{FEATURES}\\n> \").strip() or FEATURES[0]\n",
        "pred_table, gt_table = list_tp_fp_and_gt_status_for_feature(SELECTED_FEATURE)\n"
      ],
      "metadata": {
        "id": "Ojh2263Eg9P1",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOW WE CAN DETECT ANOMALIES WITH UNSUPERVISED METHODS"
      ],
      "metadata": {
        "id": "AhN-wYyL7YMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title thresholding by quantile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def run_all_score_analyses(file_path='/content/anomaly_scores_a3.csv'):\n",
        "    \"\"\"\n",
        "    This script performs adaptive anomaly detection and NTOL evaluation\n",
        "    for EVERY 'anomalyscore_' column in the provided CSV file.\n",
        "\n",
        "    The strategy is as follows:\n",
        "    1.  Load the dataset.\n",
        "    2.  Identify all columns starting with 'anomalyscore_'.\n",
        "    3.  Initialize a DataFrame to store the final performance metrics\n",
        "        for comparison.\n",
        "    4.  Loop through each identified anomaly score column:\n",
        "        a.  Print which score is being processed.\n",
        "        b.  Clean the data by dropping rows where *this specific score*\n",
        "            is NaN.\n",
        "        c.  Calculate adaptive 99th percentile thresholds for this score,\n",
        "            grouped by 'loadeddataset' and 'segment'.\n",
        "        d.  Generate 'predicted_anomaly' based on this score and its\n",
        "            group-specific thresholds.\n",
        "        e.  Apply the NTOL=3 logic by dilating the 'is_anomaly'\n",
        "            (ground truth) and the new 'predicted_anomaly' columns.\n",
        "        f.  Calculate the custom point-adjusted TP, FN, FP, and TN.\n",
        "        g.  Calculate and print the F1, Precision, and Recall for this\n",
        "            specific score.\n",
        "        h.  Store these metrics in the summary DataFrame.\n",
        "    5.  After the loop finishes, print the final summary DataFrame,\n",
        "        sorted by F1 score, to show which score performed best.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    data_file = file_path\n",
        "    quantile_threshold = 0.99\n",
        "    NTOL = 3\n",
        "    window_size = 2 * NTOL + 1\n",
        "\n",
        "    print(f\"Starting adaptive anomaly detection process for ALL scores...\")\n",
        "    print(f\"Using data file: {data_file}\")\n",
        "    print(f\"Using quantile threshold: {quantile_threshold} (99th percentile)\")\n",
        "    print(f\"Using evaluation NTOL: {NTOL} (Window size: {window_size})\\n\")\n",
        "\n",
        "    try:\n",
        "        # --- 2. Load Data ---\n",
        "        df = pd.read_csv(data_file)\n",
        "        if 'is_anomaly' not in df.columns:\n",
        "            print(\"Error: 'is_anomaly' column not found.\")\n",
        "            return\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{data_file}' was not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Identify Score Columns ---\n",
        "    all_score_columns = [col for col in df.columns if col.startswith('anomalyscore_')]\n",
        "\n",
        "    if not all_score_columns:\n",
        "        print(\"Error: No columns found starting with 'anomalyscore_'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_score_columns)} score columns to analyze:\")\n",
        "    print(all_score_columns)\n",
        "\n",
        "    # --- 4. Initialize Summary ---\n",
        "    performance_summary = []\n",
        "\n",
        "    # Pre-calculate the dilated ground truth once\n",
        "    # This is efficient as it doesn't change\n",
        "    print(\"\\nPre-calculating dilated ground truth...\")\n",
        "    group_keys = ['loadeddataset', 'segment']\n",
        "    dilated_true_series = df.groupby(group_keys)['is_anomaly'] \\\n",
        "                              .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                              .max()\n",
        "    df['y_true_dilated'] = dilated_true_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                              .fillna(0).astype(int)\n",
        "    print(\"Dilated ground truth calculated.\")\n",
        "\n",
        "    # --- 5. Loop Through Each Score Column ---\n",
        "    for score_column in all_score_columns:\n",
        "        print(f\"\\n{'-'*20} Analyzing: {score_column} {'-'*20}\")\n",
        "\n",
        "        try:\n",
        "            # --- 5a. Clean Data (specific to this column) ---\n",
        "            # We use a copy to ensure each loop is independent\n",
        "            df_cleaned = df.dropna(subset=[score_column]).copy()\n",
        "\n",
        "            if len(df_cleaned) == 0:\n",
        "                print(f\"Skipping {score_column}: All values are NaN.\")\n",
        "                continue\n",
        "\n",
        "            # --- 5b. Calculate Adaptive Thresholds & Predictions ---\n",
        "            print(\"Calculating thresholds and making predictions...\")\n",
        "            df_cleaned['adaptive_threshold'] = df_cleaned.groupby(group_keys)[score_column] \\\n",
        "                                                         .transform(lambda x: x.quantile(quantile_threshold))\n",
        "\n",
        "            df_cleaned['predicted_anomaly'] = (df_cleaned[score_column] > df_cleaned['adaptive_threshold']).astype(int)\n",
        "\n",
        "            # --- 5c. Apply NTOL Logic to Predictions ---\n",
        "            print(\"Applying NTOL to predictions...\")\n",
        "            dilated_pred_series = df_cleaned.groupby(group_keys)['predicted_anomaly'] \\\n",
        "                                            .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                                            .max()\n",
        "            df_cleaned['y_pred_dilated'] = dilated_pred_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                                              .fillna(0).astype(int)\n",
        "\n",
        "            # --- 5d. Calculate Custom Metrics ---\n",
        "            y_true = df_cleaned['is_anomaly']\n",
        "            y_pred = df_cleaned['predicted_anomaly']\n",
        "            y_true_dilated = df_cleaned['y_true_dilated'] # from pre-computation\n",
        "            y_pred_dilated = df_cleaned['y_pred_dilated']\n",
        "\n",
        "            TP = ((y_true == 1) & (y_pred_dilated == 1)).sum()\n",
        "            FN = ((y_true == 1) & (y_pred_dilated == 0)).sum()\n",
        "            FP = ((y_pred == 1) & (y_true_dilated == 0)).sum()\n",
        "            TN = ((y_true == 0) & (y_pred_dilated == 0)).sum()\n",
        "\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            # --- 5e. Print Individual Report ---\n",
        "            print(f\"--- Results for {score_column} ---\")\n",
        "            print(f\"Total Actual Anomaly Points: {y_true.sum()}\")\n",
        "            print(f\"Total Predicted Anomaly Points: {y_pred.sum()}\\n\")\n",
        "\n",
        "            print(\"Custom Confusion Matrix (Point-Adjusted):\")\n",
        "            print(\"                 Predicted Negative | Predicted Positive\")\n",
        "            print(f\"Actual Negative: {TN:>17} | {FP:>17}\")\n",
        "            print(f\"Actual Positive: {FN:>17} | {TP:>17}\\n\")\n",
        "\n",
        "            print(\"Performance Metrics (Point-Adjusted):\")\n",
        "            print(f\"  F1 Score:  {f1:.6f}\")\n",
        "            print(f\"  Precision: {precision:.6f}\")\n",
        "            print(f\"  Recall:    {recall:.6f}\")\n",
        "\n",
        "            # --- 5f. Store in Summary ---\n",
        "            performance_summary.append({\n",
        "                'score_column': score_column,\n",
        "                'f1_score': f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'TP': TP,\n",
        "                'FN': FN,\n",
        "                'FP': FP,\n",
        "                'total_predicted': y_pred.sum()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {score_column}: {e} !!!\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- 6. Print Final Summary Table ---\n",
        "    print(f\"\\n\\n{'-'*20} FINAL PERFORMANCE SUMMARY {'-'*20}\")\n",
        "\n",
        "    if performance_summary:\n",
        "        summary_df = pd.DataFrame(performance_summary)\n",
        "        summary_df = summary_df.sort_values(by='f1_score', ascending=False)\n",
        "        summary_df = summary_df.set_index('score_column')\n",
        "\n",
        "        print(summary_df.to_string(float_format=\"%.6f\"))\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No score columns were successfully processed.\")\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_score_analyses()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Okf33kU7ESdB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title POT\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "def find_anomalies_for_segment_series(segment_scores, k_val, distance):\n",
        "    \"\"\"\n",
        "    This is the core helper function that applies the adaptive peak-finding\n",
        "    logic to a single time series segment (passed as a pandas Series).\n",
        "\n",
        "    Rules:\n",
        "    1. Find all peaks and their prominences.\n",
        "    2. Calculate an adaptive prominence threshold: mean(prominences) + k * std(prominences).\n",
        "    3. Run find_peaks again, using this adaptive threshold and a minimum distance.\n",
        "    4. Return a series of 0s (not anomaly) and 1s (anomaly).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Handle edge cases ---\n",
        "    # If the segment is flat or has no variance, there are no peaks.\n",
        "    if segment_scores.nunique() <= 1:\n",
        "        return pd.Series(0, index=segment_scores.index, dtype=int)\n",
        "\n",
        "    try:\n",
        "        # --- 2. Find all peaks to get their prominence distribution ---\n",
        "        all_peaks_indices, all_props = find_peaks(segment_scores,\n",
        "                                                  prominence=(None, None))\n",
        "\n",
        "        # If no peaks are found at all, return all zeros.\n",
        "        if len(all_peaks_indices) == 0:\n",
        "            return pd.Series(0, index=segment_scores.index, dtype=int)\n",
        "\n",
        "        prominences = all_props['prominences']\n",
        "\n",
        "        # --- 3. Calculate adaptive prominence threshold ---\n",
        "        if len(prominences) == 1:\n",
        "            # If there's only one peak, it is by definition an anomaly\n",
        "            adaptive_prominence_threshold = 0\n",
        "        else:\n",
        "            prom_mean = np.mean(prominences)\n",
        "            prom_std = np.std(prominences)\n",
        "\n",
        "            # If all prominences are identical, std is 0. Threshold is just the mean.\n",
        "            if prom_std == 0:\n",
        "                adaptive_prominence_threshold = prom_mean\n",
        "            else:\n",
        "                adaptive_prominence_threshold = prom_mean + (k_val * prom_std)\n",
        "\n",
        "        # --- 4. Run find_peaks a final time with the full rule set ---\n",
        "        # We find peaks that are:\n",
        "        # a) Above the adaptive prominence threshold\n",
        "        # b) At least 'distance' points apart\n",
        "        final_peak_indices, _ = find_peaks(segment_scores,\n",
        "                                           prominence=(adaptive_prominence_threshold, None),\n",
        "                                           distance=distance)\n",
        "\n",
        "        # --- 5. Create the final prediction series ---\n",
        "        predicted_anomalies = pd.Series(0, index=segment_scores.index, dtype=int)\n",
        "\n",
        "        # Set the indices of the found peaks to 1\n",
        "        predicted_anomalies.iloc[final_peak_indices] = 1\n",
        "        return predicted_anomalies\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any errors during peak finding for a segment\n",
        "        print(f\"Warning: Error in peak finding for a segment: {e}\", file=sys.stderr)\n",
        "        return pd.Series(0, index=segment_scores.index, dtype=int)\n",
        "\n",
        "\n",
        "def run_peak_based_analysis_for_all_scores(file_path='/content/anomaly_scores_a3.csv'):\n",
        "    \"\"\"\n",
        "    Main script to run the advanced peak-based adaptive anomaly detection\n",
        "    for EVERY 'anomalyscore_' column in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    data_file = file_path\n",
        "\n",
        "    # --- Evaluation Config ---\n",
        "    NTOL = 3\n",
        "    window_size = 2 * NTOL + 1\n",
        "\n",
        "    # --- Peak-Finding Rule Set Config ---\n",
        "    # k-value for mean + k * std(prominence)\n",
        "    PEAK_K_VALUE = 2.2\n",
        "    # Distance (in points) between anomalies\n",
        "    PEAK_DISTANCE = 5\n",
        "\n",
        "    print(f\"Starting PEAK-BASED adaptive anomaly detection...\")\n",
        "    print(f\"Using data file: {data_file}\")\n",
        "    print(f\"Evaluation NTOL: {NTOL}\")\n",
        "    print(f\"Peak Rules: Prominence > (mean + {PEAK_K_VALUE}*std), Distance = {PEAK_DISTANCE}\\n\")\n",
        "\n",
        "    # Suppress warnings from scipy/numpy during processing\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "    try:\n",
        "        # --- 2. Load Data ---\n",
        "        df = pd.read_csv(data_file)\n",
        "        if 'is_anomaly' not in df.columns:\n",
        "            print(\"Error: 'is_anomaly' column not found.\")\n",
        "            return\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{data_file}' was not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Identify Score Columns ---\n",
        "    all_score_columns = [col for col in df.columns if col.startswith('anomalyscore_')]\n",
        "\n",
        "    if not all_score_columns:\n",
        "        print(\"Error: No columns found starting with 'anomalyscore_'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_score_columns)} score columns to analyze.\")\n",
        "\n",
        "    # --- 4. Initialize Summary ---\n",
        "    performance_summary = []\n",
        "    group_keys = ['loadeddataset', 'segment']\n",
        "\n",
        "    # --- 5. Pre-calculate Dilated Ground Truth (Optimization) ---\n",
        "    print(\"\\nPre-calculating dilated ground truth...\")\n",
        "    dilated_true_series = df.groupby(group_keys)['is_anomaly'] \\\n",
        "                              .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                              .max()\n",
        "    df['y_true_dilated'] = dilated_true_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                              .fillna(0).astype(int)\n",
        "    print(\"Dilated ground truth calculated.\")\n",
        "\n",
        "    # --- 6. Loop Through Each Score Column ---\n",
        "    for score_column in all_score_columns:\n",
        "        print(f\"\\n{'-'*25} Analyzing: {score_column} {'-'*25}\")\n",
        "\n",
        "        try:\n",
        "            # --- 6a. Clean Data (specific to this column) ---\n",
        "            df_cleaned = df.dropna(subset=[score_column]).copy()\n",
        "\n",
        "            if len(df_cleaned) == 0:\n",
        "                print(f\"Skipping {score_column}: All values are NaN.\")\n",
        "                continue\n",
        "\n",
        "            # --- 6b. Apply Peak-Finding Logic ---\n",
        "            print(\"Applying peak-finding logic to all segments...\")\n",
        "\n",
        "            # This is the core step. We group, select the score column,\n",
        "            # and apply our custom function to each segment (which is a Series).\n",
        "            predicted_series = df_cleaned.groupby(group_keys)[score_column].apply(\n",
        "                find_anomalies_for_segment_series,\n",
        "                k_val=PEAK_K_VALUE,\n",
        "                distance=PEAK_DISTANCE\n",
        "            )\n",
        "\n",
        "            # The result needs to be aligned back to the df_cleaned index\n",
        "            df_cleaned['predicted_anomaly'] = predicted_series.reset_index(level=group_keys, drop=True)\n",
        "            # Fill any potential NaNs from failed groups (should be rare)\n",
        "            df_cleaned['predicted_anomaly'] = df_cleaned['predicted_anomaly'].fillna(0).astype(int)\n",
        "\n",
        "            print(\"Prediction generation complete.\")\n",
        "\n",
        "            # --- 6c. Apply NTOL Logic to Predictions ---\n",
        "            print(\"Applying NTOL to predictions...\")\n",
        "            dilated_pred_series = df_cleaned.groupby(group_keys)['predicted_anomaly'] \\\n",
        "                                            .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                                            .max()\n",
        "            df_cleaned['y_pred_dilated'] = dilated_pred_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                                              .fillna(0).astype(int)\n",
        "\n",
        "            # --- 6d. Calculate Custom Metrics (NTOL) ---\n",
        "            y_true = df_cleaned['is_anomaly']\n",
        "            y_pred = df_cleaned['predicted_anomaly']\n",
        "            y_true_dilated = df_cleaned['y_true_dilated']\n",
        "            y_pred_dilated = df_cleaned['y_pred_dilated']\n",
        "\n",
        "            TP = ((y_true == 1) & (y_pred_dilated == 1)).sum()\n",
        "            FN = ((y_true == 1) & (y_pred_dilated == 0)).sum()\n",
        "            FP = ((y_pred == 1) & (y_true_dilated == 0)).sum()\n",
        "            TN = ((y_true == 0) & (y_pred_dilated == 0)).sum()\n",
        "\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            # --- 6e. Print Individual Report ---\n",
        "            print(f\"--- Results for {score_column} ---\")\n",
        "            print(f\"Total Actual Anomaly Points: {y_true.sum()}\")\n",
        "            print(f\"Total Predicted Anomaly Points (Peaks): {y_pred.sum()}\\n\")\n",
        "\n",
        "            print(\"Custom Confusion Matrix (Point-Adjusted):\")\n",
        "            print(\"                 Predicted Negative | Predicted Positive\")\n",
        "            print(f\"Actual Negative: {TN:>17} | {FP:>17}\")\n",
        "            print(f\"Actual Positive: {FN:>17} | {TP:>17}\\n\")\n",
        "\n",
        "            print(\"Performance Metrics (Point-Adjusted):\")\n",
        "            print(f\"  F1 Score:  {f1:.6f}\")\n",
        "            print(f\"  Precision: {precision:.6f}\")\n",
        "            print(f\"  Recall:    {recall:.6f}\")\n",
        "\n",
        "            # --- 6f. Store in Summary ---\n",
        "            performance_summary.append({\n",
        "                'score_column': score_column,\n",
        "                'f1_score': f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'TP': TP,\n",
        "                'FN': FN,\n",
        "                'FP': FP,\n",
        "                'total_predicted_peaks': y_pred.sum()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {score_column}: {e} !!!\", file=sys.stderr)\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # --- 7. Print Final Summary Table ---\n",
        "    print(f\"\\n\\n{'-'*25} FINAL PEAK-BASED PERFORMANCE SUMMARY {'-'*25}\")\n",
        "\n",
        "    if performance_summary:\n",
        "        summary_df = pd.DataFrame(performance_summary)\n",
        "        summary_df = summary_df.sort_values(by='f1_score', ascending=False)\n",
        "        summary_df = summary_df.set_index('score_column')\n",
        "\n",
        "        print(summary_df.to_string(float_format=\"%.6f\"))\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No score columns were successfully processed.\")\n",
        "\n",
        "    # Reset warnings\n",
        "    warnings.filterwarnings('default')\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_peak_based_analysis_for_all_scores()"
      ],
      "metadata": {
        "id": "zjRJ7mhtHCp8",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title EVT a3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import genpareto\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "def get_evt_threshold_for_segment(segment_scores, gate_quantile, final_quantile):\n",
        "    \"\"\"\n",
        "    This is the core helper function that applies the EVT/POT\n",
        "    logic to a single time series segment (passed as a pandas Series).\n",
        "\n",
        "    1. Sets a \"gate\" threshold 'u' at the 'gate_quantile'.\n",
        "    2. Collects all 'exceedances' (points > u).\n",
        "    3. Tries to fit a Generalized Pareto Distribution (GPD) to these exceedances.\n",
        "    4. Uses the fitted GPD to calculate the final, much rarer 'final_quantile' threshold.\n",
        "    5. If the GPD fit fails or is unstable (e.g., too few exceedances),\n",
        "       it safely falls back to just using the 'final_quantile' directly.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Set the \"gate\" threshold ---\n",
        "        u = segment_scores.quantile(gate_quantile)\n",
        "\n",
        "        # --- 2. Collect exceedances ---\n",
        "        # We need to drop NaNs and any values <= u\n",
        "        exceedances = segment_scores[segment_scores > u].dropna() - u\n",
        "\n",
        "        # --- 5. Fallback Logic ---\n",
        "        # If we have too few points (e.g., < 10) to fit a stable GPD,\n",
        "        # or if all values are identical (nunique=1), fall back to a simple quantile.\n",
        "        if len(exceedances) < 10 or exceedances.nunique() == 1:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # --- 3. Fit GPD ---\n",
        "        # Fit the GPD to the exceedances.\n",
        "        # We fix location 'floc=0' as exceedances start at 0.\n",
        "        c, loc, scale = genpareto.fit(exceedances, floc=0)\n",
        "\n",
        "        # --- 4. Calculate Final Threshold ---\n",
        "        # We need to find the threshold 'T' such that P(X > T) = (1 - final_quantile)\n",
        "        # T = u + GPD.isf( P(X > T) / P(X > u) )\n",
        "\n",
        "        prob_gate = 1.0 - gate_quantile\n",
        "        prob_final = 1.0 - final_quantile\n",
        "\n",
        "        # Probability to find *within the tail*\n",
        "        prob_target_in_tail = prob_final / prob_gate\n",
        "\n",
        "        # If our target is \"less extreme\" than the gate, this is invalid.\n",
        "        if prob_target_in_tail >= 1.0:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # Use Inverse Survival Function (isf) to find the exceedance value\n",
        "        y_p = genpareto.isf(prob_target_in_tail, c, loc=loc, scale=scale)\n",
        "\n",
        "        # The final threshold is the gate + the calculated exceedance\n",
        "        threshold = u + y_p\n",
        "\n",
        "        # Final sanity check for nan/inf\n",
        "        if not np.isfinite(threshold):\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        return threshold\n",
        "\n",
        "    except Exception:\n",
        "        # If *anything* goes wrong (e.g., failed convergence), fall back.\n",
        "        return segment_scores.quantile(final_quantile)\n",
        "\n",
        "\n",
        "def run_evt_based_analysis_for_all_scores(file_path='/content/anomaly_scores_a3.csv'):\n",
        "    \"\"\"\n",
        "    Main script to run an adaptive anomaly detection based on\n",
        "    Extreme Value Theory (EVT) for EVERY 'anomalyscore_' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    data_file = file_path\n",
        "\n",
        "    # --- Evaluation Config ---\n",
        "    NTOL = 3\n",
        "    window_size = 2 * NTOL + 1\n",
        "\n",
        "    # --- EVT Rule Set Config ---\n",
        "    GATE_QUANTILE = 0.87  # 80th percentile \"gate\"\n",
        "    FINAL_QUANTILE = 0.995 # 99.5th percentile we want to find\n",
        "\n",
        "    print(f\"Starting EXTREME VALUE THEORY (EVT) adaptive detection...\")\n",
        "    print(f\"Using data file: {data_file}\")\n",
        "    print(f\"Evaluation NTOL: {NTOL}\")\n",
        "    print(f\"Rule: Gate at {GATE_QUANTILE}, calculate threshold for {FINAL_QUANTILE}\\n\")\n",
        "\n",
        "    # Suppress warnings from scipy.stats.fit\n",
        "    warnings.filterwarnings('ignore', category=Warning)\n",
        "\n",
        "    try:\n",
        "        # --- 2. Load Data ---\n",
        "        df = pd.read_csv(data_file)\n",
        "        if 'is_anomaly' not in df.columns:\n",
        "            print(\"Error: 'is_anomaly' column not found.\")\n",
        "            return\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{data_file}' was not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Identify Score Columns ---\n",
        "    all_score_columns = [col for col in df.columns if col.startswith('anomalyscore_')]\n",
        "    #all_score_columns = [col for col in df.columns if col.startswith('H0_')]\n",
        "    if not all_score_columns:\n",
        "        print(\"Error: No columns found starting with 'anomalyscore_'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_score_columns)} score columns to analyze.\")\n",
        "\n",
        "    # --- 4. Initialize Summary ---\n",
        "    performance_summary = []\n",
        "    group_keys = ['loadeddataset', 'segment']\n",
        "\n",
        "    # --- 5. Pre-calculate Dilated Ground Truth (Optimization) ---\n",
        "    print(\"\\nPre-calculating dilated ground truth...\")\n",
        "    dilated_true_series = df.groupby(group_keys)['is_anomaly'] \\\n",
        "                              .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                              .max()\n",
        "    df['y_true_dilated'] = dilated_true_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                              .fillna(0).astype(int)\n",
        "    print(\"Dilated ground truth calculated.\")\n",
        "\n",
        "    # --- 6. Loop Through Each Score Column ---\n",
        "    for score_column in all_score_columns:\n",
        "        print(f\"\\n{'-'*25} Analyzing: {score_column} {'-'*25}\")\n",
        "\n",
        "        try:\n",
        "            # --- 6a. Clean Data (specific to this column) ---\n",
        "            df_cleaned = df.dropna(subset=[score_column]).copy()\n",
        "\n",
        "            if len(df_cleaned) == 0:\n",
        "                print(f\"Skipping {score_column}: All values are NaN.\")\n",
        "                continue\n",
        "\n",
        "            # --- 6b. Apply EVT Threshold Logic ---\n",
        "            print(\"Calculating adaptive EVT thresholds...\")\n",
        "\n",
        "            # We use .transform() to apply our custom function to each group\n",
        "            # 'transform' will pass the entire segment Series to our function\n",
        "            df_cleaned['adaptive_threshold'] = df_cleaned.groupby(group_keys)[score_column] \\\n",
        "                                                         .transform(get_evt_threshold_for_segment,\n",
        "                                                                    gate_quantile=GATE_QUANTILE,\n",
        "                                                                    final_quantile=FINAL_QUANTILE)\n",
        "\n",
        "            # --- 6c. Generate Predictions ---\n",
        "            df_cleaned['predicted_anomaly'] = (df_cleaned[score_column] > df_cleaned['adaptive_threshold']).astype(int)\n",
        "\n",
        "            print(\"Prediction generation complete.\")\n",
        "\n",
        "            # --- 6d. Apply NTOL Logic to Predictions ---\n",
        "            print(\"Applying NTOL to predictions...\")\n",
        "            dilated_pred_series = df_cleaned.groupby(group_keys)['predicted_anomaly'] \\\n",
        "                                            .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                                            .max()\n",
        "            df_cleaned['y_pred_dilated'] = dilated_pred_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                                              .fillna(0).astype(int)\n",
        "\n",
        "            # --- 6e. Calculate Custom Metrics (NTOL) ---\n",
        "            y_true = df_cleaned['is_anomaly']\n",
        "            y_pred = df_cleaned['predicted_anomaly']\n",
        "            y_true_dilated = df_cleaned['y_true_dilated']\n",
        "            y_pred_dilated = df_cleaned['y_pred_dilated']\n",
        "\n",
        "            TP = ((y_true == 1) & (y_pred_dilated == 1)).sum()\n",
        "            FN = ((y_true == 1) & (y_pred_dilated == 0)).sum()\n",
        "            FP = ((y_pred == 1) & (y_true_dilated == 0)).sum()\n",
        "            TN = ((y_true == 0) & (y_pred_dilated == 0)).sum()\n",
        "\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            # --- 6f. Print Individual Report ---\n",
        "            print(f\"--- Results for {score_column} (EVT) ---\")\n",
        "            print(f\"Total Actual Anomaly Points: {y_true.sum()}\")\n",
        "            print(f\"Total Predicted Anomaly Points: {y_pred.sum()}\\n\")\n",
        "\n",
        "            print(\"Custom Confusion Matrix (Point-Adjusted):\")\n",
        "            print(\"                 Predicted Negative | Predicted Positive\")\n",
        "            print(f\"Actual Negative: {TN:>17} | {FP:>17}\")\n",
        "            print(f\"Actual Positive: {FN:>17} | {TP:>17}\\n\")\n",
        "\n",
        "            print(\"Performance Metrics (Point-Adjusted):\")\n",
        "            print(f\"  F1 Score:  {f1:.6f}\")\n",
        "            print(f\"  Precision: {precision:.6f}\")\n",
        "            print(f\"  Recall:    {recall:.6f}\")\n",
        "\n",
        "            # --- 6g. Store in Summary ---\n",
        "            performance_summary.append({\n",
        "                'score_column': score_column,\n",
        "                'f1_score': f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'TP': TP,\n",
        "                'FN': FN,\n",
        "                'FP': FP,\n",
        "                'total_predicted': y_pred.sum()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            # This will catch errors during the loop for one score\n",
        "            # and allow the analysis to continue with the next score.\n",
        "            print(f\"!!! Error processing {score_column}: {e} !!!\", file=sys.stderr)\n",
        "            import traceback\n",
        "            traceback.print_exc(file=sys.stderr)\n",
        "\n",
        "    # --- 7. Print Final Summary Table ---\n",
        "    print(f\"\\n\\n{'-'*25} FINAL EVT (q={FINAL_QUANTILE}) PERFORMANCE SUMMARY {'-'*25}\")\n",
        "\n",
        "    if performance_summary:\n",
        "        summary_df = pd.DataFrame(performance_summary)\n",
        "        summary_df = summary_df.sort_values(by='f1_score', ascending=False)\n",
        "        summary_df = summary_df.set_index('score_column')\n",
        "\n",
        "        print(summary_df.to_string(float_format=\"%.6f\"))\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No score columns were successfully processed.\")\n",
        "\n",
        "    # Reset warnings\n",
        "    warnings.filterwarnings('default')\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_evt_based_analysis_for_all_scores()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpiwUpTrL2Gr",
        "outputId": "725d1e0c-ee5d-473d-fc61-267d1d439c40",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting EXTREME VALUE THEORY (EVT) adaptive detection...\n",
            "Using data file: /content/anomaly_scores_a3.csv\n",
            "Evaluation NTOL: 3\n",
            "Rule: Gate at 0.87, calculate threshold for 0.995\n",
            "\n",
            "Found 13 score columns to analyze.\n",
            "\n",
            "Pre-calculating dilated ground truth...\n",
            "Dilated ground truth calculated.\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_auc -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_auc (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 893\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160493 |                74\n",
            "Actual Positive:               157 |               777\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.870588\n",
            "  Precision: 0.913043\n",
            "  Recall:    0.831906\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_auc_over_max -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_auc_over_max (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 869\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160655 |                47\n",
            "Actual Positive:               167 |               767\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.877574\n",
            "  Precision: 0.942260\n",
            "  Recall:    0.821199\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_auc_over_l2 -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_auc_over_l2 (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 895\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160462 |                57\n",
            "Actual Positive:               139 |               795\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.890258\n",
            "  Precision: 0.933099\n",
            "  Recall:    0.851178\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_bottleneck -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_bottleneck (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 869\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160655 |                47\n",
            "Actual Positive:               167 |               767\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.877574\n",
            "  Precision: 0.942260\n",
            "  Recall:    0.821199\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_tail_q90 -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_tail_q90 (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 907\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160130 |               150\n",
            "Actual Positive:               173 |               761\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.824932\n",
            "  Precision: 0.835346\n",
            "  Recall:    0.814775\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_sum_centroid -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_sum_centroid (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 1102\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            159991 |               182\n",
            "Actual Positive:               197 |               737\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.795467\n",
            "  Precision: 0.801959\n",
            "  Recall:    0.789079\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_l2norm -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_l2norm (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 1105\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160170 |               213\n",
            "Actual Positive:               253 |               681\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.745077\n",
            "  Precision: 0.761745\n",
            "  Recall:    0.729122\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_pete -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_pete (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 1122\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            159989 |               196\n",
            "Actual Positive:               211 |               723\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.780356\n",
            "  Precision: 0.786725\n",
            "  Recall:    0.774090\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_energy_conc -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_energy_conc (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 815\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160724 |               243\n",
            "Actual Positive:               361 |               573\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.654857\n",
            "  Precision: 0.702206\n",
            "  Recall:    0.613490\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_dom_share -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_dom_share (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 907\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160130 |               150\n",
            "Actual Positive:               173 |               761\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.824932\n",
            "  Precision: 0.835346\n",
            "  Recall:    0.814775\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_tail_curve -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_tail_curve (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 925\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            159468 |               758\n",
            "Actual Positive:               779 |               155\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.167840\n",
            "  Precision: 0.169770\n",
            "  Recall:    0.165953\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_cen_to_energy -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_cen_to_energy (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 815\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            160724 |               243\n",
            "Actual Positive:               361 |               573\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.654857\n",
            "  Precision: 0.702206\n",
            "  Recall:    0.613490\n",
            "\n",
            "------------------------- Analyzing: anomalyscore_h0_gini -------------------------\n",
            "Calculating adaptive EVT thresholds...\n",
            "Prediction generation complete.\n",
            "Applying NTOL to predictions...\n",
            "--- Results for anomalyscore_h0_gini (EVT) ---\n",
            "Total Actual Anomaly Points: 934\n",
            "Total Predicted Anomaly Points: 1093\n",
            "\n",
            "Custom Confusion Matrix (Point-Adjusted):\n",
            "                 Predicted Negative | Predicted Positive\n",
            "Actual Negative:            158584 |               538\n",
            "Actual Positive:               374 |               560\n",
            "\n",
            "Performance Metrics (Point-Adjusted):\n",
            "  F1 Score:  0.551181\n",
            "  Precision: 0.510018\n",
            "  Recall:    0.599572\n",
            "\n",
            "\n",
            "------------------------- FINAL EVT (q=0.995) PERFORMANCE SUMMARY -------------------------\n",
            "                               f1_score  precision   recall   TP   FN   FP  total_predicted\n",
            "score_column                                                                               \n",
            "anomalyscore_h0_auc_over_l2    0.890258   0.933099 0.851178  795  139   57              895\n",
            "anomalyscore_bottleneck        0.877574   0.942260 0.821199  767  167   47              869\n",
            "anomalyscore_h0_auc_over_max   0.877574   0.942260 0.821199  767  167   47              869\n",
            "anomalyscore_h0_auc            0.870588   0.913043 0.831906  777  157   74              893\n",
            "anomalyscore_tail_q90          0.824932   0.835346 0.814775  761  173  150              907\n",
            "anomalyscore_h0_dom_share      0.824932   0.835346 0.814775  761  173  150              907\n",
            "anomalyscore_sum_centroid      0.795467   0.801959 0.789079  737  197  182             1102\n",
            "anomalyscore_pete              0.780356   0.786725 0.774090  723  211  196             1122\n",
            "anomalyscore_h0_l2norm         0.745077   0.761745 0.729122  681  253  213             1105\n",
            "anomalyscore_h0_energy_conc    0.654857   0.702206 0.613490  573  361  243              815\n",
            "anomalyscore_h0_cen_to_energy  0.654857   0.702206 0.613490  573  361  243              815\n",
            "anomalyscore_h0_gini           0.551181   0.510018 0.599572  560  374  538             1093\n",
            "anomalyscore_h0_tail_curve     0.167840   0.169770 0.165953  155  779  758              925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title EVT a4\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import genpareto\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "def get_evt_threshold_for_segment(segment_scores, gate_quantile, final_quantile):\n",
        "    \"\"\"\n",
        "    This is the core helper function that applies the EVT/POT\n",
        "    logic to a single time series segment (passed as a pandas Series).\n",
        "\n",
        "    1. Sets a \"gate\" threshold 'u' at the 'gate_quantile'.\n",
        "    2. Collects all 'exceedances' (points > u).\n",
        "    3. Tries to fit a Generalized Pareto Distribution (GPD) to these exceedances.\n",
        "    4. Uses the fitted GPD to calculate the final, much rarer 'final_quantile' threshold.\n",
        "    5. If the GPD fit fails or is unstable (e.g., too few exceedances),\n",
        "       it safely falls back to just using the 'final_quantile' directly.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Set the \"gate\" threshold ---\n",
        "        u = segment_scores.quantile(gate_quantile)\n",
        "\n",
        "        # --- 2. Collect exceedances ---\n",
        "        # We need to drop NaNs and any values <= u\n",
        "        exceedances = segment_scores[segment_scores > u].dropna() - u\n",
        "\n",
        "        # --- 5. Fallback Logic ---\n",
        "        # If we have too few points (e.g., < 10) to fit a stable GPD,\n",
        "        # or if all values are identical (nunique=1), fall back to a simple quantile.\n",
        "        if len(exceedances) < 10 or exceedances.nunique() == 1:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # --- 3. Fit GPD ---\n",
        "        # Fit the GPD to the exceedances.\n",
        "        # We fix location 'floc=0' as exceedances start at 0.\n",
        "        c, loc, scale = genpareto.fit(exceedances, floc=0)\n",
        "\n",
        "        # --- 4. Calculate Final Threshold ---\n",
        "        # We need to find the threshold 'T' such that P(X > T) = (1 - final_quantile)\n",
        "        # T = u + GPD.isf( P(X > T) / P(X > u) )\n",
        "\n",
        "        prob_gate = 1.0 - gate_quantile\n",
        "        prob_final = 1.0 - final_quantile\n",
        "\n",
        "        # Probability to find *within the tail*\n",
        "        prob_target_in_tail = prob_final / prob_gate\n",
        "\n",
        "        # If our target is \"less extreme\" than the gate, this is invalid.\n",
        "        if prob_target_in_tail >= 1.0:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # Use Inverse Survival Function (isf) to find the exceedance value\n",
        "        y_p = genpareto.isf(prob_target_in_tail, c, loc=loc, scale=scale)\n",
        "\n",
        "        # The final threshold is the gate + the calculated exceedance\n",
        "        threshold = u + y_p\n",
        "\n",
        "        # Final sanity check for nan/inf\n",
        "        if not np.isfinite(threshold):\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        return threshold\n",
        "\n",
        "    except Exception:\n",
        "        # If *anything* goes wrong (e.g., failed convergence), fall back.\n",
        "        return segment_scores.quantile(final_quantile)\n",
        "\n",
        "\n",
        "def run_evt_based_analysis_for_all_scores(file_path='/content/anomaly_scores_a4.csv'):\n",
        "    \"\"\"\n",
        "    Main script to run an adaptive anomaly detection based on\n",
        "    Extreme Value Theory (EVT) for EVERY 'anomalyscore_' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    data_file = file_path\n",
        "\n",
        "    # --- Evaluation Config ---\n",
        "    NTOL = 3\n",
        "    window_size = 2 * NTOL + 1\n",
        "\n",
        "    # --- EVT Rule Set Config ---\n",
        "    GATE_QUANTILE = 0.78  # 80th percentile \"gate\"\n",
        "    FINAL_QUANTILE = 0.993 # 99.3th percentile we want to find\n",
        "\n",
        "    print(f\"Starting EXTREME VALUE THEORY (EVT) adaptive detection...\")\n",
        "    print(f\"Using data file: {data_file}\")\n",
        "    print(f\"Evaluation NTOL: {NTOL}\")\n",
        "    print(f\"Rule: Gate at {GATE_QUANTILE}, calculate threshold for {FINAL_QUANTILE}\\n\")\n",
        "\n",
        "    # Suppress warnings from scipy.stats.fit\n",
        "    warnings.filterwarnings('ignore', category=Warning)\n",
        "\n",
        "    try:\n",
        "        # --- 2. Load Data ---\n",
        "        df = pd.read_csv(data_file)\n",
        "        if 'is_anomaly' not in df.columns:\n",
        "            print(\"Error: 'is_anomaly' column not found.\")\n",
        "            return\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{data_file}' was not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Identify Score Columns ---\n",
        "    all_score_columns = [col for col in df.columns if col.startswith('anomalyscore_')]\n",
        "\n",
        "    if not all_score_columns:\n",
        "        print(\"Error: No columns found starting with 'anomalyscore_'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_score_columns)} score columns to analyze.\")\n",
        "\n",
        "    # --- 4. Initialize Summary ---\n",
        "    performance_summary = []\n",
        "    group_keys = ['loadeddataset', 'segment']\n",
        "\n",
        "    # --- 5. Pre-calculate Dilated Ground Truth (Optimization) ---\n",
        "    print(\"\\nPre-calculating dilated ground truth...\")\n",
        "    dilated_true_series = df.groupby(group_keys)['is_anomaly'] \\\n",
        "                              .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                              .max()\n",
        "    df['y_true_dilated'] = dilated_true_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                              .fillna(0).astype(int)\n",
        "    print(\"Dilated ground truth calculated.\")\n",
        "\n",
        "    # --- 6. Loop Through Each Score Column ---\n",
        "    for score_column in all_score_columns:\n",
        "        print(f\"\\n{'-'*25} Analyzing: {score_column} {'-'*25}\")\n",
        "\n",
        "        try:\n",
        "            # --- 6a. Clean Data (specific to this column) ---\n",
        "            df_cleaned = df.dropna(subset=[score_column]).copy()\n",
        "\n",
        "            if len(df_cleaned) == 0:\n",
        "                print(f\"Skipping {score_column}: All values are NaN.\")\n",
        "                continue\n",
        "\n",
        "            # --- 6b. Apply EVT Threshold Logic ---\n",
        "            print(\"Calculating adaptive EVT thresholds...\")\n",
        "\n",
        "            # We use .transform() to apply our custom function to each group\n",
        "            # 'transform' will pass the entire segment Series to our function\n",
        "            df_cleaned['adaptive_threshold'] = df_cleaned.groupby(group_keys)[score_column] \\\n",
        "                                                         .transform(get_evt_threshold_for_segment,\n",
        "                                                                    gate_quantile=GATE_QUANTILE,\n",
        "                                                                    final_quantile=FINAL_QUANTILE)\n",
        "\n",
        "            # --- 6c. Generate Predictions ---\n",
        "            df_cleaned['predicted_anomaly'] = (df_cleaned[score_column] > df_cleaned['adaptive_threshold']).astype(int)\n",
        "\n",
        "            print(\"Prediction generation complete.\")\n",
        "\n",
        "            # --- 6d. Apply NTOL Logic to Predictions ---\n",
        "            print(\"Applying NTOL to predictions...\")\n",
        "            dilated_pred_series = df_cleaned.groupby(group_keys)['predicted_anomaly'] \\\n",
        "                                            .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                                            .max()\n",
        "            df_cleaned['y_pred_dilated'] = dilated_pred_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                                              .fillna(0).astype(int)\n",
        "\n",
        "            # --- 6e. Calculate Custom Metrics (NTOL) ---\n",
        "            y_true = df_cleaned['is_anomaly']\n",
        "            y_pred = df_cleaned['predicted_anomaly']\n",
        "            y_true_dilated = df_cleaned['y_true_dilated']\n",
        "            y_pred_dilated = df_cleaned['y_pred_dilated']\n",
        "\n",
        "            TP = ((y_true == 1) & (y_pred_dilated == 1)).sum()\n",
        "            FN = ((y_true == 1) & (y_pred_dilated == 0)).sum()\n",
        "            FP = ((y_pred == 1) & (y_true_dilated == 0)).sum()\n",
        "            TN = ((y_true == 0) & (y_pred_dilated == 0)).sum()\n",
        "\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            # --- 6f. Print Individual Report ---\n",
        "            print(f\"--- Results for {score_column} (EVT) ---\")\n",
        "            print(f\"Total Actual Anomaly Points: {y_true.sum()}\")\n",
        "            print(f\"Total Predicted Anomaly Points: {y_pred.sum()}\\n\")\n",
        "\n",
        "            print(\"Custom Confusion Matrix (Point-Adjusted):\")\n",
        "            print(\"                 Predicted Negative | Predicted Positive\")\n",
        "            print(f\"Actual Negative: {TN:>17} | {FP:>17}\")\n",
        "            print(f\"Actual Positive: {FN:>17} | {TP:>17}\\n\")\n",
        "\n",
        "            print(\"Performance Metrics (Point-Adjusted):\")\n",
        "            print(f\"  F1 Score:  {f1:.6f}\")\n",
        "            print(f\"  Precision: {precision:.6f}\")\n",
        "            print(f\"  Recall:    {recall:.6f}\")\n",
        "\n",
        "            # --- 6g. Store in Summary ---\n",
        "            performance_summary.append({\n",
        "                'score_column': score_column,\n",
        "                'f1_score': f1,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'TP': TP,\n",
        "                'FN': FN,\n",
        "                'FP': FP,\n",
        "                'total_predicted': y_pred.sum()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            # This will catch errors during the loop for one score\n",
        "            # and allow the analysis to continue with the next score.\n",
        "            print(f\"!!! Error processing {score_column}: {e} !!!\", file=sys.stderr)\n",
        "            import traceback\n",
        "            traceback.print_exc(file=sys.stderr)\n",
        "\n",
        "    # --- 7. Print Final Summary Table ---\n",
        "    print(f\"\\n\\n{'-'*25} FINAL EVT (q={FINAL_QUANTILE}) PERFORMANCE SUMMARY {'-'*25}\")\n",
        "\n",
        "    if performance_summary:\n",
        "        summary_df = pd.DataFrame(performance_summary)\n",
        "        summary_df = summary_df.sort_values(by='f1_score', ascending=False)\n",
        "        summary_df = summary_df.set_index('score_column')\n",
        "\n",
        "        print(summary_df.to_string(float_format=\"%.6f\"))\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No score columns were successfully processed.\")\n",
        "\n",
        "    # Reset warnings\n",
        "    warnings.filterwarnings('default')\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_evt_based_analysis_for_all_scores()"
      ],
      "metadata": {
        "id": "V5rDl5SwYCGy",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title finding the best parameters\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import genpareto\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "def get_evt_threshold_for_segment(segment_scores, gate_quantile, final_quantile):\n",
        "    \"\"\"\n",
        "    This is the core helper function that applies the EVT/POT\n",
        "    logic to a single time series segment (passed as a pandas Series).\n",
        "\n",
        "    1. Sets a \"gate\" threshold 'u' at the 'gate_quantile'.\n",
        "    2. Collects all 'exceedances' (points > u).\n",
        "    3. Tries to fit a Generalized Pareto Distribution (GPD) to these exceedances.\n",
        "    4. Uses the fitted GPD to calculate the final, much rarer 'final_quantile' threshold.\n",
        "    5. If the GPD fit fails or is unstable (e.g., too few exceedances),\n",
        "       it safely falls back to just using the 'final_quantile' directly.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Set the \"gate\" threshold ---\n",
        "        u = segment_scores.quantile(gate_quantile)\n",
        "\n",
        "        # --- 2. Collect exceedances ---\n",
        "        # We need to drop NaNs and any values <= u\n",
        "        exceedances = segment_scores[segment_scores > u].dropna() - u\n",
        "\n",
        "        # --- 5. Fallback Logic ---\n",
        "        # If we have too few points (e.g., < 10) to fit a stable GPD,\n",
        "        # or if all values are identical (nunique=1), fall back to a simple quantile.\n",
        "        if len(exceedances) < 10 or exceedances.nunique() <= 1:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # --- 3. Fit GPD ---\n",
        "        # Fit the GPD to the exceedances.\n",
        "        # We fix location 'floc=0' as exceedances start at 0.\n",
        "        c, loc, scale = genpareto.fit(exceedances, floc=0)\n",
        "\n",
        "        # --- 4. Calculate Final Threshold ---\n",
        "        # T = u + GPD.isf( P(X > T) / P(X > u) )\n",
        "\n",
        "        prob_gate = 1.0 - gate_quantile\n",
        "        prob_final = 1.0 - final_quantile\n",
        "\n",
        "        # Probability to find *within the tail*\n",
        "        prob_target_in_tail = prob_final / prob_gate\n",
        "\n",
        "        # If our target is \"less extreme\" than the gate, this is invalid.\n",
        "        if prob_target_in_tail >= 1.0 or prob_target_in_tail <= 0:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # Use Inverse Survival Function (isf) to find the exceedance value\n",
        "        y_p = genpareto.isf(prob_target_in_tail, c, loc=loc, scale=scale)\n",
        "\n",
        "        # The final threshold is the gate + the calculated exceedance\n",
        "        threshold = u + y_p\n",
        "\n",
        "        # Final sanity check for nan/inf or negative thresholds\n",
        "        if not np.isfinite(threshold) or threshold < 0:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        return threshold\n",
        "\n",
        "    except Exception:\n",
        "        # If *anything* goes wrong (e.g., failed convergence), fall back.\n",
        "        return segment_scores.quantile(final_quantile)\n",
        "\n",
        "\n",
        "def run_evt_grid_search(file_path='anomaly_scores_a3.csv'):\n",
        "    \"\"\"\n",
        "    Main script to run a grid search for the best EVT parameters\n",
        "    for EVERY 'anomalyscore_' column.\n",
        "\n",
        "    MODIFIED: This version only prints the *single best* feature\n",
        "    for each parameter set, instead of all 13.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    # Using the original, correct filename\n",
        "    data_file = file_path\n",
        "\n",
        "    # --- Evaluation Config ---\n",
        "    NTOL = 3\n",
        "    window_size = 2 * NTOL + 1\n",
        "\n",
        "    # --- EVT Grid Search Parameters ---\n",
        "    GATE_QUANTILE_LIST = [0.75,0.77,0.79,0.80,0.81,0.83,0.85,0.86,0.87,0.88,0.89,0.90,0.91,0.93,0.95,0.99]\n",
        "    FINAL_QUANTILE_LIST = [ 0.995]\n",
        "\n",
        "    print(f\"Starting EVT PARAMETER GRID SEARCH...\")\n",
        "    print(f\"Using data file: {data_file}\")\n",
        "    print(f\"Evaluation NTOL: {NTOL}\")\n",
        "    print(f\"Gate Quantiles to test: {GATE_QUANTILE_LIST}\")\n",
        "    print(f\"Final Quantiles to test: {FINAL_QUANTILE_LIST}\\n\")\n",
        "\n",
        "    # Suppress warnings from scipy.stats.fit\n",
        "    warnings.filterwarnings('ignore', category=Warning)\n",
        "\n",
        "    try:\n",
        "        # --- 2. Load Data ---\n",
        "        df = pd.read_csv(data_file)\n",
        "        if 'is_anomaly' not in df.columns:\n",
        "            print(\"Error: 'is_anomaly' column not found.\")\n",
        "            return\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{data_file}' was not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Identify Score Columns ---\n",
        "    all_score_columns = [col for col in df.columns if col.startswith('anomalyscore_')]\n",
        "\n",
        "    if not all_score_columns:\n",
        "        print(\"Error: No columns found starting with 'anomalyscore_'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_score_columns)} score columns to analyze.\")\n",
        "\n",
        "    # --- 4. Initialize Summary ---\n",
        "    master_performance_summary = []\n",
        "\n",
        "    group_keys = ['loadeddataset', 'segment']\n",
        "\n",
        "    # --- 5. Pre-calculate Dilated Ground Truth (Optimization) ---\n",
        "    print(\"\\nPre-calculating dilated ground truth...\")\n",
        "    dilated_true_series = df.groupby(group_keys)['is_anomaly'] \\\n",
        "                              .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                              .max()\n",
        "    # This is the line you provided, ensures robust handling\n",
        "    df['y_true_dilated'] = dilated_true_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                              .fillna(0).astype(int)\n",
        "    print(\"Dilated ground truth calculated.\")\n",
        "\n",
        "    # --- 6. Run the Grid Search Loop ---\n",
        "\n",
        "    # This list will store only the single best result from each set\n",
        "    best_results_by_param_set = []\n",
        "\n",
        "    for gate_q in GATE_QUANTILE_LIST:\n",
        "        for final_q in FINAL_QUANTILE_LIST:\n",
        "\n",
        "            print(f\"\\n--- Analyzing Parameter Set: GateQ={gate_q}, FinalQ={final_q} ---\")\n",
        "\n",
        "            # This list will hold the 13 results for this *specific* parameter set\n",
        "            current_param_set_results = []\n",
        "\n",
        "            for score_column in all_score_columns:\n",
        "\n",
        "                # Removed the chatty \"Analyzing...\" print here\n",
        "\n",
        "                try:\n",
        "                    # --- 6a. Clean Data ---\n",
        "                    df_cleaned = df.dropna(subset=[score_column]).copy()\n",
        "\n",
        "                    if len(df_cleaned) == 0:\n",
        "                        continue # Skip this score if it's all NaN\n",
        "\n",
        "                    # --- 6b. Apply EVT Threshold Logic ---\n",
        "                    df_cleaned['adaptive_threshold'] = df_cleaned.groupby(group_keys)[score_column] \\\n",
        "                                                                 .transform(get_evt_threshold_for_segment,\n",
        "                                                                            gate_quantile=gate_q,\n",
        "                                                                            final_quantile=final_q)\n",
        "\n",
        "                    # --- 6c. Generate Predictions ---\n",
        "                    df_cleaned['predicted_anomaly'] = (df_cleaned[score_column] > df_cleaned['adaptive_threshold']).astype(int)\n",
        "\n",
        "                    # --- 6d. Apply NTOL Logic to Predictions ---\n",
        "                    dilated_pred_series = df_cleaned.groupby(group_keys)['predicted_anomaly'] \\\n",
        "                                                    .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                                                    .max()\n",
        "                    df_cleaned['y_pred_dilated'] = dilated_pred_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                                                      .fillna(0).astype(int)\n",
        "\n",
        "                    # --- 6e. Calculate Custom Metrics (NTOL) ---\n",
        "                    y_true = df_cleaned['is_anomaly']\n",
        "                    y_pred = df_cleaned['predicted_anomaly']\n",
        "                    y_true_dilated = df_cleaned['y_true_dilated']\n",
        "                    y_pred_dilated = df_cleaned['y_pred_dilated']\n",
        "\n",
        "                    TP = ((y_true == 1) & (y_pred_dilated == 1)).sum()\n",
        "                    FN = ((y_true == 1) & (y_pred_dilated == 0)).sum()\n",
        "                    FP = ((y_pred == 1) & (y_true_dilated == 0)).sum()\n",
        "                    TN = ((y_true == 0) & (y_pred_dilated == 0)).sum()\n",
        "\n",
        "                    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "                    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                    # --- 6f. Store in *temporary* and *master* lists ---\n",
        "                    result_details = {\n",
        "                        'score_column': score_column,\n",
        "                        'gate_quantile': gate_q,\n",
        "                        'final_quantile': final_q,\n",
        "                        'f1_score': f1,\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'TP': TP,\n",
        "                        'FN': FN,\n",
        "                        'FP': FP,\n",
        "                        'total_predicted': y_pred.sum()\n",
        "                    }\n",
        "                    current_param_set_results.append(result_details)\n",
        "                    master_performance_summary.append(result_details)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"!!! Error processing {score_column} with G={gate_q}, F={final_q}: {e} !!!\", file=sys.stderr)\n",
        "                    import traceback\n",
        "                    traceback.print_exc(file=sys.stderr)\n",
        "\n",
        "            # --- End of score_column loop ---\n",
        "            # Now, find the best result *from this parameter set* and print it\n",
        "            if current_param_set_results:\n",
        "                best_for_set = max(current_param_set_results, key=lambda x: x['f1_score'])\n",
        "                best_results_by_param_set.append(best_for_set) # Store for final summary\n",
        "\n",
        "                print(f\"    -> Best F1 for this set: {best_for_set['f1_score']:.6f} \"\n",
        "                      f\"(using {best_for_set['score_column']})\")\n",
        "            else:\n",
        "                print(\"    -> No results for this parameter set.\")\n",
        "\n",
        "    # --- 7. Print Final Summary Table ---\n",
        "    print(f\"\\n\\n{'-'*25} FINAL EVT GRID SEARCH SUMMARY {'-'*25}\")\n",
        "\n",
        "    if master_performance_summary:\n",
        "        # Create DataFrames from our collected results\n",
        "        full_summary_df = pd.DataFrame(master_performance_summary)\n",
        "        best_set_summary_df = pd.DataFrame(best_results_by_param_set)\n",
        "\n",
        "        # Sort the \"best of each set\" list to find the overall winners\n",
        "        best_set_summary_df = best_set_summary_df.sort_values(by='f1_score', ascending=False)\n",
        "\n",
        "        print(\"\\n--- TOP 20 BEST PARAMETER SETS (and their best feature) ---\")\n",
        "        print(best_set_summary_df.head(20).to_string(float_format=\"%.6f\",\n",
        "                                                    columns=['gate_quantile', 'final_quantile', 'score_column', 'f1_score', 'precision', 'recall']))\n",
        "\n",
        "        print(\"\\n\\n--- !!! BEST OVERALL RESULT !!! ---\")\n",
        "        best_overall = best_set_summary_df.iloc[0]\n",
        "        print(best_overall)\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No combinations were successfully processed.\")\n",
        "\n",
        "    # Reset warnings\n",
        "    warnings.filterwarnings('default')\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_evt_grid_search()"
      ],
      "metadata": {
        "id": "drDHiLlQXvaT",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title finding the best parameters\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import genpareto\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "def get_evt_threshold_for_segment(segment_scores, gate_quantile, final_quantile):\n",
        "    \"\"\"\n",
        "    This is the core helper function that applies the EVT/POT\n",
        "    logic to a single time series segment (passed as a pandas Series).\n",
        "\n",
        "    1. Sets a \"gate\" threshold 'u' at the 'gate_quantile'.\n",
        "    2. Collects all 'exceedances' (points > u).\n",
        "    3. Tries to fit a Generalized Pareto Distribution (GPD) to these exceedances.\n",
        "    4. Uses the fitted GPD to calculate the final, much rarer 'final_quantile' threshold.\n",
        "    5. If the GPD fit fails or is unstable (e.g., too few exceedances),\n",
        "       it safely falls back to just using the 'final_quantile' directly.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Set the \"gate\" threshold ---\n",
        "        u = segment_scores.quantile(gate_quantile)\n",
        "\n",
        "        # --- 2. Collect exceedances ---\n",
        "        # We need to drop NaNs and any values <= u\n",
        "        exceedances = segment_scores[segment_scores > u].dropna() - u\n",
        "\n",
        "        # --- 5. Fallback Logic ---\n",
        "        # If we have too few points (e.g., < 10) to fit a stable GPD,\n",
        "        # or if all values are identical (nunique=1), fall back to a simple quantile.\n",
        "        if len(exceedances) < 10 or exceedances.nunique() <= 1:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # --- 3. Fit GPD ---\n",
        "        # Fit the GPD to the exceedances.\n",
        "        # We fix location 'floc=0' as exceedances start at 0.\n",
        "        c, loc, scale = genpareto.fit(exceedances, floc=0)\n",
        "\n",
        "        # --- 4. Calculate Final Threshold ---\n",
        "        # T = u + GPD.isf( P(X > T) / P(X > u) )\n",
        "\n",
        "        prob_gate = 1.0 - gate_quantile\n",
        "        prob_final = 1.0 - final_quantile\n",
        "\n",
        "        # Probability to find *within the tail*\n",
        "        prob_target_in_tail = prob_final / prob_gate\n",
        "\n",
        "        # If our target is \"less extreme\" than the gate, this is invalid.\n",
        "        if prob_target_in_tail >= 1.0 or prob_target_in_tail <= 0:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        # Use Inverse Survival Function (isf) to find the exceedance value\n",
        "        y_p = genpareto.isf(prob_target_in_tail, c, loc=loc, scale=scale)\n",
        "\n",
        "        # The final threshold is the gate + the calculated exceedance\n",
        "        threshold = u + y_p\n",
        "\n",
        "        # Final sanity check for nan/inf or negative thresholds\n",
        "        if not np.isfinite(threshold) or threshold < 0:\n",
        "            return segment_scores.quantile(final_quantile)\n",
        "\n",
        "        return threshold\n",
        "\n",
        "    except Exception:\n",
        "        # If *anything* goes wrong (e.g., failed convergence), fall back.\n",
        "        return segment_scores.quantile(final_quantile)\n",
        "\n",
        "\n",
        "def run_evt_grid_search(file_path='anomaly_scores_a4.csv'):\n",
        "    \"\"\"\n",
        "    Main script to run a grid search for the best EVT parameters\n",
        "    for EVERY 'anomalyscore_' column.\n",
        "\n",
        "    MODIFIED: This version only prints the *single best* feature\n",
        "    for each parameter set, instead of all 13.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    # Using the original, correct filename\n",
        "    data_file = file_path\n",
        "\n",
        "    # --- Evaluation Config ---\n",
        "    NTOL = 3\n",
        "    window_size = 2 * NTOL + 1\n",
        "\n",
        "    # --- EVT Grid Search Parameters ---\n",
        "    GATE_QUANTILE_LIST = np.round(np.arange(0.60, 0.901, 0.01), 2)\n",
        "    FINAL_QUANTILE_LIST = np.round(np.arange(0.980, 0.9971, 0.001), 3)\n",
        "\n",
        "    print(f\"Starting EVT PARAMETER GRID SEARCH...\")\n",
        "    print(f\"Using data file: {data_file}\")\n",
        "    print(f\"Evaluation NTOL: {NTOL}\")\n",
        "    print(f\"Gate Quantiles to test: {GATE_QUANTILE_LIST}\")\n",
        "    print(f\"Final Quantiles to test: {FINAL_QUANTILE_LIST}\\n\")\n",
        "\n",
        "    # Suppress warnings from scipy.stats.fit\n",
        "    warnings.filterwarnings('ignore', category=Warning)\n",
        "\n",
        "    try:\n",
        "        # --- 2. Load Data ---\n",
        "        df = pd.read_csv(data_file)\n",
        "        if 'is_anomaly' not in df.columns:\n",
        "            print(\"Error: 'is_anomaly' column not found.\")\n",
        "            return\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{data_file}' was not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Identify Score Columns ---\n",
        "    all_score_columns = [col for col in df.columns if col.startswith('anomalyscore_')]\n",
        "\n",
        "    if not all_score_columns:\n",
        "        print(\"Error: No columns found starting with 'anomalyscore_'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_score_columns)} score columns to analyze.\")\n",
        "\n",
        "    # --- 4. Initialize Summary ---\n",
        "    master_performance_summary = []\n",
        "\n",
        "    group_keys = ['loadeddataset', 'segment']\n",
        "\n",
        "    # --- 5. Pre-calculate Dilated Ground Truth (Optimization) ---\n",
        "    print(\"\\nPre-calculating dilated ground truth...\")\n",
        "    dilated_true_series = df.groupby(group_keys)['is_anomaly'] \\\n",
        "                              .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                              .max()\n",
        "    # This is the line you provided, ensures robust handling\n",
        "    df['y_true_dilated'] = dilated_true_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                              .fillna(0).astype(int)\n",
        "    print(\"Dilated ground truth calculated.\")\n",
        "\n",
        "    # --- 6. Run the Grid Search Loop ---\n",
        "\n",
        "    # This list will store only the single best result from each set\n",
        "    best_results_by_param_set = []\n",
        "\n",
        "    for gate_q in GATE_QUANTILE_LIST:\n",
        "        for final_q in FINAL_QUANTILE_LIST:\n",
        "\n",
        "            print(f\"\\n--- Analyzing Parameter Set: GateQ={gate_q}, FinalQ={final_q} ---\")\n",
        "\n",
        "            # This list will hold the 13 results for this *specific* parameter set\n",
        "            current_param_set_results = []\n",
        "\n",
        "            for score_column in all_score_columns:\n",
        "\n",
        "                # Removed the chatty \"Analyzing...\" print here\n",
        "\n",
        "                try:\n",
        "                    # --- 6a. Clean Data ---\n",
        "                    df_cleaned = df.dropna(subset=[score_column]).copy()\n",
        "\n",
        "                    if len(df_cleaned) == 0:\n",
        "                        continue # Skip this score if it's all NaN\n",
        "\n",
        "                    # --- 6b. Apply EVT Threshold Logic ---\n",
        "                    df_cleaned['adaptive_threshold'] = df_cleaned.groupby(group_keys)[score_column] \\\n",
        "                                                                 .transform(get_evt_threshold_for_segment,\n",
        "                                                                            gate_quantile=gate_q,\n",
        "                                                                            final_quantile=final_q)\n",
        "\n",
        "                    # --- 6c. Generate Predictions ---\n",
        "                    df_cleaned['predicted_anomaly'] = (df_cleaned[score_column] > df_cleaned['adaptive_threshold']).astype(int)\n",
        "\n",
        "                    # --- 6d. Apply NTOL Logic to Predictions ---\n",
        "                    dilated_pred_series = df_cleaned.groupby(group_keys)['predicted_anomaly'] \\\n",
        "                                                    .rolling(window=window_size, center=True, min_periods=1) \\\n",
        "                                                    .max()\n",
        "                    df_cleaned['y_pred_dilated'] = dilated_pred_series.reset_index(level=group_keys, drop=True) \\\n",
        "                                                                      .fillna(0).astype(int)\n",
        "\n",
        "                    # --- 6e. Calculate Custom Metrics (NTOL) ---\n",
        "                    y_true = df_cleaned['is_anomaly']\n",
        "                    y_pred = df_cleaned['predicted_anomaly']\n",
        "                    y_true_dilated = df_cleaned['y_true_dilated']\n",
        "                    y_pred_dilated = df_cleaned['y_pred_dilated']\n",
        "\n",
        "                    TP = ((y_true == 1) & (y_pred_dilated == 1)).sum()\n",
        "                    FN = ((y_true == 1) & (y_pred_dilated == 0)).sum()\n",
        "                    FP = ((y_pred == 1) & (y_true_dilated == 0)).sum()\n",
        "                    TN = ((y_true == 0) & (y_pred_dilated == 0)).sum()\n",
        "\n",
        "                    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "                    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                    # --- 6f. Store in *temporary* and *master* lists ---\n",
        "                    result_details = {\n",
        "                        'score_column': score_column,\n",
        "                        'gate_quantile': gate_q,\n",
        "                        'final_quantile': final_q,\n",
        "                        'f1_score': f1,\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'TP': TP,\n",
        "                        'FN': FN,\n",
        "                        'FP': FP,\n",
        "                        'total_predicted': y_pred.sum()\n",
        "                    }\n",
        "                    current_param_set_results.append(result_details)\n",
        "                    master_performance_summary.append(result_details)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"!!! Error processing {score_column} with G={gate_q}, F={final_q}: {e} !!!\", file=sys.stderr)\n",
        "                    import traceback\n",
        "                    traceback.print_exc(file=sys.stderr)\n",
        "\n",
        "            # --- End of score_column loop ---\n",
        "            # Now, find the best result *from this parameter set* and print it\n",
        "            if current_param_set_results:\n",
        "                best_for_set = max(current_param_set_results, key=lambda x: x['f1_score'])\n",
        "                best_results_by_param_set.append(best_for_set) # Store for final summary\n",
        "\n",
        "                print(f\"    -> Best F1 for this set: {best_for_set['f1_score']:.6f} \"\n",
        "                      f\"(using {best_for_set['score_column']})\")\n",
        "            else:\n",
        "                print(\"    -> No results for this parameter set.\")\n",
        "\n",
        "    # --- 7. Print Final Summary Table ---\n",
        "    print(f\"\\n\\n{'-'*25} FINAL EVT GRID SEARCH SUMMARY {'-'*25}\")\n",
        "\n",
        "    if master_performance_summary:\n",
        "        # Create DataFrames from our collected results\n",
        "        full_summary_df = pd.DataFrame(master_performance_summary)\n",
        "        best_set_summary_df = pd.DataFrame(best_results_by_param_set)\n",
        "\n",
        "        # Sort the \"best of each set\" list to find the overall winners\n",
        "        best_set_summary_df = best_set_summary_df.sort_values(by='f1_score', ascending=False)\n",
        "\n",
        "        print(\"\\n--- TOP 20 BEST PARAMETER SETS (and their best feature) ---\")\n",
        "        print(best_set_summary_df.head(20).to_string(float_format=\"%.6f\",\n",
        "                                                    columns=['gate_quantile', 'final_quantile', 'score_column', 'f1_score', 'precision', 'recall']))\n",
        "\n",
        "        print(\"\\n\\n--- !!! BEST OVERALL RESULT !!! ---\")\n",
        "        best_overall = best_set_summary_df.iloc[0]\n",
        "        print(best_overall)\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No combinations were successfully processed.\")\n",
        "\n",
        "    # Reset warnings\n",
        "    warnings.filterwarnings('default')\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_evt_grid_search()"
      ],
      "metadata": {
        "id": "rWM9LhOYObt7",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title a sample of auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def tri_hat(b, d, t):\n",
        "    m = (b + d) / 2.0\n",
        "    h = (d - b) / 2.0\n",
        "    y = np.zeros_like(t, dtype=float)\n",
        "    left  = (t >= b) & (t <= m)\n",
        "    right = (t > m) & (t <= d)\n",
        "    y[left]  = (t[left]  - b) / (m - b) * h\n",
        "    y[right] = (d - t[right]) / (d - m) * h\n",
        "    return y\n",
        "\n",
        "bars = [(0,4), (10,12)]\n",
        "t = np.linspace(-1, 13, 2000)\n",
        "hats = [tri_hat(b, d, t) for (b, d) in bars]\n",
        "envelope = np.max(np.vstack(hats), axis=0)\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "for (b,d), h in zip(bars, hats):\n",
        "    plt.plot(t, h, linewidth=2, label=f\"hat({b},{d})\")\n",
        "plt.plot(t, envelope, 'k--', linewidth=2, label=\"max-envelope $\\Lambda(t)$\")\n",
        "plt.xlabel(\"filtration parameter $t$\")\n",
        "plt.ylabel(\"height\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"toy_auc_l2.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1Typ1R-fBC5C",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}